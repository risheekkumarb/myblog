{
 "cells": [
  {
   "cell_type": "raw",
   "id": "853520d2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"GEPA Deepdive\"\n",
    "author: \"Risheek kumar B\"\n",
    "date: \"2025-12-21\"\n",
    "categories: [code, research paper, analysis, reimplementation]\n",
    "description: \"An indepth hands-on explanation with code about GEPA - its core and motivations\"\n",
    "image: 'static/blogpost_image.webp'\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6bfd9",
   "metadata": {},
   "source": [
    "\n",
    "> This article was made possible by the research papers referenced throughout, the [Weaviate discussion with Lakshya A Agrawal](https://www.youtube.com/watch?v=fREQrxhBSk0), and [guidance](https://www.answer.ai/posts/2025-10-13-video-to-doc.html) from Kerem Turgutlu at answer.ai. Created using the [solveit](https://solveit.fast.ai) platform by fast.ai.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707e1ba",
   "metadata": {},
   "source": [
    "You've spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works... 70% of the time. You have 50 labeled examples. Now what?\n",
    "\n",
    "Fine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn't scale. You're stuck.\n",
    "\n",
    "But what if you could match RL's optimization performance using 50 examples instead of 5,000?\n",
    "\n",
    "**[GEPA](https://arxiv.org/abs/2507.19457)** (Genetic-Pareto) does exactly thisâ€”by exploiting something traditional optimization ignores: the detailed textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM *reflect on its own failures* and propose improvements directly.\n",
    "\n",
    "In this post, we'll unpack how it works, why modern LLMs' improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399b11e",
   "metadata": {},
   "source": [
    "### The Problem: Scalar Rewards and Expensive Rollouts\n",
    "\n",
    "Traditional AI optimization techniquesâ€”reinforcement learning and fine-tuningâ€”have achieved str results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?\n",
    "\n",
    "Consider:\n",
    "\n",
    "- **Agentic pipelines** that invoke simulations, query rate-limited APIs, or run multi-step tool chains\n",
    "- **Code generation for novel hardware**, where each evaluation requires compiling for custom silicon and executing on the device\n",
    "- **Complex reasoning tasks** with expensive verification steps\n",
    "\n",
    "Collecting thousands of rollouts simply isn't feasible in these settings.\n",
    "\n",
    "Why? **RL learns by comparison**. A 500-step trajectory collapses to `reward = 0.73`. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectoriesâ€”*this one scored 0.8, that one scored 0.4, what differed?*â€”requiring sample counts that expensive domains can't support.\n",
    "\n",
    "> *RL and fine-tuning require generating large amounts of rollouts to gather scalar learning signalsâ€”sample inefficient by design.*\n",
    "\n",
    "When each rollout costs minutes (or dollars), this approach breaks down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147d333",
   "metadata": {},
   "source": [
    "### The Insight: LLM Pipelines Generate Rich Textual Traces\n",
    "\n",
    "Modern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:\n",
    "\n",
    "* **Reasoning traces** â€” Chain-of-Thought and ReAct logs expose the model's explicit thought process. When a multi-hop QA system fails, you can see *where* the reasoning went wrong: \"The capital of France is Paris. Paris is in Germany...\" The failure mode is visible in the text.\n",
    "* **Environment feedback** â€” Compiler errors don't just say \"failed.\" They say `cannot find symbol 'x', did you mean 'y'?` API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.\n",
    "* **Evaluation rubrics** â€” LLM-as-judge systems don't just score 3/5. They explain: \"Response was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.\"\n",
    "\n",
    "Each of these is far more informative than `reward = 0.73`.\n",
    "\n",
    "But here's the thing: these traces aren't just logs for debuggingâ€”they're potential *input to the optimizer*. A compiler error that says \"did you mean 'y'?\" contains the fix. A rubric that says \"too verbose\" specifies exactly what to change.\n",
    "\n",
    "Traditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.\n",
    "\n",
    "But what if we could just... read the feedback?\n",
    "\n",
    "This is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd100e75",
   "metadata": {},
   "source": [
    "### The Opportunity: LLMs Can Reflect on Their Own Failures\n",
    "\n",
    "**LLMs already have prior knowledge about the domains they're working in, and they're increasingly capable of self-reflection.**\n",
    "\n",
    "Consider what happens with different types of feedback:\n",
    "\n",
    "**Compiler errors** â€” When the compiler returns `cannot find symbol 'x', did you mean 'y'?`, the LLM doesn't need thousands of examples to learn the fix. It already knows the library's API. One error message is enough.\n",
    "\n",
    "> *\"The language model already knows that x is not a valid API name in the library but y is. Next time I should try this.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "**LLM-as-judge feedback** â€” When a judge says \"your summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,\" the model can directly incorporate \"be concise, match casual tone\" into its next attempt. No statistical signal extraction required.\n",
    "\n",
    "**Reasoning trace failures** â€” When a multi-hop QA trace shows the model correctly retrieved \"Paris is the capital of France\" but then hallucinated \"Paris is in Germany,\" the failure point is *visible in the text*. You can see exactly where the reasoning derailed.\n",
    "\n",
    "**Privacy-aware rewriting (PUPA task)** â€” In the paper's experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains *why* a rewrite failedâ€”\"leaked the user's company name\" or \"removed too much context, degrading response quality\"â€”giving the optimizer actionable signal from each example.\n",
    "\n",
    "#### How They Compare\n",
    "\n",
    "| Approach | How it learns |\n",
    "|----------|---------------|\n",
    "| **RL** | Compare thousands of trajectories statistically: \"These 500 scored 0.8, those 500 scored 0.4â€”what differed?\" |\n",
    "| **Reflection** | Read the feedback directly: \"The compiler said use y, so use y.\" |\n",
    "\n",
    "RL would need hundreds of rollouts to statistically isolate that `xâ†’y` is the fix. The LLM gets it from one error message.\n",
    "\n",
    "#### From Fixes to Generalizable Rules\n",
    "\n",
    "Modern LLMs don't just extract point fixesâ€”they can derive *generalizable lessons*:\n",
    "\n",
    "- Beyond \"use `y` instead of `x`\": \"always verify symbol names against the library's namespace before generating code\"\n",
    "- Beyond \"response was too long\": \"for Slack contexts, limit responses to 150 words and use bullet points\"\n",
    "- Beyond \"leaked company name\": \"scan for proper nouns and replace with generic placeholders\"\n",
    "\n",
    "> *\"LLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "These rules get folded directly into the prompt as instructionsâ€”accumulating improvements across examples rather than treating each failure in isolation.\n",
    "\n",
    "LLMs that can genuinely reflect and generalize make this approach viable now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b3a53",
   "metadata": {},
   "source": [
    "### Why This Works Now\n",
    "\n",
    "This approach wasn't viable with earlier LLMs. In March 2023, roboticist [Eric Jang observed](https://evjang.com/2023/03/26/self-reflection.html) that self-reflection capability \"seems to be emergent in GPT-4 but not GPT-3.5 or Claude.\" When asked to write a non-rhyming poem, GPT-4 produced rhymesâ€”but when prompted \"did the poem meet the assignment?\" it apologized and corrected itself. GPT-3.5 and Claude couldn't recognize their errors.\n",
    "\n",
    "The [Reflexion paper](https://arxiv.org/abs/2303.11366) (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4's baseline 80%â€”without any weight updates. Similarly, [Self-Refine](https://arxiv.org/abs/2303.17651) showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.\n",
    "\n",
    "But there's a catch. A [comprehensive 2024 survey](https://aclanthology.org/2024.tacl-1.78/) found that pure \"intrinsic\" self-correctionâ€”where the LLM reflects with no external signalâ€”rarely helps, and can even degrade performance. What *does* work is self-correction with **reliable external feedback**: compiler errors, test results, structured rubrics.\n",
    "\n",
    "GEPA exploits exactly this. The [CRITIC paper](https://arxiv.org/abs/2305.11738) (ICLR 2024) highlights that external feedback is \"crucial\" for successful self-improvement. GEPA doesn't ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedbackâ€”the compiler said this, the profiler showed that, the judge flagged this rubricâ€”and asks it to reflect on *that*.\n",
    "\n",
    "The shift: LLMs can now *process feedback and generalize lessons* effectivelyâ€”not that they introspect perfectly.\n",
    "\n",
    "> *\"Earlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessons... But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.\"* â€” Lakshya A Agrawal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072596",
   "metadata": {},
   "source": [
    "### Optimizer Evolution: From Few-Shot to Reflection\n",
    "\n",
    "To understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessorâ€”and GEPA represents the latest capability unlock.\n",
    "\n",
    "![Optimizer Evolution](static/optimizer%20evolution.webp)\n",
    "\n",
    "#### Bootstrap Few-Shot (DSPy, 2023)\n",
    "\n",
    "The original insight: you don't need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as [few-shot demonstrations](https://www.promptingguide.ai/techniques/fewshot) for future runs. The system bootstraps its own examples from successful executions.\n",
    "\n",
    "**Example**: Your QA system correctly answers \"What's the capital of France?\" â†’ \"Paris\". That (question, answer) pair becomes a demonstration shown to the model on future queries.\n",
    "\n",
    "**Limitation**: Demonstrations are static snapshots. Once selected, they don't adapt when new failure modes emerge. And there's no instruction optimization â€” the system prompt stays identical whether you're handling edge cases or common inputs.\n",
    "\n",
    "#### OPRO (Google DeepMind, 2023)\n",
    "\n",
    "[OPRO](https://arxiv.org/abs/2309.03409) (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.\n",
    "\n",
    "**Example**: The optimizer sees:\n",
    "\n",
    "- `\"Solve the math problem step by step\"` â†’ score 0.65\n",
    "- `\"Show your work and verify the answer\"` â†’ score 0.72\n",
    "- `\"Break the problem into cases and check each\"` â†’ score 0.78\n",
    "\n",
    "It proposes: `\"Systematically enumerate cases and verify each solution\"` â†’ score 0.81\n",
    "\n",
    "**Limitation**: Score-only signal. The optimizer sees *that* `prompt_v3` scored 0.72 but not *why*. Did it make algebraic errors? Miss edge cases? The number alone doesn't say.\n",
    "\n",
    "#### MiPRO (2024)\n",
    "\n",
    "[MiPRO](https://arxiv.org/abs/2406.11695) recognized that instructions and demonstrations interactâ€”the *same* instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.\n",
    "\n",
    "**The search space problem**: Say you have 10 candidate instructions and 5 possible demo sets. That's 50 combinations. Now add instruction variants (\"Be concise\" vs \"Be brief\" vs \"Answer in one sentence\")â€”suddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.\n",
    "\n",
    "**MiPRO's solution: a cheap surrogate model**. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you *have* run. The predictor learns patterns like \"instructions mentioning 'step-by-step' tend to score higher\" or \"demos with longer reasoning traces correlate with better performance.\"\n",
    "\n",
    "The workflow:\n",
    "\n",
    "1. **Bootstrap**: Run a small random sample of combinations (say, 30 out of 500)\n",
    "2. **Train surrogate**: Fit the predictor on those 30 (instruction, demos) â†’ score pairs\n",
    "3. **Predict cheaply**: Score all 500 combinations using the surrogate (milliseconds, not dollars)\n",
    "4. **Evaluate selectively**: Only run full evaluation on the top-predicted candidates\n",
    "5. **Repeat**: Add new results to training data, retrain surrogate, sample again\n",
    "\n",
    "**Example**: After 30 random evaluations, the surrogate learns:\n",
    "\n",
    "- Instructions with \"step-by-step\" â†’ +0.08 average\n",
    "- Demo set B (which has chain-of-thought examples) â†’ +0.05 average\n",
    "- Combining both â†’ predicted 0.79\n",
    "\n",
    "MiPRO focuses budget on high-predicted combinations rather than exhaustive search.\n",
    "\n",
    "**Limitation**: The surrogate learns *correlations*, not *causation*. It knows \"step-by-step instructions score higher\" but not *why*â€”maybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt that's 0.75 on everything beats one that's 0.95 on hard cases but 0.60 overallâ€”even though that hard-case specialist might contain crucial insights.\n",
    "\n",
    "#### SIMBA (DSPy, 2024)\n",
    "\n",
    "[SIMBA](https://dspy.ai/api/optimizers/SIMBA/) (Stochastic Introspective Mini-Batch Ascent) introduced self-reflection into prompt optimization. Rather than treating prompts as black boxes with hidden payout rates, SIMBA has the LLM analyze its own performance and propose improvements.\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "1. **Sample mini-batches**: Instead of evaluating on the full dataset, SIMBA samples small batches of examples\n",
    "2. **Identify hard cases**: Track which examples show high output variability or consistent failures\n",
    "3. **Generate reflective rules**: Ask the LLM to analyze why those cases failed and propose improvement rules\n",
    "4. **Update prompts**: Incorporate the rules as new instructions, or add successful examples as demonstrations\n",
    "5. **Repeat**: Iterate with new mini-batches, accumulating insights\n",
    "\n",
    "**Example**: After several mini-batches, SIMBA notices examples 7 and 12 consistently fail. It prompts the LLM: \"These examples failed. What pattern do you see?\" The LLM reflects: \"Both involve multi-step calculations where I lost track of units.\" SIMBA adds: \"Always track units through each calculation step.\"\n",
    "\n",
    "**The key innovation**: SIMBA bridges the gap between score-only optimization (OPRO, MiPRO) and full trajectory reflection (GEPA). It uses introspection on failure patterns rather than just comparing aggregate scores.\n",
    "\n",
    "**Limitation**: SIMBA's reflection is still relatively shallowâ€”it identifies patterns across examples but doesn't deeply analyze individual execution traces. GEPA extends this by feeding the optimizer rich textual feedback (compiler errors, rubrics, reasoning traces) from each trajectory.\n",
    "\n",
    "#### GEPA (2025): The Reflection Shift\n",
    "\n",
    "GEPA breaks from this trajectory in two ways:\n",
    "\n",
    "| What changed | Before GEPA | With GEPA |\n",
    "|--------------|-------------|-----------|\n",
    "| **Learning signal** | `score = 0.6` | \"Exceeded word limit. Missing keyword. Compiler error: use y not x.\" |\n",
    "| **Selection strategy** | Best aggregate score | Pareto frontier of diverse specialists |\n",
    "\n",
    "**1. From scalar scores to textual feedback** â€” Instead of just knowing *that* a prompt scored 0.6, GEPA sees *why*: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.\n",
    "\n",
    "**Example**: OPRO sees `score = 0.6`. GEPA sees:\n",
    "> \"Failed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword 'disclaimer'. Passed examples 1-6, 8-11.\"\n",
    "\n",
    "The LLM reflects: \"I should add an instruction about word limits and required keywords.\"\n",
    "\n",
    "**2. From greedy to Pareto selection** â€” Instead of always promoting the highest-scoring candidate, GEPA maintains a *[Pareto frontier](https://en.wikipedia.org/wiki/Pareto_efficiency)*: candidates that each excel at *something* no other candidate beats. \n",
    "\n",
    "**Example**: Three candidates evaluated on 10 examples:\n",
    "\n",
    "- `prompt_A`: 8/10 overall, but fails hard cases #7 and #9\n",
    "- `prompt_B`: 6/10 overall, but nails hard cases #7 and #9  \n",
    "- `prompt_C`: 7/10 overall, no unique strengths\n",
    "\n",
    "Greedy selection keeps only `prompt_A`. Pareto selection keeps both `prompt_A` *and* `prompt_B`â€”because B's insights about hard cases might combine with A's general strength. `prompt_C` gets dropped (dominated by A on everything).\n",
    "\n",
    "The difference: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains whyâ€”and proposes the fix directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b9c6c",
   "metadata": {},
   "source": [
    "## How GEPA Works: Building It From Scratch\n",
    "\n",
    "GEPA combines two key innovations: **reflective prompt mutation** (learning from textual feedback) and **Pareto selection** (preserving diverse specialists). Each helps on its own; combined, they reinforce each other.\n",
    "\n",
    "We'll build them from scratch in this section:\n",
    "\n",
    "1. **Reflective mutation** â€” How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.\n",
    "\n",
    "2. **Pareto selection** â€” Why always improving your \"best\" prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.\n",
    "\n",
    "3. **Merge** â€” How GEPA combines insights from divergent lineages (covered after the complete algorithm).\n",
    "\n",
    "By the end, you'll see how these mechanisms combine into GEPA's full evolutionary loop.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Start: Using GEPA in 30 Seconds\n",
    "\n",
    "> ðŸ“– **Full tutorial**: [GEPA for AIME (Math)](https://dspy.ai/tutorials/gepa_aime/) â€” optimizing GPT-4.1 Mini from 46.6% â†’ 56.6% on AIME 2025.\n",
    "\n",
    "Before diving deep, here's what using GEPA looks like in practice:\n",
    "\n",
    "**Step 1: Configure your language model**\n",
    "\n",
    "```python\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, max_tokens=32000)\n",
    "dspy.configure(lm=lm)\n",
    "```\n",
    "\n",
    "**Step 2: Define your program**\n",
    "\n",
    "```python\n",
    "program = dspy.ChainOfThought(\"problem -> answer\")\n",
    "```\n",
    "\n",
    "**Step 3: Define a metric that returns feedback (not just a score)**\n",
    "\n",
    "This is the key difference from other optimizersâ€”your metric explains *why* something failed:\n",
    "\n",
    "```python\n",
    "def metric_with_feedback(example, prediction, trace=None, **kwargs):\n",
    "    correct_answer = example.answer\n",
    "    pred_answer = prediction.answer\n",
    "    \n",
    "    score = int(correct_answer == pred_answer)\n",
    "    \n",
    "    if score == 1:\n",
    "        feedback = f\"Correct! The answer is '{correct_answer}'.\"\n",
    "    else:\n",
    "        feedback = f\"Incorrect. Expected '{correct_answer}', got '{pred_answer}'.\"\n",
    "        # Add any additional context that could help improvement:\n",
    "        if hasattr(example, 'solution'):\n",
    "            feedback += f\" Solution: {example.solution}\"\n",
    "    \n",
    "    return dspy.Prediction(score=score, feedback=feedback)\n",
    "```\n",
    "\n",
    "**Step 4: Optimize with GEPA**\n",
    "\n",
    "```python\n",
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"light\",           # Budget preset: \"light\", \"medium\", or \"heavy\"\n",
    "    num_threads=32,         # Parallel evaluation threads\n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")\n",
    "```\n",
    "\n",
    "GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally. If you just want to use it, the code above is sufficientâ€”see the [DSPy GEPA API Reference](https://dspy.ai/api/optimizers/GEPA/overview/) for full parameter details.\n",
    "\n",
    "But what's actually happening? Your metric returns *why* something failed (not just a score), an LLM reads that feedback and proposes improved instructions, Pareto selection preserves diverse specialists rather than just the highest-scoring prompt, and merge operations combine insights from divergent lineages.\n",
    "\n",
    "The rest of this section builds the core mechanism from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a880cb3",
   "metadata": {},
   "source": [
    "### GEPA: REFLECTIVE PROMPT EVOLUTION (Flow diagram from paper)\n",
    "\n",
    "![GEPA Flowchart](static/GEPA_flowchart.png)\n",
    "\n",
    "The diagram above shows GEPA's loop. Each prompt is evaluated on every training task, producing a per-instance score matrix. Pareto filtering preserves prompts that excel at something no other prompt beats. New candidates come from **reflective mutation** (analyzing textual feedback) or **merge** (combining two specialists' insights). Only candidates passing a minibatch screen get full evaluation.\n",
    "\n",
    "The next section implements reflective mutation: the \"Reflect and Propose New Prompt\" step in the diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b94da1",
   "metadata": {},
   "source": [
    "### Hands-On: Building Reflective Mutation from Scratch\n",
    "\n",
    "To see how this works by implementing GEPA's core mechanism on a real task.\n",
    "\n",
    "**The Problem: AIME Math Competition**\n",
    "\n",
    "We'll optimize prompts for solving [AIME](https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination) (American Invitational Mathematics Examination) problems â€” challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dset = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n",
    "# 90 problems with solutions and integer answers\n",
    "```\n",
    "\n",
    "| problem | solution | answer |\n",
    "|---------|----------|--------|\n",
    "| Quadratic polynomials $P(x)$ and $Q(x)$ have l... | Let $R(x)=P(x)+Q(x).$ Since the $x^2$-terms of... | 116 |\n",
    "| Three spheres with radii $11$, $13$, and $19$ ... | This solution refers to the Diagram section... | 756 |\n",
    "\n",
    "**Why AIME for testing prompt optimization?**\n",
    "\n",
    "1. **Clear ground truth** â€” Every answer is an integer (0-999), so evaluation is unambiguous\n",
    "2. **Rich failure modes** â€” Wrong answers come from algebraic errors, missed cases, misread constraints\n",
    "3. **Domain knowledge helps** â€” Prompts that encode strategies (\"subtract equations pairwise\", \"enumerate all cases\") measurably improve performance\n",
    "4. **Small dataset** â€” Only 90 problems, so sample efficiency matters\n",
    "\n",
    "**The Setup**\n",
    "\n",
    "```python\n",
    "# Split: 10 train, 10 validation (simulating scarce labeled data)\n",
    "tdf = df.sample(45).iloc[:10]  # Training mini-batches drawn from here\n",
    "vdf = df.drop(tdf.index).iloc[:10]  # Held-out validation\n",
    "\n",
    "# Base model: Gemini 2.5 Flash via LiteLLM\n",
    "# Metric: Exact match (predicted integer == ground truth)\n",
    "def metric(ground_truth, prediction):\n",
    "    return int(ground_truth) == prediction['answer']\n",
    "```\n",
    "\n",
    "**Seed Prompt**\n",
    "\n",
    "We start with a minimal instruction:\n",
    "\n",
    "```python\n",
    "seed_prompt = \"\"\"You are given a problem and you have to give the answer \n",
    "along with reasoning. Do not return anything apart from json. \n",
    "It should be parsable by json.loads()\"\"\"\n",
    "```\n",
    "\n",
    "Baseline validation accuracy: **10%** (1/10 correct)\n",
    "\n",
    "Can reflective mutation improve this? Let's find out.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: The Feedback Function\n",
    "\n",
    "First, we need a function that tells the reflection LLM what went wrong. We'll start with **minimal feedback**â€”just the correct answer:\n",
    "\n",
    "```python\n",
    "def feedback(ground_truth, prediction):\n",
    "    if int(ground_truth) != prediction['answer']:\n",
    "        return f'You got it wrong! The solution is {ground_truth}'\n",
    "    return 'You got it right!'\n",
    "```\n",
    "\n",
    "This is deliberately simple. Later we'll discuss how richer feedback (like expert solutions) can improve results.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: The Reflection Prompt\n",
    "\n",
    "Following GEPA's structure, we build a prompt that shows the LLM its failures:\n",
    "\n",
    "```python\n",
    "REFLECTION_TEMPLATE = \"\"\"I provided an assistant with the following instructions:\n",
    "<curr_instructions>\n",
    "{current_prompt}\n",
    "\n",
    "The following are examples with assistant's responses and feedback:\n",
    "<inputs_outputs_feedback>\n",
    "{examples}\n",
    "\n",
    "Your task: write a new instruction for the assistant.\n",
    "\n",
    "- Read inputs carefully and identify the input format and task description\n",
    "- Read all responses and feedback. Identify niche/domain-specific factual information\n",
    "- If the assistant used a generalizable strategy, include that in the instruction\n",
    "\n",
    "Provide the new instructions.\n",
    "\"\"\"\n",
    "\n",
    "def mk_reflection_prompt(df, curr_prompt):\n",
    "    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n",
    "    examples = []\n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        example = f\"\"\"# Example {i+1}\n",
    "## problem\n",
    "{row['problem']}\n",
    "## prediction\n",
    "{row['pred']}\n",
    "## feedback\n",
    "{feedback(row.answer, row.pred)}\n",
    "\"\"\"\n",
    "        examples.append(example)\n",
    "    \n",
    "    return REFLECTION_TEMPLATE.format(\n",
    "        current_prompt=curr_prompt,\n",
    "        examples=\"\\n\".join(examples)\n",
    "    )\n",
    "```\n",
    "\n",
    "**Example filled-in reflection prompt:**\n",
    "\n",
    "```\n",
    "I provided an assistant with the following instructions:\n",
    "<curr_instructions>\n",
    "You are given a problem and you have to give the answer along with reasoning. \n",
    "Do not return anything apart from json. It should be parsable by json.loads()\n",
    "\n",
    "The following are examples with assistant's responses and feedback:\n",
    "<inputs_outputs_feedback>\n",
    "# Example 1\n",
    "## problem\n",
    "Quadratic polynomials P(x) and Q(x) have leading coefficient 1. The sum of the roots of P(x) is 7...\n",
    "## prediction\n",
    "{\"answer\": 42, \"reasoning\": \"I solved the system and got x=7, y=6\"}\n",
    "## feedback\n",
    "You got it wrong! The solution is 116\n",
    "\n",
    "# Example 2\n",
    "## problem\n",
    "Three spheres with radii 11, 13, and 19 are mutually externally tangent...\n",
    "## prediction\n",
    "{\"answer\": 756, \"reasoning\": \"Using the tangent sphere formula...\"}\n",
    "## feedback\n",
    "You got it right!\n",
    "\n",
    "# Example 3\n",
    "## problem\n",
    "Find the remainder when 2^2024 is divided by 1000...\n",
    "## prediction\n",
    "{\"answer\": 16, \"reasoning\": \"I computed powers of 2 mod 1000...\"}\n",
    "## feedback\n",
    "You got it wrong! The solution is 896\n",
    "\n",
    "Your task: write a new instruction for the assistant.\n",
    "\n",
    "- Read inputs carefully and identify the input format and task description\n",
    "- Read all responses and feedback. Identify niche/domain-specific factual information\n",
    "- If the assistant used a generalizable strategy, include that in the instruction\n",
    "\n",
    "Provide the new instructions.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: The Complete Optimization Loop\n",
    "\n",
    "```python\n",
    "def mk_reflection_prompt(mb, curr_prompt):\n",
    "    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n",
    "    examples = []\n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        example = f\"\"\"# Example {i+1}\n",
    "## problem\n",
    "{row['problem']}\n",
    "## prediction\n",
    "{row['pred']}\n",
    "## feedback\n",
    "{feedback(row.answer, row.pred)}\n",
    "\"\"\"\n",
    "        examples.append(example)\n",
    "    \n",
    "    return REFLECTION_TEMPLATE.format(\n",
    "        current_prompt=curr_prompt,\n",
    "        examples=\"\\n\".join(examples)\n",
    "    )\n",
    "\n",
    "def reflect(mb, curr_prompt):\n",
    "    \"\"\"Ask LLM to reflect on failures and propose improved instruction.\"\"\"\n",
    "    refl_prompt = mk_reflection_prompt(mb, curr_prompt)\n",
    "    return _call(refl_prompt, format=ReflectionModel)['new_instruction']\n",
    "\n",
    "def optimize_prompt(seed_prompt, traindf, valdf, n_iters=3, mb_size=3):\n",
    "    \"\"\"Greedy reflective prompt optimization.\"\"\"\n",
    "    prompts, train_scores, val_scores = [seed_prompt], [], []\n",
    "    mb = traindf.sample(mb_size)  # Fixed minibatch for this run\n",
    "    \n",
    "    print(f'Baseline validation: {eval_val(seed_prompt, valdf):.2%}')\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        # Evaluate current prompt on minibatch\n",
    "        mb_eval, mb_score = eval_mb(prompts[-1], mb)\n",
    "        print(f\"ðŸ“Š Minibatch: {mb_score:.2%}\")\n",
    "        \n",
    "        # Reflect and propose new instruction\n",
    "        new_instr = reflect(mb_eval, prompts[-1])\n",
    "        new_prompt = new_instr  # The new instruction becomes the new prompt\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_score = eval_val(new_prompt, valdf)\n",
    "        print(f\"ðŸ“Š Validation: {val_score:.2%}\")\n",
    "        \n",
    "        prompts.append(new_prompt)\n",
    "        val_scores.append(val_score)\n",
    "    \n",
    "    return dict(prompts=prompts, val_scores=val_scores)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What Actually Happened\n",
    "\n",
    "Running this on AIME problems with Gemini 2.5 Flash:\n",
    "\n",
    "| Iteration | Minibatch | Validation | What the reflection learned |\n",
    "|-----------|-----------|------------|----------------------------|\n",
    "| Baseline  | â€”         | 10%        | â€” |\n",
    "| 1         | 0%        | 10%        | JSON formatting details, output structure rules |\n",
    "| 2         | 0%        | **30%**    | Systems of equations strategy, remainder/modular arithmetic tips |\n",
    "| 3         | 67%       | **10%**    | Over-specialized on number theory, solved #9 but lost generality |\n",
    "\n",
    "**The good**: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:\n",
    "\n",
    "> *\"When dealing with systems of equations like $xy + Az = C$, $yz + Ax = C$, $zx + Ay = C$, consider subtracting equations pairwise to find relationships between variables, such as $(x-z)(y-A)=0$, which implies $x=z$ or $y=A$. Systematically explore all such cases.\"*\n",
    "\n",
    "This is directly from the actual outputâ€”the reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.\n",
    "\n",
    "**The bad**: Iteration 3 achieved 67% on its minibatch but dropped to **10% validation**. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:\n",
    "\n",
    "> *\"When the problem involves number theory and remainders (e.g., $n \\pmod x$, $n \\pmod y$), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.\"*\n",
    "\n",
    "This over-specialized advice (\"can lead to an answer of 0\") actively hurt performance on non-number-theory problems, dropping validation from 30% back to **10%** (1/10)â€”though notably, it did solve problem #9, which earlier prompts couldn't.\n",
    "\n",
    "---\n",
    "\n",
    "### The Greedy Selection Problem\n",
    "\n",
    "This demonstrates exactly why GEPA uses **Pareto selection** instead of always taking the \"best\" prompt:\n",
    "\n",
    "1. **Iteration 2's prompt** was a specialistâ€”it learned something valuable about systems of equations\n",
    "2. **Iteration 3** tried to improve on iteration 2, but the minibatch had different problems\n",
    "3. The reflection **overwrote** the systems-of-equations insight while adding number-theory tips that were *too specific*\n",
    "4. Result: **catastrophic forgetting**\n",
    "\n",
    "With greedy selection, we would have discarded iteration 2's valuable insight. Pareto selection would keep itâ€”because it was *best on at least one validation instance*.\n",
    "\n",
    "---\n",
    "\n",
    "### The Missing Ingredient: Rich Feedback\n",
    "\n",
    "Our minimal feedback (`\"You got it wrong! The solution is 349\"`) only tells the model *that* it failed, not *why* or *how to fix it*.\n",
    "\n",
    "The AIME dataset includes expert solutions. A richer feedback function could use them:\n",
    "\n",
    "```python\n",
    "def feedback_rich(row):\n",
    "    if int(row.answer) != row.pred['answer']:\n",
    "        sol = row.solution[:500] + \"...\" if len(row.solution) > 500 else row.solution\n",
    "        return f\"\"\"Wrong! Expected {row.answer}, got {row.pred['answer']}.\n",
    "        \n",
    "Model's reasoning: {row.pred['short_reasoning']}\n",
    "\n",
    "Expert solution approach:\n",
    "{sol}\"\"\"\n",
    "    return \"Correct!\"\n",
    "```\n",
    "\n",
    "**Example output** for a wrong answer:\n",
    "\n",
    "```\n",
    "Wrong! Expected 116, got 42.\n",
    "\n",
    "Model's reasoning: I set up the system of equations and solved for x=7, y=6, giving 7*6=42.\n",
    "\n",
    "Expert solution approach:\n",
    "Let R(x)=P(x)+Q(x). Since the xÂ²-terms of P and Q have leading coefficient 1, \n",
    "R(x) is quadratic with leading coefficient 2. Given the roots condition, we can \n",
    "write R(x) = 2(x-râ‚)(x-râ‚‚). Expanding and comparing coefficients...\n",
    "```\n",
    "\n",
    "With rich feedback, the reflection LLM can extract *specific strategies* from the expert solution rather than having to guess what went wrong. This is what makes GEPA sample-efficient: **the feedback contains the fix**.\n",
    "\n",
    "Compare this to RL, which would only see `reward = 0` and have to statistically infer what went wrong across thousands of trajectories.\n",
    "\n",
    "---\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Reflective mutation works** â€” Even with minimal feedback, the LLM extracted useful domain knowledge\n",
    "2. **Greedy selection fails** â€” Iteration 3's collapse shows why we need to preserve specialist insights\n",
    "3. **Feedback quality matters** â€” Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with\n",
    "4. **Sample efficiency is real** â€” We saw meaningful optimization with just 3 iterations on 3 examples each\n",
    "\n",
    "This is the core limitation of greedy optimization: **catastrophic forgetting**. The solution? Pareto selectionâ€”which we'll build from scratch next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2065c1",
   "metadata": {},
   "source": [
    "### Hands-On: Building the Pareto Frontier\n",
    "\n",
    "In the reflective mutation section, we saw greedy selection failâ€”iteration 3's over-specialized prompt dropped validation from 30% to 10%, losing iteration 2's valuable systems-of-equations insights. The fundamental problem: always improving your \"best\" prompt discards specialist knowledge.\n",
    "\n",
    "GEPA's solution: **Pareto selection**. Instead of keeping one best prompt, maintain a *frontier* of prompts where each excels at something no other prompt beats.\n",
    "\n",
    "---\n",
    "\n",
    "#### What is Pareto Dominance?\n",
    "\n",
    "A prompt **dominates** another if it's at least as good everywhere, and strictly better somewhere:\n",
    "\n",
    "- **â‰¥** on *every* validation instance, AND  \n",
    "- **>** on *at least one* instance\n",
    "\n",
    "If prompt A dominates prompt B, we can safely discard Bâ€”A is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.\n",
    "\n",
    "**Example**: Consider four prompts evaluated on 10 validation instances. We'll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:\n",
    "\n",
    "| Prompt | Instances Solved | Aggregate | Status |\n",
    "|--------|------------------|-----------|--------|\n",
    "| P0 (seed) | #0 only | 10% | Dominated by P2 |\n",
    "| P1 (iter 1) | #0 only | 10% | Dominated by P2 |\n",
    "| P2 (iter 2) | #0, #1, #2 | 30% | **Frontier** âœ“ |\n",
    "| P3 (iter 3) | #9 only | 10% | **Frontier** âœ“ |\n",
    "\n",
    "P2 dominates both P0 and P1â€”it solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which *nothing else could*. \n",
    "\n",
    "The Pareto frontier is **{P2, P3}**. Both contain unique value.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementation: Dominance Checking\n",
    "\n",
    "We represent per-instance scores as boolean arrays (1 = solved, 0 = failed):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def dominates(candidate_scores, other_scores):\n",
    "    \"\"\"Does candidate dominate other? (>= everywhere, > somewhere)\"\"\"\n",
    "    candidate = np.array(candidate_scores)\n",
    "    other = np.array(other_scores)\n",
    "    return (candidate >= other).all() and (candidate > other).any()\n",
    "\n",
    "def is_dominated_by_any(new_scores, frontier_scores):\n",
    "    \"\"\"Is new_scores dominated by ANY prompt in the frontier?\"\"\"\n",
    "    new = np.array(new_scores)\n",
    "    for existing in frontier_scores:\n",
    "        if dominates(np.array(existing), new):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_dominated_indices(new_scores, frontier_scores):\n",
    "    \"\"\"Which frontier prompts does new_scores dominate?\"\"\"\n",
    "    new = np.array(new_scores)\n",
    "    return [i for i, existing in enumerate(frontier_scores) \n",
    "            if dominates(new, np.array(existing))]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Tracing Through: Why P3 Survives\n",
    "\n",
    "Let's verify the dominance relationships from our example:\n",
    "\n",
    "```python\n",
    "P0 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\n",
    "P1 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\n",
    "P2 = [1,1,1,0,0,0,0,0,0,0]  # Solves: #0, #1, #2\n",
    "P3 = [0,0,0,0,0,0,0,0,0,1]  # Solves: #9\n",
    "\n",
    "# Does P2 dominate P0?\n",
    "dominates(P2, P0)  # True: P2 >= P0 everywhere, P2 > P0 on #1, #2\n",
    "\n",
    "# Does P2 dominate P1?\n",
    "dominates(P2, P1)  # True: P2 >= P1 everywhere, P2 > P1 on #1, #2\n",
    "\n",
    "# Does P2 dominate P3?\n",
    "dominates(P2, P3)  # False! P2 loses on #9 (0 < 1)\n",
    "\n",
    "# Does P3 dominate P2?\n",
    "dominates(P3, P2)  # False! P3 loses on #0, #1, #2\n",
    "```\n",
    "\n",
    "Neither P2 nor P3 dominates the otherâ€”they're **Pareto incomparable**. Each solves problems the other can't. Both stay on the frontier.\n",
    "\n",
    "![Pareto Frontier Explained](static/pareto_visual.webp)\n",
    "\n",
    "---\n",
    "\n",
    "#### The Complete Frontier Manager\n",
    "\n",
    "```python\n",
    "class ParetoFrontier:\n",
    "    def __init__(self):\n",
    "        self.prompts = []\n",
    "        self.scores = []  # scores[i][j] = prompt i's score on instance j\n",
    "    \n",
    "    def add(self, prompt, instance_scores):\n",
    "        \"\"\"Try to add a prompt. Returns True if it joins the frontier.\"\"\"\n",
    "        # Reject if dominated by existing frontier member\n",
    "        if is_dominated_by_any(instance_scores, self.scores):\n",
    "            return False\n",
    "        \n",
    "        # Remove any frontier members this prompt dominates\n",
    "        dominated = get_dominated_indices(instance_scores, self.scores)\n",
    "        for i in sorted(dominated, reverse=True):  # Remove from end first\n",
    "            del self.prompts[i]\n",
    "            del self.scores[i]\n",
    "        \n",
    "        # Add to frontier\n",
    "        self.prompts.append(prompt)\n",
    "        self.scores.append(instance_scores)\n",
    "        return True\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Sample a prompt, weighted by unique wins.\"\"\"\n",
    "        weights = []\n",
    "        scores_arr = np.array(self.scores)\n",
    "        for i in range(len(self.prompts)):\n",
    "            # How many instances is this prompt *uniquely* best on?\n",
    "            others_best = np.delete(scores_arr, i, axis=0).max(axis=0) if len(self.prompts) > 1 else np.zeros_like(scores_arr[i])\n",
    "            unique_wins = (scores_arr[i] > others_best).sum()\n",
    "            weights.append(unique_wins + 1)  # +1 smoothing\n",
    "        \n",
    "        return np.random.choice(self.prompts, p=np.array(weights)/sum(weights))\n",
    "    \n",
    "    def best_aggregate(self):\n",
    "        \"\"\"Return the prompt with highest aggregate score.\"\"\"\n",
    "        aggregates = [sum(s) for s in self.scores]\n",
    "        return self.prompts[np.argmax(aggregates)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Putting It Together: Pareto-Guided Optimization\n",
    "\n",
    "```python\n",
    "def optimize_with_pareto(seed_prompt, traindf, valdf, n_iters=5, mb_size=3):\n",
    "    \"\"\"Reflective mutation with Pareto frontier selection.\"\"\"\n",
    "    frontier = ParetoFrontier()\n",
    "    \n",
    "    # Initialize with seed\n",
    "    seed_scores = evaluate_per_instance(seed_prompt, valdf)\n",
    "    frontier.add(seed_prompt, seed_scores)\n",
    "    print(f'Baseline: {sum(seed_scores)/len(seed_scores):.1%}')\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        print(f\"\\n{'='*40}\\nIteration {i+1}\\n{'='*40}\")\n",
    "        \n",
    "        # Sample parent from frontier (weighted by unique wins)\n",
    "        parent = frontier.sample()\n",
    "        \n",
    "        # Run on minibatch, reflect, propose mutation\n",
    "        mb = traindf.sample(mb_size)\n",
    "        mb_results = evaluate_with_traces(parent, mb)\n",
    "        new_prompt = reflect_and_mutate(parent, mb_results)\n",
    "        \n",
    "        # Evaluate on full validation set\n",
    "        new_scores = evaluate_per_instance(new_prompt, valdf)\n",
    "        new_agg = sum(new_scores) / len(new_scores)\n",
    "        print(f\"New prompt: {new_agg:.1%} aggregate\")\n",
    "        \n",
    "        # Try to add to frontier\n",
    "        if frontier.add(new_prompt, new_scores):\n",
    "            print(f\"âœ“ Added to frontier (size: {len(frontier.prompts)})\")\n",
    "        else:\n",
    "            print(f\"âœ— Dominated, rejected\")\n",
    "    \n",
    "    return frontier.best_aggregate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### What We Observed on AIME\n",
    "\n",
    "Running this on AIME problems with Gemini 2.5 Flash:\n",
    "\n",
    "| Iteration | Aggregate | Instances Solved | Frontier Action |\n",
    "|-----------|-----------|------------------|-----------------|\n",
    "| Seed | 10% | #0 | Initialize |\n",
    "| 1 | 10% | #0 | Dominated by seed, rejected |\n",
    "| 2 | 30% | #0, #1, #2 | Added, dominates seed |\n",
    "| 3 | 10% | #9 only | **Added** âœ“ (unique win on #9) |\n",
    "\n",
    "**The key moment**: Iteration 3 scored only 10%â€”worse than iteration 2's 30%. Greedy selection would discard it entirely.\n",
    "\n",
    "But it solved **instance #9**, which nothing else could. Pareto selection preserves it.\n",
    "\n",
    "Our final frontier: **{P2, P3}**\n",
    "\n",
    "- P2: Strong generalist (30%), knows systems-of-equations strategies\n",
    "- P3: Instance-9 specialist (10%), knows whatever cracked that specific problem\n",
    "\n",
    "Both insights survive. The merge operation (covered later) can combine them.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters: No More Catastrophic Forgetting\n",
    "\n",
    "| Selection Strategy | What happens to specialists |\n",
    "|-------------------|----------------------------|\n",
    "| **Greedy** | Discarded whenever aggregate score drops |\n",
    "| **Pareto** | Preserved if they solve *anything* unique |\n",
    "\n",
    "Greedy selection caused our iteration 3 collapseâ€”the number-theory prompt overwrote the algebra prompt's insights. Pareto selection prevents this by construction: you can't remove a prompt from the frontier unless something else does everything it does, *plus more*.\n",
    "\n",
    "This drives GEPA's sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier retains them automatically.\n",
    "\n",
    "---\n",
    "\n",
    "Pareto selection preserved iteration 3's prompt despite its 10% aggregate scoreâ€”because it solved instance #9, which nothing else could. The intuition for why keeping \"losers\" helps comes from **quality-diversity** algorithms in evolutionary computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476efebe",
   "metadata": {},
   "source": [
    "### Why Pareto Works: Quality-Diversity and Map Elites\n",
    "\n",
    "We've now built both core mechanisms from scratchâ€”reflective mutation and Pareto selection. Before diving into the full algorithm, let's step back and understand *why* this approach works so well.\n",
    "\n",
    "The Pareto frontier isn't a novel inventionâ€”it draws from **quality-diversity (QD)** algorithms, a family of techniques from evolutionary computation that work well when diversity itself is valuable.\n",
    "\n",
    "Traditional optimization asks: *\"What's the single best solution?\"*\n",
    "\n",
    "Quality-diversity asks: *\"What's the best solution of each *type*?\"*\n",
    "\n",
    "Complex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensivelyâ€”but the best engine knows *both* styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one \"best\" approach.\n",
    "\n",
    "#### Map Elites\n",
    "\n",
    "[Map Elites](https://arxiv.org/abs/1504.04909) (Mouret & Clune, 2015) maintains an *archive* organized by behavior:\n",
    "\n",
    "1. **Define behavior dimensions** â€” characteristics describing *how* a solution works (not just how well)\n",
    "2. **Discretize into bins** â€” each cell represents a \"niche\"\n",
    "3. **Keep the best per bin** â€” new solutions compete only within their niche\n",
    "4. **Mutate from the archive** â€” sample from any occupied bin, mutate, place in appropriate bin\n",
    "\n",
    "The result: diverse specialists, each optimal *of its type*. The archive provides *stepping stones*: a mutation from one niche might discover something useful for another. Diversity doubles as a search strategy.\n",
    "\n",
    "#### GEPA's Adaptation: Validation Instances as Niches\n",
    "\n",
    "GEPA recognizes that **the validation set itself defines the behavior space**:\n",
    "\n",
    "| Map Elites | GEPA |\n",
    "|------------|------|\n",
    "| Behavior = continuous dimensions | Behavior = which validation instances are solved |\n",
    "| Bins = discretized regions | \"Bins\" = individual validation instances |\n",
    "| Archive = best per bin | Pareto frontier = non-dominated prompts across instances |\n",
    "\n",
    "Each validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the \"specialist for that niche\"â€”it stays because it demonstrates *something works*, even with low aggregate score.\n",
    "\n",
    "> *\"By tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "This motivated GEPA's design: Pareto selection over per-instance scores naturally implements QD's \"best per niche\" principle, while the merge operation recombines insights across nichesâ€”exactly what stepping-stone search requires.\n",
    "\n",
    "With both mechanisms in place, there's one more operation that makes GEPA powerful: **merge**â€”combining insights from divergent lineages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c5ff9",
   "metadata": {},
   "source": [
    "### The Lineage Tree and System-Aware Merge\n",
    "\n",
    "Pareto selection preserves specialistsâ€”but it creates a new problem: insights get siloed in separate branches.\n",
    "\n",
    "Consider what happens after 10 iterations of GEPA on AIME problems:\n",
    "\n",
    "![Lineage Tree with Merge Operation](static/Merge_Visual.webp)\n",
    "\n",
    "The P2â†’P4 lineage accumulated **algebra insights**. The P3â†’P5 lineage accumulated **number theory insights**. Both survive on the Pareto frontier because each solves problems the other can't.\n",
    "\n",
    "But what about a problem requiring *both*?\n",
    "\n",
    "---\n",
    "\n",
    "#### The Recombination Problem\n",
    "\n",
    "Suppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:\n",
    "\n",
    "- P4 knows to subtract equations pairwise, but doesn't think to reduce mod p\n",
    "- P5 knows CRT, but misses the algebraic setup\n",
    "\n",
    "With mutation alone, P4 would need to *independently rediscover* number theory (which P5 already knows), or vice versa. The knowledge exists in our populationâ€”just in different branches.\n",
    "\n",
    "Merge addresses this by combining insights from divergent lineages into a single candidate.\n",
    "\n",
    "---\n",
    "\n",
    "#### How Merge Works\n",
    "\n",
    "GEPA's merge is \"system-aware\"â€”the LLM understands what it's combining, so it can resolve contradictions and synthesize coherently rather than blindly concatenating (unlike genetic algorithm crossover which swaps segments randomly). The reflection LLM receives:\n",
    "\n",
    "1. **Both parent prompts** with their full instruction text\n",
    "2. **Lineage context** â€” what types of problems each lineage solved\n",
    "3. **Conflict guidance** â€” instructions to resolve contradictions, not ignore them\n",
    "\n",
    "The prompt asks the LLM to *synthesize*, not concatenate:\n",
    "\n",
    "> \"Create a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don't simply concatenateâ€”synthesize into coherent guidance.\"\n",
    "\n",
    "**Concrete example** â€” merging our AIME specialists:\n",
    "\n",
    "| Parent | Specialty | Key instruction |\n",
    "|--------|-----------|-----------------|\n",
    "| P4 | Algebra | \"Subtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.\" |\n",
    "| P5 | Number theory | \"Apply CRT for modular constraints. Check for contradictions.\" |\n",
    "\n",
    "**Merged offspring P6:**\n",
    "\n",
    "> \"Approach: (1) For equation systems, subtract pairwise to expose factor relationshipsâ€”enumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.\"\n",
    "\n",
    "P6 inherits both toolkits and adds meta-knowledge about when to combine them. This is the \"system-aware\" partâ€”the merge understands the semantics of what it's combining.\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Merge vs. Mutate\n",
    "\n",
    "GEPA alternates between operations based on frontier state:\n",
    "\n",
    "| Condition | Operation | Rationale |\n",
    "|-----------|-----------|-----------|\n",
    "| Early iterations (< 5) | Mutate | Let lineages diverge first; nothing to merge yet |\n",
    "| Frontier has one dominant lineage | Mutate | No orthogonal insights to combine |\n",
    "| Frontier has divergent specialists | Merge | Recombine discoveries from parallel explorations |\n",
    "| Recent merge succeeded | Mutate | Refine the merged candidate |\n",
    "\n",
    "The paper describes the decision:\n",
    "\n",
    "> *\"As we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "---\n",
    "\n",
    "#### Merge vs. Genetic Algorithm Crossover\n",
    "\n",
    "GEPA's merge is analogous to crossover in genetic algorithms, but smarter:\n",
    "\n",
    "| Genetic Algorithms | GEPA Merge |\n",
    "|--------------------|------------|\n",
    "| Genes = bit positions | Insights = natural language instructions |\n",
    "| Crossover = swap bit segments randomly | Merge = LLM synthesizes with understanding |\n",
    "| Can create invalid offspring | Can resolve contradictions |\n",
    "| Blind to semantics | Aware of what instructions *mean* |\n",
    "\n",
    "Random crossover might produce: \"Subtract equations pairwise. Apply CRT. Subtract equations pairwise.\" (nonsense duplication)\n",
    "\n",
    "LLM merge produces: \"Set up algebra first, then apply number-theoretic constraints.\" (coherent synthesis)\n",
    "\n",
    "---\n",
    "\n",
    "#### How the Pieces Fit\n",
    "\n",
    "Pareto selection preserves the diversity that makes merge valuable in the first place. Lineage tracking tells us which candidates come from divergent branches. Merge recombines their discoveries, and then Pareto selection preserves successful merges alongside the remaining specialists.\n",
    "\n",
    "If you only kept one \"best\" prompt, there'd be nothing interesting to merge. And without merge, insights stay siloed even when the frontier is diverse.\n",
    "\n",
    "> **Implementation note**: The full GEPA implementation includes safeguards to ensure merge candidates actually have different insights worth combining (checking for common ancestors, avoiding redundant merges, verifying that descendants improved on their ancestor). See the [DSPy source](https://github.com/stanfordnlp/dspy) for details.\n",
    "\n",
    "Mutation refines a single lineage through reflection. Merge recombines what parallel lineages discovered. Pareto selection preserves both.\n",
    "\n",
    "With reflective mutation, Pareto selection, and merge all in place, here's how they combine into GEPA's full optimization loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ae8c8",
   "metadata": {},
   "source": [
    "### The Complete Algorithm\n",
    "\n",
    "Here's how the pieces combine into the full optimization loop.\n",
    "\n",
    "#### Algorithm Overview\n",
    "\n",
    "```python\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "def gepa(\n",
    "    base_prompt: str,\n",
    "    trainset: list,\n",
    "    valset: list,\n",
    "    evaluate_fn: Callable,          # (prompt, example) -> score (0 or 1)\n",
    "    run_with_feedback_fn: Callable, # (prompt, examples) -> (traces: list[str], feedback: list[str])\n",
    "    reflect_fn: Callable,           # (parent_prompt, traces, feedback) -> new_prompt: str\n",
    "    merge_fn: Callable,             # (prompt1, prompt2) -> merged_prompt: str\n",
    "    max_iterations: int = 20,\n",
    "    minibatch_size: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    GEPA: Genetic-Pareto prompt optimization.\n",
    "    \n",
    "    Returns the best aggregate prompt; access full frontier via returned dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Helper functions ---\n",
    "    \n",
    "    def evaluate_all(prompt, dataset):\n",
    "        \"\"\"Return per-instance scores as list.\"\"\"\n",
    "        return [evaluate_fn(prompt, ex) for ex in dataset]\n",
    "    \n",
    "    def evaluate_minibatch(prompt, minibatch):\n",
    "        \"\"\"Return aggregate score on minibatch.\"\"\"\n",
    "        return sum(evaluate_fn(prompt, ex) for ex in minibatch) / len(minibatch)\n",
    "    \n",
    "    def dominates(scores_a, scores_b):\n",
    "        \"\"\"Does A dominate B? (>= everywhere, > somewhere)\"\"\"\n",
    "        a, b = np.array(scores_a), np.array(scores_b)\n",
    "        return (a >= b).all() and (a > b).any()\n",
    "    \n",
    "    def is_dominated_by_frontier(new_scores, frontier, scores):\n",
    "        \"\"\"Is new_scores dominated by ANY frontier member?\"\"\"\n",
    "        return any(dominates(scores[c], new_scores) for c in frontier)\n",
    "    \n",
    "    def sample_from_frontier(frontier, scores):\n",
    "        \"\"\"Sample weighted by unique wins.\"\"\"\n",
    "        n_instances = len(next(iter(scores.values())))\n",
    "        weights = []\n",
    "        for candidate in frontier:\n",
    "            if len(frontier) == 1:\n",
    "                unique_wins = n_instances\n",
    "            else:\n",
    "                others_max = np.array([scores[o] for o in frontier if o != candidate]).max(axis=0)\n",
    "                unique_wins = (np.array(scores[candidate]) > others_max).sum()\n",
    "            weights.append(unique_wins + 1)  # +1 smoothing\n",
    "        return random.choices(frontier, weights=weights)[0]\n",
    "    \n",
    "    def get_root(prompt, lineage):\n",
    "        \"\"\"Trace lineage back to root.\"\"\"\n",
    "        while lineage.get(prompt) is not None:\n",
    "            prompt = lineage[prompt]\n",
    "        return prompt\n",
    "    \n",
    "    def should_merge(frontier, lineage, iteration):\n",
    "        \"\"\"Decide whether to merge or mutate.\"\"\"\n",
    "        if len(frontier) < 2 or iteration < 5:\n",
    "            return False\n",
    "        n_lineages = len(set(get_root(c, lineage) for c in frontier))\n",
    "        return random.random() < (n_lineages - 1) / len(frontier)\n",
    "    \n",
    "    def select_divergent(frontier, parent, lineage):\n",
    "        \"\"\"Select a candidate from a different lineage.\"\"\"\n",
    "        parent_root = get_root(parent, lineage)\n",
    "        others = [c for c in frontier if get_root(c, lineage) != parent_root]\n",
    "        return random.choice(others) if others else random.choice(frontier)\n",
    "    \n",
    "    def best_aggregate(frontier, scores):\n",
    "        \"\"\"Return prompt with highest aggregate score.\"\"\"\n",
    "        return max(frontier, key=lambda c: sum(scores[c]))\n",
    "    \n",
    "    # --- Main loop ---\n",
    "    \n",
    "    candidates = [base_prompt]\n",
    "    scores = {base_prompt: evaluate_all(base_prompt, valset)}\n",
    "    pareto_frontier = [base_prompt]\n",
    "    lineage = {base_prompt: None}\n",
    "    \n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        \n",
    "        # 1. SAMPLE: Select parent from Pareto frontier\n",
    "        parent = sample_from_frontier(pareto_frontier, scores)\n",
    "        \n",
    "        # 2. PROPOSE: Either mutate or merge\n",
    "        minibatch = random.sample(trainset, min(minibatch_size, len(trainset)))\n",
    "        \n",
    "        if should_merge(pareto_frontier, lineage, iteration):\n",
    "            other_parent = select_divergent(pareto_frontier, parent, lineage)\n",
    "            new_prompt = merge_fn(parent, other_parent)\n",
    "        else:\n",
    "            traces, feedback = run_with_feedback_fn(parent, minibatch)\n",
    "            new_prompt = reflect_fn(parent, traces, feedback)\n",
    "        \n",
    "        # 3. EVALUATE: Mini-batch gate\n",
    "        parent_mb_score = evaluate_minibatch(parent, minibatch)\n",
    "        new_mb_score = evaluate_minibatch(new_prompt, minibatch)\n",
    "        if new_mb_score <= parent_mb_score:\n",
    "            continue  # Reject: didn't improve on mini-batch\n",
    "        \n",
    "        # Full evaluation\n",
    "        new_scores = evaluate_all(new_prompt, valset)\n",
    "        \n",
    "        # 4. UPDATE: Pareto frontier maintenance\n",
    "        if is_dominated_by_frontier(new_scores, pareto_frontier, scores):\n",
    "            continue  # Reject: dominated by existing candidate\n",
    "        \n",
    "        # Remove dominated candidates\n",
    "        pareto_frontier = [c for c in pareto_frontier \n",
    "                          if not dominates(new_scores, scores[c])]\n",
    "        \n",
    "        # Add new candidate\n",
    "        candidates.append(new_prompt)\n",
    "        scores[new_prompt] = new_scores\n",
    "        pareto_frontier.append(new_prompt)\n",
    "        lineage[new_prompt] = parent\n",
    "    \n",
    "    return {\n",
    "        'best': best_aggregate(pareto_frontier, scores),\n",
    "        'frontier': pareto_frontier,\n",
    "        'scores': scores,\n",
    "        'lineage': lineage,\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### The Key Decision Points\n",
    "\n",
    "**1. Candidate Sampling** â€” Weighted by unique wins, so specialists get attention proportional to their unique value.\n",
    "\n",
    "**2. Mutation vs Merge** â€” Early iterations favor mutation; merge probability increases as frontier diversifies.\n",
    "\n",
    "**3. Mini-Batch Gating**\n",
    "\n",
    "Before expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:\n",
    "\n",
    "> *\"We propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "**4. Pareto Update**\n",
    "\n",
    "The frontier update follows the dominance logic we implemented earlier:\n",
    "\n",
    "- **Reject** new candidates dominated by existing ones (they add nothing)\n",
    "- **Remove** existing candidates dominated by the new one (they're obsolete)\n",
    "- **Keep** all non-dominated candidates (each offers unique value)\n",
    "\n",
    "---\n",
    "\n",
    "#### Complexity Analysis\n",
    "\n",
    "| Operation | Cost |\n",
    "|-----------|------|\n",
    "| Mutation (3-4 rollouts + reflection) | 3-4 LLM calls + 1 reflection call |\n",
    "| Mini-batch evaluation | 3-4 metric evaluations |\n",
    "| Full validation evaluation | N metric evaluations (N = valset size) |\n",
    "| Pareto check | O(F Ã— N) comparisons (F = frontier size) |\n",
    "\n",
    "The mini-batch gate matters because most mutations failâ€”they either don't improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Each Component Matters\n",
    "\n",
    "| Component | Without it | With it |\n",
    "|-----------|------------|---------|\n",
    "| **Textual feedback** | Optimizer sees only `score=0.6` | Optimizer reads \"wrong answer, expected X, got Y because...\" |\n",
    "| **Pareto selection** | Specialists discarded when aggregate drops | Specialists preserved if they solve anything unique |\n",
    "| **Lineage tracking** | No memory of evolutionary history | Can identify divergent branches for merge |\n",
    "| **Merge operation** | Insights stay siloed in separate branches | Orthogonal discoveries can combine |\n",
    "| **Mini-batch gating** | Evaluate every candidate fully | Reject obvious failures cheaply |\n",
    "\n",
    "These interact: Pareto selection preserves the diversity that makes merge valuable, textual feedback makes both mutation and merge more effective, and mini-batch gating keeps evaluation costs reasonable.\n",
    "\n",
    "---\n",
    "\n",
    "#### What Gets Returned\n",
    "\n",
    "The algorithm returns `best_aggregate(pareto_frontier)`â€”the prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use `track_stats=True` to access all candidates and their per-instance scores.\n",
    "\n",
    "---\n",
    "\n",
    "That's the full loop. Now let's see what the optimized prompts actually look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88efbc55",
   "metadata": {},
   "source": [
    "### What GEPA Learns: Domain-Specific Knowledge Encoding\n",
    "\n",
    "We've built the full algorithm. What does all this machinery actually *produce*? The output is prompts that encode domain expertise.\n",
    "\n",
    "GEPA can **encode domain-specific knowledge directly into prompts**â€”turning what experts know implicitly into explicit instructions that persist across examples.\n",
    "\n",
    "#### Prompts as Knowledge Containers\n",
    "\n",
    "Traditional optimization treats prompts as opaque strings to be scored. GEPA treats them as **knowledge containers** that accumulate insights through the reflection loop:\n",
    "\n",
    "![Failure Accumulation Experience](static/failure_acc_experience.webp)\n",
    "\n",
    "Each iteration doesn't just fix one errorâ€”it extracts the *lesson* behind the error.\n",
    "\n",
    "**Concrete example from our AIME experiments:**\n",
    "\n",
    "| Stage | Prompt excerpt |\n",
    "|-------|----------------|\n",
    "| **Seed** | \"You are given a problem and you have to give the answer along with reasoning.\" |\n",
    "| **After iter 2** | \"...For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.\" |\n",
    "| **After iter 3** | \"...When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.\" |\n",
    "\n",
    "The prompt evolved from generic instruction to encoding **competition math heuristics**. The LLM didn't invent theseâ€”it extracted them from its training knowledge, triggered by seeing specific failure modes.\n",
    "\n",
    "#### Three Categories of Captured Knowledge\n",
    "\n",
    "We observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):\n",
    "\n",
    "**1. Format and Interface Knowledge**\n",
    "- Output schemas (\"return JSON with keys: answer, reasoning\")\n",
    "- API conventions (\"use library.method(), not library_method()\")\n",
    "- Parsing requirements (\"integers only, no leading zeros\")\n",
    "\n",
    "This is easiest to extract since format errors produce explicit feedback.\n",
    "\n",
    "**2. Strategic Knowledge**\n",
    "- Problem-solving heuristics (\"try small cases first\")\n",
    "- Domain patterns (\"competition problems often combine algebra and number theory\")\n",
    "- Failure mode awareness (\"watch for off-by-one errors in counting\")\n",
    "\n",
    "This emerges from reflecting on *why* approaches failed, not just *that* they failed.\n",
    "\n",
    "**3. Factual Domain Knowledge**\n",
    "- API names and signatures (\"torch.einsum, not torch.einstein_sum\")\n",
    "- Domain constants (\"standard gravity = 9.81 m/sÂ²\")\n",
    "- Constraint relationships (\"in valid Sudoku, each row/column/box contains 1-9 exactly once\")\n",
    "\n",
    "The LLM already knows thisâ€”reflection surfaces it into the prompt where it's consistently applied.\n",
    "\n",
    "#### Why Prompts Beat Weights (Sometimes)\n",
    "\n",
    "Fine-tuning encodes knowledge in model weightsâ€”opaque, distributed, hard to inspect. GEPA encodes knowledge in natural languageâ€”readable and editable.\n",
    "\n",
    "| Aspect | Fine-tuning | GEPA prompts |\n",
    "|--------|-------------|--------------|\n",
    "| **Inspectability** | Black box | Human-readable instructions |\n",
    "| **Editability** | Requires retraining | Edit the text directly |\n",
    "| **Composability** | Train new model | Merge prompt sections |\n",
    "| **Sample efficiency** | Thousands of examples | Tens of examples |\n",
    "\n",
    "A GEPA-optimized prompt can be read by a human to understand what strategies it learned. You can edit it to add domain knowledge the optimizer missed, or transfer it to different LLMs.\n",
    "\n",
    "#### The Preservation Problem\n",
    "\n",
    "But new insights can overwrite old ones. We saw this in our hands-on experiment when iteration 3's number theory insights overwrote iteration 2's algebra insights, causing catastrophic forgetting.\n",
    "\n",
    "Pareto selection prevents this. By preserving prompts that are best on *any* validation instance, it keeps specialized knowledge around even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier, and the merge operation can later combine their insights.\n",
    "\n",
    "Without this, GEPA would periodically erase its own discoveries.\n",
    "\n",
    "#### Limitation: Knowledge Must Be Triggerable\n",
    "\n",
    "GEPA can only surface knowledge the base LLM already has. It's extraction, not creation. Stronger base models yield better results because there's more latent knowledge to work with. For specialized domains (custom hardware APIs, proprietary protocols), you may need human-written seed instructions or few-shot examples to get the reflection loop started.\n",
    "\n",
    "---\n",
    "\n",
    "So far we've focused on *training*: optimize prompts on labeled examples, deploy the best one. But GEPA's machineryâ€”reflective mutation, Pareto selection, mergeâ€”can also run at *inference time* as a search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93556b",
   "metadata": {},
   "source": [
    "## Beyond Training: GEPA for Inference-Time Search\n",
    "\n",
    "Everything we've covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that flips this on its head.\n",
    "\n",
    "### Two Paradigms of Operation\n",
    "\n",
    "**Train-then-generalize** (what we've built so far):\n",
    "\n",
    "- Optimize prompts on a training set\n",
    "- Select the best-aggregate prompt from the Pareto frontier\n",
    "- Deploy that prompt on new, unseen tasks\n",
    "- Goal: learn *generalizable* lessons that transfer\n",
    "\n",
    "**Test-time search** (inference-time optimization):\n",
    "\n",
    "- You have a batch of hard tasks you need to solve *now*\n",
    "- Optimize directly on the tasks themselves\n",
    "- GEPA searches for solutions, storing the best prompt *per task*\n",
    "- Goal: maximize performance on *these specific instances*\n",
    "\n",
    "The mechanics are identicalâ€”reflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you're using GEPA as a **search algorithm** over the solution space. This is an instance of [inference-time compute scaling](https://openai.com/index/learning-to-reason-with-llms/)â€”investing more computation at inference to solve harder problems.\n",
    "\n",
    "**The key mechanical change**: In normal GEPA, you have separate `trainset` (to learn from via reflection) and `valset` (to evaluate generalization). The Pareto frontier tracks per-instance performance on `valset`, preserving prompts that generalize well.\n",
    "\n",
    "For test-time search, pass the *same* problems as both `trainset` and `valset`:\n",
    "\n",
    "```python\n",
    "# Test-time search: optimize directly on the tasks you want to solve\n",
    "optimized = GEPA(metric=metric_with_feedback, auto=\"medium\").compile(\n",
    "    program,\n",
    "    trainset=hard_problems,  # The actual tasks you need solved\n",
    "    valset=hard_problems,    # Same held-out validation\n",
    ")\n",
    "```\n",
    "\n",
    "This tells GEPA: \"I don't care about generalizationâ€”optimize directly on *these specific problems*.\" The Pareto frontier now tracks \"best prompt for each problem\" rather than \"prompts that transfer to unseen data.\" See the [GEPA API documentation](https://dspy.ai/api/optimizers/GEPA/overview/) for full parameter details.\n",
    "\n",
    "> *\"Given a batch of tasks that we want to solve and given some budget... GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "---\n",
    "\n",
    "### Why GEPA Beats High-Temperature Sampling\n",
    "\n",
    "Traditional inference-time strategies sample at high temperature to generate many candidates, then pick the best. But these samples tend to be *similar*â€”variations on the same approach. GEPA induces **genuine diversity** through Pareto tracking (maintaining candidates that excel at *something different*) and reflective mutation (proposing *structurally different* strategies based on what went wrong, not random perturbations).\n",
    "\n",
    "When feedback says \"memory bandwidth bottleneck,\" the next candidate might switch from a naive loop to shared-memory tilingâ€”a qualitative change that temperature variation rarely discovers. On the [MATH benchmark](https://arxiv.org/abs/2103.03874), this approach achieves **93% accuracy** compared to 67% with basic DSPy ChainOfThought.\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Bootstrapping at inference\n",
    "\n",
    "During training, you iterate a fixed number of times and deploy the result. At inference time, you can keep iterating *on a single hard problem* until it's solvedâ€”and GEPA's reflective loop creates a self-bootstrapping dynamic:\n",
    "\n",
    "1. **Round 1**: Generate rollout â†’ compiler error (\"undefined variable x\") â†’ reflect â†’ propose fix\n",
    "2. **Round 2**: Code compiles â†’ runtime error (division by zero) â†’ reflect â†’ propose fix  \n",
    "3. **Round 3**: Runtime works â†’ wrong output (\"expected 42, got 41\") â†’ reflect â†’ propose fix\n",
    "4. **Round 4**: Correct output âœ“\n",
    "\n",
    "Each iteration surfaces *the next* failure modeâ€”you can't discover the runtime error until the compile error is fixed. Traditional sampling generates 100 candidates that all hit the same compiler error. GEPA's iterative reflection *progresses through* the failure cascade.\n",
    "\n",
    "> *\"It identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it's kind of like self-bootstrapping data to train itself.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "This is why test-time GEPA can solve problems that stumped training: it has the budget to chase failure modes deeper than any fixed training run.\n",
    "\n",
    "> **Note**: When optimizing a batch of tasks, Pareto selection ensures that fixing one problem doesn't discard prompts that solved othersâ€”see \"Cross-Task Transfer Within a Batch\" below.\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-Task Transfer Within a Batch\n",
    "\n",
    "When solving related tasks (e.g., a batch of [CUDA kernels](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)), insights compound across the batch:\n",
    "\n",
    "> *\"All of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "The frontier maintains multiple specialized prompts simultaneously:\n",
    "\n",
    "| Prompt | Specialization | Problems solved |\n",
    "|--------|----------------|-----------------|\n",
    "| P_conv | Convolutional operators | #1, #4, #7 |\n",
    "| P_reduce | Reduction/summation operators | #2, #5, #8 |\n",
    "| P_matmul | Matrix multiplication | #3, #6 |\n",
    "\n",
    "> *\"One prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "When a new problem arrives, GEPA tries candidates from across the frontierâ€”the convolution specialist might crack it, or the reduction specialist, or insights from both might merge.\n",
    "\n",
    "![Cross-Task Insight Transfer](static/cross_task_transfer.webp)\n",
    "\n",
    "---\n",
    "\n",
    "### Background Optimization Loops\n",
    "\n",
    "The self-bootstrapping pattern suggests a natural application: **background GEPA loops for personalization** in tools like Cursor and other AI-assisted environments.\n",
    "\n",
    "> *\"For your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt... cursor can learn a user-specific prompt that works well specifically for you.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "Every correction you provideâ€”rejecting a verbose explanation, fixing a code styleâ€”becomes a training signal that accumulates into a personalized prompt. The infrastructure isn't widespread yet, but the pattern points toward continuous adaptation rather than one-shot optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### What GEPA Stores\n",
    "\n",
    "For each task in the batch, GEPA tracks both artifacts:\n",
    "\n",
    "> *\"GEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "- **Best outputs** â€” the actual solutions, ready to use\n",
    "- **Best prompts** â€” specialized strategies representing different subdomains of your problem space\n",
    "\n",
    "You can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.\n",
    "\n",
    "---\n",
    "\n",
    "When the conditions *are* rightâ€”rich feedback, high-value tasks worth the compute, domains where the LLM has strong priorsâ€”test-time GEPA can beat sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc20c9",
   "metadata": {},
   "source": [
    "### Case Study: Code Optimization for Novel Hardware\n",
    "\n",
    "The GEPA paper demonstrates test-time search on **code optimization for hardware with limited pre-training data**â€”a domain well-suited to GEPA's reflective approach.\n",
    "\n",
    "#### AMD NPU Kernels: Optimization Without Pre-Training Knowledge\n",
    "\n",
    "AMD's NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The [NPUEval benchmark](https://arxiv.org/abs/2507.14403) shows how difficult this is: even with compiler feedback and RAG, state-of-the-art LLMs achieve only ~10% mean vectorization score.\n",
    "\n",
    "GEPA's approach doesn't require prior examplesâ€”it iteratively generates kernels, receives compiler errors or performance metrics, reflects on feedback, and proposes targeted improvements. The compiler error messages contain the fix: \"Symbol not found: `npu_matmul`\" triggers reflection that surfaces the correct API.\n",
    "\n",
    "> *\"GEPA can be used to generate optimized kernels for AMD's NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "#### CUDA Kernels: Outperforming Human Baselines\n",
    "\n",
    "[KernelBench](https://scalingintelligence.stanford.edu/blogs/kernelbench/) (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels. The benchmark sets a low baseline: **frontier reasoning models match the PyTorch baseline on less than 20% of tasks** using the fastâ‚ metric (correct *and* faster than PyTorch). Efficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven't learned to apply reliably.\n",
    "\n",
    "The KernelBench paper shows that feedback-driven refinement improves results substantiallyâ€”fastâ‚ scores jumped 3-6x when execution results and profiler feedback were provided in context. GEPA applies this same principle systematically, with Pareto tracking to preserve diverse optimization strategies rather than ad-hoc iteration.\n",
    "\n",
    "> *\"For CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "The [self-bootstrapping dynamic](#self-bootstrapping-at-inference) is especially effective here: each compilation error or profiler bottleneck reveals the next fix, letting GEPA progress through failure cascades that stump one-shot sampling.\n",
    "\n",
    "#### Cross-Kernel Transfer\n",
    "\n",
    "When optimizing a batch of related kernels, insights compound across tasks:\n",
    "\n",
    "> *\"If I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.\"* â€” Lakshya A Agrawal\n",
    "\n",
    "As GEPA optimizes each kernel, the Pareto frontier accumulates specialized promptsâ€”one excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax. This cross-task transfer is why batch optimization outperforms solving each kernel independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d51ab9",
   "metadata": {},
   "source": [
    "## Conclusion: When to Reach for GEPA\n",
    "\n",
    "We opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculusâ€”matching RL's optimization performance at a fraction of the sample cost by doing something RL can't: *reading the feedback*.\n",
    "\n",
    "On AIME math problems: 46.6% â†’ 56.6%. On MATH benchmark: 67% â†’ 93%. CUDA kernels that outperform human-written PyTorch baselines. All through prompt optimization alone, no fine-tuning required.\n",
    "\n",
    "As LLMs have gotten better at self-reflection, this kind of optimization has become practical. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix.\n",
    "\n",
    "---\n",
    "\n",
    "### Use GEPA for Train-Then-Generalize When:\n",
    "\n",
    "- You have **rich textual feedback** (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)\n",
    "- Your evaluation budget is **limited** (50-500 examples, not 5,000)\n",
    "- You're optimizing **compound AI systems** where prompts orchestrate multi-step pipelines\n",
    "- You need **interpretable results**â€”prompts you can read, edit, and reason about\n",
    "\n",
    "### Use GEPA for Test-Time Search When:\n",
    "\n",
    "- You have a **batch of high-value tasks** worth the compute investment\n",
    "- Each task produces **execution feedback** (tests, profilers, validators)\n",
    "- Tasks are **related enough** for cross-task transfer to help\n",
    "\n",
    "### Stick with Traditional Approaches When:\n",
    "\n",
    "- You have **abundant labeled data** and compute budget for fine-tuning\n",
    "- Feedback is **purely scalar** with no explanatory signal\n",
    "- The task is **already solved** by few-shot prompting\n",
    "- You need **sub-second latency**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Get Started\n",
    "\n",
    "Ready to try GEPA on your own pipelines?\n",
    "\n",
    "- **[GEPA for AIME Tutorial](https://dspy.ai/tutorials/gepa_aime/)** â€” Complete walkthrough from setup to optimized results\n",
    "- **[GEPA API Reference](https://dspy.ai/api/optimizers/GEPA/overview/)** â€” Full parameter documentation\n",
    "- **[Paper](https://arxiv.org/abs/2507.19457)** â€” Algorithm details and experimental methodology\n",
    "\n",
    "The core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads itâ€”and learns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "solveit_dialog_mode": "standard",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
