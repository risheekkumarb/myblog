<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Risheekkumar Baskaran</title>
<link>https://blog.risheekkumar.in/</link>
<atom:link href="https://blog.risheekkumar.in/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog about tech, programming, and life.</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sat, 20 Dec 2025 18:30:00 GMT</lastBuildDate>
<item>
  <title>GEPA Deepdive</title>
  <dc:creator>Risheek kumar B</dc:creator>
  <link>https://blog.risheekkumar.in/posts/gepa-deepdive/gepa_final_article.html</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>This article was made possible by the research papers referenced throughout, the <a href="https://www.youtube.com/watch?v=fREQrxhBSk0">Weaviate discussion with Lakshya A Agrawal</a>, and guidance from Kerem Turgutlu at answer.ai. Created using the <a href="https://solveit.fast.ai">solveit</a> platform by fast.ai.</p>
</blockquote>
<p>Youâ€™ve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It worksâ€¦ 70% of the time. You have 50 labeled examples. Now what?</p>
<p>Fine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesnâ€™t scale. Youâ€™re stuck.</p>
<p>But what if you could match RLâ€™s optimization performance using 50 examples instead of 5,000?</p>
<p><strong><a href="https://arxiv.org/abs/2507.19457">GEPA</a></strong> (Genetic-Pareto) achieves exactly thisâ€”by exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM <em>reflect on its own failures</em> and propose improvements directly.</p>
<p>In this post, weâ€™ll unpack how it works, why modern LLMsâ€™ improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.</p>
<section id="the-problem-scalar-rewards-and-expensive-rollouts" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-scalar-rewards-and-expensive-rollouts">The Problem: Scalar Rewards and Expensive Rollouts</h3>
<p>Traditional AI optimization techniquesâ€”reinforcement learning and fine-tuningâ€”have achieved remarkable results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?</p>
<p>Consider: - <strong>Agentic pipelines</strong> that invoke simulations, query rate-limited APIs, or run multi-step tool chains - <strong>Code generation for novel hardware</strong>, where each evaluation requires compiling for custom silicon and executing on the device - <strong>Complex reasoning tasks</strong> with expensive verification steps</p>
<p>Collecting thousands of rollouts simply isnâ€™t feasible in these settings.</p>
<p>The core issue: <strong>RL learns by comparison</strong>. A 500-step trajectory collapses to <code>reward = 0.73</code>. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectoriesâ€”<em>this one scored 0.8, that one scored 0.4, what differed?</em>â€”requiring sample counts that expensive domains canâ€™t support.</p>
<blockquote class="blockquote">
<p><em>RL and fine-tuning require generating large amounts of rollouts to gather scalar learning signalsâ€”sample inefficient by design.</em></p>
</blockquote>
<p>When each rollout costs minutes (or dollars), this approach breaks down.</p>
</section>
<section id="the-insight-llm-pipelines-generate-rich-textual-traces" class="level3">
<h3 class="anchored" data-anchor-id="the-insight-llm-pipelines-generate-rich-textual-traces">The Insight: LLM Pipelines Generate Rich Textual Traces</h3>
<p>Modern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:</p>
<ul>
<li><strong>Reasoning traces</strong> â€” Chain-of-Thought and ReAct logs expose the modelâ€™s explicit thought process. When a multi-hop QA system fails, you can see <em>where</em> the reasoning went wrong: â€œThe capital of France is Paris. Paris is in Germanyâ€¦â€ The failure mode is visible in the text.</li>
<li><strong>Environment feedback</strong> â€” Compiler errors donâ€™t just say â€œfailed.â€ They say <code>cannot find symbol 'x', did you mean 'y'?</code> API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.</li>
<li><strong>Evaluation rubrics</strong> â€” LLM-as-judge systems donâ€™t just score 3/5. They explain: â€œResponse was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.â€</li>
</ul>
<p>Each of these is dramatically richer than <code>reward = 0.73</code>.</p>
<p><strong>The key realization</strong>: these traces arenâ€™t just logs for debuggingâ€”theyâ€™re potential <em>input to the optimizer</em>. A compiler error that says â€œdid you mean â€˜yâ€™?â€ contains the fix. A rubric that says â€œtoo verboseâ€ specifies exactly what to change.</p>
<p>Traditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.</p>
<p>But what if we could justâ€¦ read the feedback?</p>
<p>This is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?</p>
</section>
<section id="the-opportunity-llms-can-reflect-on-their-own-failures" class="level3">
<h3 class="anchored" data-anchor-id="the-opportunity-llms-can-reflect-on-their-own-failures">The Opportunity: LLMs Can Reflect on Their Own Failures</h3>
<p>Hereâ€™s the key insight enabling a new optimization paradigm: <strong>LLMs already have prior knowledge about the domains theyâ€™re working in, and theyâ€™re increasingly capable of self-reflection.</strong></p>
<p>Consider what happens with different types of feedback:</p>
<p><strong>Compiler errors</strong> â€” When the compiler returns <code>cannot find symbol 'x', did you mean 'y'?</code>, the LLM doesnâ€™t need thousands of examples to learn the fix. It already knows the libraryâ€™s API. One error message is enough.</p>
<blockquote class="blockquote">
<p><em>â€œThe language model already knows that x is not a valid API name in the library but y is. Next time I should try this.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p><strong>LLM-as-judge feedback</strong> â€” When a judge says â€œyour summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,â€ the model can directly incorporate â€œbe concise, match casual toneâ€ into its next attempt. No statistical signal extraction required.</p>
<p><strong>Reasoning trace failures</strong> â€” When a multi-hop QA trace shows the model correctly retrieved â€œParis is the capital of Franceâ€ but then hallucinated â€œParis is in Germany,â€ the failure point is <em>visible in the text</em>. You can see exactly where the reasoning derailed.</p>
<p><strong>Privacy-aware rewriting (PUPA task)</strong> â€” In the paperâ€™s experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains <em>why</em> a rewrite failedâ€”â€œleaked the userâ€™s company nameâ€ or â€œremoved too much context, degrading response qualityâ€â€”giving the optimizer actionable signal from each example.</p>
<section id="the-core-asymmetry" class="level4">
<h4 class="anchored" data-anchor-id="the-core-asymmetry">The Core Asymmetry</h4>
<p>This is the fundamental difference GEPA exploits:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>How it learns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>RL</strong></td>
<td>Compare thousands of trajectories statistically: â€œThese 500 scored 0.8, those 500 scored 0.4â€”what differed?â€</td>
</tr>
<tr class="even">
<td><strong>Reflection</strong></td>
<td>Read the feedback directly: â€œThe compiler said use y, so use y.â€</td>
</tr>
</tbody>
</table>
<p>RL would need hundreds of rollouts to statistically isolate that <code>xâ†’y</code> is the fix. The LLM gets it from one error message.</p>
</section>
<section id="from-fixes-to-generalizable-rules" class="level4">
<h4 class="anchored" data-anchor-id="from-fixes-to-generalizable-rules">From Fixes to Generalizable Rules</h4>
<p>Better still, modern LLMs donâ€™t just extract point fixesâ€”they can derive <em>generalizable lessons</em>:</p>
<ul>
<li>Not just â€œuse <code>y</code> instead of <code>x</code>â€ â†’ but â€œalways verify symbol names against the libraryâ€™s namespace before generating codeâ€</li>
<li>Not just â€œresponse was too longâ€ â†’ but â€œfor Slack contexts, limit responses to 150 words and use bullet pointsâ€</li>
<li>Not just â€œleaked company nameâ€ â†’ but â€œscan for proper nouns and replace with generic placeholdersâ€</li>
</ul>
<blockquote class="blockquote">
<p><em>â€œLLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>These rules get folded directly into the prompt as instructionsâ€”compounding improvements across examples rather than treating each failure in isolation.</p>
<p>This capability unlockâ€”LLMs that can genuinely reflect and generalizeâ€”is what makes GEPA viable now when it wouldnâ€™t have been two years ago.</p>
</section>
</section>
<section id="why-now-the-reflection-capability-unlock" class="level3">
<h3 class="anchored" data-anchor-id="why-now-the-reflection-capability-unlock">Why Now? The Reflection Capability Unlock</h3>
<p>This approach wasnâ€™t viable with earlier LLMs. In March 2023, roboticist <a href="https://evjang.com/2023/03/26/self-reflection.html">Eric Jang observed</a> that self-reflection capability â€œseems to be emergent in GPT-4 but not GPT-3.5 or Claude.â€ When asked to write a non-rhyming poem, GPT-4 produced rhymesâ€”but when prompted â€œdid the poem meet the assignment?â€ it apologized and corrected itself. GPT-3.5 and Claude couldnâ€™t recognize their errors.</p>
<p>The <a href="https://arxiv.org/abs/2303.11366">Reflexion paper</a> (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4â€™s baseline 80%â€”without any weight updates. Similarly, <a href="https://arxiv.org/abs/2303.17651">Self-Refine</a> showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.</p>
<p>But thereâ€™s a crucial nuance. A <a href="https://aclanthology.org/2024.tacl-1.78/">comprehensive 2024 survey</a> found that pure â€œintrinsicâ€ self-correctionâ€”where the LLM reflects with no external signalâ€”rarely helps, and can even degrade performance. What <em>does</em> work is self-correction with <strong>reliable external feedback</strong>: compiler errors, test results, structured rubrics.</p>
<p>This is precisely what GEPA exploits. The <a href="https://arxiv.org/abs/2305.11738">CRITIC paper</a> (ICLR 2024) highlights that external feedback is â€œcrucialâ€ for successful self-improvement. GEPA doesnâ€™t ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedbackâ€”the compiler said this, the profiler showed that, the judge flagged this rubricâ€”and asks it to reflect on <em>that</em>.</p>
<p>The capability unlock isnâ€™t â€œLLMs can now introspect perfectly.â€ Itâ€™s â€œLLMs can now <em>process feedback and generalize lessons</em> effectively.â€</p>
<blockquote class="blockquote">
<p><em>â€œEarlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessonsâ€¦ But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
</section>
<section id="optimizer-evolution-from-few-shot-to-reflection" class="level3">
<h3 class="anchored" data-anchor-id="optimizer-evolution-from-few-shot-to-reflection">Optimizer Evolution: From Few-Shot to Reflection</h3>
<p>To understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessorâ€”and GEPA represents the latest capability unlock.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.risheekkumar.in/posts/gepa-deepdive/static/optimizer evolution.webp" class="img-fluid figure-img"></p>
<figcaption>Optimizer Evolution</figcaption>
</figure>
</div>
<section id="bootstrap-few-shot-dspy-2023" class="level4">
<h4 class="anchored" data-anchor-id="bootstrap-few-shot-dspy-2023">Bootstrap Few-Shot (DSPy, 2023)</h4>
<p>The original insight: you donâ€™t need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as <a href="https://www.promptingguide.ai/techniques/fewshot">few-shot demonstrations</a> for future runs. The system <a href="https://dspy.ai/learn/optimization/optimizers/">bootstraps its own examples</a> from successful executions.</p>
<p><strong>Example</strong>: Your QA system correctly answers â€œWhatâ€™s the capital of France?â€ â†’ â€œParisâ€. That (question, answer) pair becomes a demonstration shown to the model on future queries.</p>
<p><strong>Limitation</strong>: Demonstrations are static snapshots. Once selected, they donâ€™t adapt when new failure modes emerge. And thereâ€™s no <a href="https://arxiv.org/abs/2309.03409">instruction optimization</a>â€”the system prompt stays identical whether youâ€™re handling edge cases or common inputs.</p>
</section>
<section id="opro-google-deepmind-2023" class="level4">
<h4 class="anchored" data-anchor-id="opro-google-deepmind-2023">OPRO (Google DeepMind, 2023)</h4>
<p><a href="https://arxiv.org/abs/2309.03409">OPRO</a> (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.</p>
<p><strong>Example</strong>: The optimizer sees: - <code>"Solve the math problem step by step"</code> â†’ score 0.65 - <code>"Show your work and verify the answer"</code> â†’ score 0.72 - <code>"Break the problem into cases and check each"</code> â†’ score 0.78</p>
<p>It proposes: <code>"Systematically enumerate cases and verify each solution"</code> â†’ score 0.81</p>
<p><strong>Limitation</strong>: Score-only signal. The optimizer sees <em>that</em> <code>prompt_v3</code> scored 0.72 but not <em>why</em>. Did it make algebraic errors? Miss edge cases? The number alone doesnâ€™t say.</p>
</section>
<section id="mipro-2024" class="level4">
<h4 class="anchored" data-anchor-id="mipro-2024">MiPRO (2024)</h4>
<p><a href="https://arxiv.org/abs/2406.11695">MiPRO</a> recognized that instructions and demonstrations interactâ€”the <em>same</em> instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.</p>
<p><strong>The search space problem</strong>: Say you have 10 candidate instructions and 5 possible demo sets. Thatâ€™s 50 combinations. Now add instruction variants (â€œBe conciseâ€ vs â€œBe briefâ€ vs â€œAnswer in one sentenceâ€)â€”suddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.</p>
<p><strong>MiPROâ€™s solution: a cheap surrogate model</strong>. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you <em>have</em> run. The predictor learns patterns like â€œinstructions mentioning â€˜step-by-stepâ€™ tend to score higherâ€ or â€œdemos with longer reasoning traces correlate with better performance.â€</p>
<p>The workflow:</p>
<ol type="1">
<li><strong>Bootstrap</strong>: Run a small random sample of combinations (say, 30 out of 500)</li>
<li><strong>Train surrogate</strong>: Fit the predictor on those 30 (instruction, demos) â†’ score pairs</li>
<li><strong>Predict cheaply</strong>: Score all 500 combinations using the surrogate (milliseconds, not dollars)</li>
<li><strong>Evaluate selectively</strong>: Only run full evaluation on the top-predicted candidates</li>
<li><strong>Repeat</strong>: Add new results to training data, retrain surrogate, sample again</li>
</ol>
<p><strong>Example</strong>: After 30 random evaluations, the surrogate learns: - Instructions with â€œstep-by-stepâ€ â†’ +0.08 average - Demo set B (which has chain-of-thought examples) â†’ +0.05 average - Combining both â†’ predicted 0.79</p>
<p>MiPRO focuses budget on high-predicted combinations rather than exhaustive search.</p>
<p><strong>Limitation</strong>: The surrogate learns <em>correlations</em>, not <em>causation</em>. It knows â€œstep-by-step instructions score higherâ€ but not <em>why</em>â€”maybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt thatâ€™s 0.75 on everything beats one thatâ€™s 0.95 on hard cases but 0.60 overallâ€”even though that hard-case specialist might contain crucial insights.</p>
</section>
<section id="simba-2024" class="level4">
<h4 class="anchored" data-anchor-id="simba-2024">SIMBA (2024)</h4>
<p><a href="https://arxiv.org/abs/2410.17116">SIMBA</a> reframes prompt optimization as a <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a> problemâ€”a classic framework for sequential decision-making under uncertainty.</p>
<p><strong>The bandit analogy</strong>: Imagine youâ€™re in a casino with 100 slot machines. Each has a different (unknown) payout rate. You have 50 tokens. How do you maximize winnings?</p>
<ul>
<li><strong>Pure exploitation</strong>: Find one machine that seems good, play it 50 times. Problem: maybe you got lucky earlyâ€”another machine is actually better.</li>
<li><strong>Pure exploration</strong>: Try each machine once, thenâ€¦ youâ€™re out of tokens before you learn anything useful.</li>
<li><strong>Smart balance</strong>: Track your uncertainty about each machine. Play machines where youâ€™re <em>uncertain</em> (might be great!) more than machines youâ€™re <em>confident</em> are mediocre.</li>
</ul>
<p>SIMBA applies this to prompts. Each candidate prompt is a â€œslot machine.â€ Each evaluation is a â€œpull.â€ The score is the â€œpayout.â€</p>
<p><strong>How it works</strong>:</p>
<ol type="1">
<li><strong>Initialize</strong>: Start with a pool of candidate prompts (maybe generated by an LLM or hand-written)</li>
<li><strong>Track statistics</strong>: For each prompt, maintain: average score so far, number of times evaluated, and a <em>confidence interval</em> (range of plausible true scores)</li>
<li><strong>Sample strategically</strong>: Use <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit#Upper_confidence_bounds">Upper Confidence Bound (UCB)</a> to pick which prompt to evaluate nextâ€”favoring prompts with high uncertainty OR high average</li>
<li><strong>Update beliefs</strong>: After evaluation, narrow the confidence interval for that prompt</li>
<li><strong>Repeat</strong>: Eventually, confidence intervals separateâ€”you know which prompts are best</li>
</ol>
<p><strong>Example in action</strong>: You have 20 candidate prompts, budget for 50 evaluations.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Evaluations</th>
<th>Avg Score</th>
<th>Confidence Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>8</td>
<td>0.74</td>
<td>[0.70, 0.78]</td>
</tr>
<tr class="even">
<td>B</td>
<td>2</td>
<td>0.71</td>
<td>[0.55, 0.87]</td>
</tr>
<tr class="odd">
<td>C</td>
<td>5</td>
<td>0.68</td>
<td>[0.62, 0.74]</td>
</tr>
</tbody>
</table>
<p>Which to evaluate next? - Prompt A: probably ~0.74, weâ€™re confident - Prompt B: could be 0.55 (bad) or 0.87 (best!)â€”high uncertainty - Prompt C: probably ~0.68, weâ€™re fairly confident itâ€™s worse than A</p>
<p>SIMBA picks <strong>Prompt B</strong>â€”the uncertainty is valuable. If B turns out great, we found a winner. If bad, weâ€™ve ruled it out cheaply.</p>
<p><strong>Why this beats random search</strong>: Random would waste evaluations on prompts we already know are bad. SIMBA focuses budget on <em>decisions that matter</em>â€”resolving uncertainty between plausibly-good candidates.</p>
<p><strong>Limitation</strong>: SIMBA efficiently <em>finds</em> the best prompt but doesnâ€™t understand <em>why</em> it works. The bandit framework treats prompts as black boxes with hidden payout ratesâ€”it canâ€™t reason about â€œthis prompt works because it specifies output format.â€ And like MiPRO, it optimizes aggregate score: a prompt scoring 0.75 uniformly beats one scoring 0.95 on hard cases but 0.60 elsewhereâ€”even if the hard-case specialist contains insights worth preserving.</p>
</section>
<section id="gepa-2025-the-reflection-shift" class="level4">
<h4 class="anchored" data-anchor-id="gepa-2025-the-reflection-shift">GEPA (2025): The Reflection Shift</h4>
<p>GEPA breaks from this trajectory in two fundamental ways:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 34%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>What changed</th>
<th>Before GEPA</th>
<th>With GEPA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Learning signal</strong></td>
<td><code>score = 0.6</code></td>
<td>â€œExceeded word limit. Missing keyword. Compiler error: use y not x.â€</td>
</tr>
<tr class="even">
<td><strong>Selection strategy</strong></td>
<td>Best aggregate score</td>
<td>Pareto frontier of diverse specialists</td>
</tr>
</tbody>
</table>
<p><strong>1. From scalar scores to textual feedback</strong> â€” Instead of just knowing <em>that</em> a prompt scored 0.6, GEPA sees <em>why</em>: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.</p>
<p><strong>Example</strong>: OPRO sees <code>score = 0.6</code>. GEPA sees: &gt; â€œFailed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword â€˜disclaimerâ€™. Passed examples 1-6, 8-11.â€</p>
<p>The LLM reflects: â€œI should add an instruction about word limits and required keywords.â€</p>
<p><strong>2. From greedy to Pareto selection</strong> â€” Instead of always promoting the highest-scoring candidate, GEPA maintains a <em><a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto frontier</a></em>: candidates that each excel at <em>something</em> no other candidate beats.</p>
<p><strong>Example</strong>: Three candidates evaluated on 10 examples: - <code>prompt_A</code>: 8/10 overall, but fails hard cases #7 and #9 - <code>prompt_B</code>: 6/10 overall, but nails hard cases #7 and #9<br>
- <code>prompt_C</code>: 7/10 overall, no unique strengths</p>
<p>Greedy selection keeps only <code>prompt_A</code>. Pareto selection keeps both <code>prompt_A</code> <em>and</em> <code>prompt_B</code>â€”because Bâ€™s insights about hard cases might combine with Aâ€™s general strength. <code>prompt_C</code> gets dropped (dominated by A on everything).</p>
<p>The contrast with prior optimizers is stark: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains whyâ€”and proposes the fix directly.</p>
</section>
</section>
<section id="how-gepa-works-building-it-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-gepa-works-building-it-from-scratch">How GEPA Works: Building It From Scratch</h2>
<p>GEPA combines two key innovations: <strong>reflective prompt mutation</strong> (learning from textual feedback) and <strong>Pareto selection</strong> (preserving diverse specialists). Each is powerful alone; together they compound.</p>
<p>Weâ€™ll build them from scratch in this section:</p>
<ol type="1">
<li><p><strong>Reflective mutation</strong> â€” How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.</p></li>
<li><p><strong>Pareto selection</strong> â€” Why always improving your â€œbestâ€ prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.</p></li>
<li><p><strong>Merge</strong> â€” How GEPA combines insights from divergent lineages (covered after the complete algorithm).</p></li>
</ol>
<p>By the end, youâ€™ll see how these mechanisms combine into GEPAâ€™s full evolutionary loop.</p>
<hr>
<section id="quick-start-using-gepa-in-30-seconds" class="level3">
<h3 class="anchored" data-anchor-id="quick-start-using-gepa-in-30-seconds">ğŸš€ Quick Start: Using GEPA in 30 Seconds</h3>
<blockquote class="blockquote">
<p>ğŸ“– <strong>Full tutorial</strong>: <a href="https://dspy.ai/tutorials/gepa_aime/">GEPA for AIME (Math)</a> â€” optimizing GPT-4.1 Mini from 46.6% â†’ 56.6% on AIME 2025.</p>
</blockquote>
<p>Before diving deep, hereâ€™s what using GEPA looks like in practice:</p>
<p><strong>Step 1: Configure your language model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> dspy</span>
<span id="cb1-2"></span>
<span id="cb1-3">lm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dspy.LM(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai/gpt-4.1-mini"</span>, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, max_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32000</span>)</span>
<span id="cb1-4">dspy.configure(lm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lm)</span></code></pre></div></div>
<p><strong>Step 2: Define your program</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">program <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dspy.ChainOfThought(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"problem -&gt; answer"</span>)</span></code></pre></div></div>
<p><strong>Step 3: Define a metric that returns feedback (not just a score)</strong></p>
<p>This is the key difference from other optimizersâ€”your metric explains <em>why</em> something failed:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> metric_with_feedback(example, prediction, trace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kwargs):</span>
<span id="cb3-2">    correct_answer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> example.answer</span>
<span id="cb3-3">    pred_answer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prediction.answer</span>
<span id="cb3-4">    </span>
<span id="cb3-5">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(correct_answer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> pred_answer)</span>
<span id="cb3-6">    </span>
<span id="cb3-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb3-8">        feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Correct! The answer is '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>correct_answer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'."</span></span>
<span id="cb3-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-10">        feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Incorrect. Expected '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>correct_answer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">', got '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>pred_answer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'."</span></span>
<span id="cb3-11">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add any additional context that could help improvement:</span></span>
<span id="cb3-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">hasattr</span>(example, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'solution'</span>):</span>
<span id="cb3-13">            feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f" Solution: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>example<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>solution<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb3-14">    </span>
<span id="cb3-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> dspy.Prediction(score<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>score, feedback<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feedback)</span></code></pre></div></div>
<p><strong>Step 4: Optimize with GEPA</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> dspy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GEPA</span>
<span id="cb4-2"></span>
<span id="cb4-3">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GEPA(</span>
<span id="cb4-4">    metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>metric_with_feedback,</span>
<span id="cb4-5">    auto<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"light"</span>,           <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Budget preset: "light", "medium", or "heavy"</span></span>
<span id="cb4-6">    num_threads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>,         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Parallel evaluation threads</span></span>
<span id="cb4-7">)</span>
<span id="cb4-8"></span>
<span id="cb4-9">optimized_program <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optimizer.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(</span>
<span id="cb4-10">    program,</span>
<span id="cb4-11">    trainset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_set,</span>
<span id="cb4-12">    valset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>val_set,</span>
<span id="cb4-13">)</span></code></pre></div></div>
<p>Thatâ€™s it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.</p>
<hr>
<p><strong>Whatâ€™s happening under the hood?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 56%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Textual feedback</strong></td>
<td>Your metric returns <em>why</em> something failed, not just a score</td>
</tr>
<tr class="even">
<td><strong>Reflective mutation</strong></td>
<td>An LLM reads the feedback and proposes improved instructions</td>
</tr>
<tr class="odd">
<td><strong>Pareto selection</strong></td>
<td>Diverse specialists are preserved, not just the â€œbestâ€ prompt</td>
</tr>
<tr class="even">
<td><strong>Merge operations</strong></td>
<td>Insights from divergent lineages get combined</td>
</tr>
</tbody>
</table>
<p>The rest of this section explains <em>why</em> each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you needâ€”see the <a href="https://dspy.ai/api/optimizers/GEPA/overview/">DSPy GEPA API Reference</a> for full parameter details.</p>
<p>Now letâ€™s make this concrete by building the core mechanism from scratch.</p>
</section>
<section id="hands-on-building-reflective-mutation-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-building-reflective-mutation-from-scratch">Hands-On: Building Reflective Mutation from Scratch</h3>
<p>Letâ€™s make this concrete by implementing GEPAâ€™s core mechanism on a real task.</p>
<p><strong>The Problem: AIME Math Competition</strong></p>
<p>Weâ€™ll optimize prompts for solving <a href="https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination">AIME</a> (American Invitational Mathematics Examination) problems â€” challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb5-2">dset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"AI-MO/aimo-validation-aime"</span>)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>]</span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 90 problems with solutions and integer answers</span></span></code></pre></div></div>
<p><strong>Why AIME for testing prompt optimization?</strong></p>
<ol type="1">
<li><strong>Clear ground truth</strong> â€” Every answer is an integer (0-999), so evaluation is unambiguous</li>
<li><strong>Rich failure modes</strong> â€” Wrong answers come from algebraic errors, missed cases, misread constraints</li>
<li><strong>Domain knowledge helps</strong> â€” Prompts that encode strategies (â€œsubtract equations pairwiseâ€, â€œenumerate all casesâ€) measurably improve performance</li>
<li><strong>Small dataset</strong> â€” Only 90 problems, so sample efficiency matters</li>
</ol>
<p><strong>The Setup</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Split: 10 train, 10 validation (simulating scarce labeled data)</span></span>
<span id="cb6-2">tdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.sample(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">45</span>).iloc[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training mini-batches drawn from here</span></span>
<span id="cb6-3">vdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(tdf.index).iloc[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Held-out validation</span></span>
<span id="cb6-4"></span>
<span id="cb6-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Base model: Gemini 2.5 Flash via LiteLLM</span></span>
<span id="cb6-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Metric: Exact match (predicted integer == ground truth)</span></span>
<span id="cb6-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> metric(ground_truth, prediction):</span>
<span id="cb6-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(ground_truth) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> prediction[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>]</span></code></pre></div></div>
<p><strong>Seed Prompt</strong></p>
<p>We start with a minimal instruction:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">seed_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""You are given a problem and you have to give the answer </span></span>
<span id="cb7-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">along with reasoning. Do not return anything apart from json. </span></span>
<span id="cb7-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">It should be parsable by json.loads()"""</span></span></code></pre></div></div>
<p>Baseline validation accuracy: <strong>10%</strong> (1/10 correct)</p>
<p>Can reflective mutation improve this? Letâ€™s find out.</p>
<hr>
</section>
<section id="step-1-the-feedback-function" class="level3">
<h3 class="anchored" data-anchor-id="step-1-the-feedback-function">Step 1: The Feedback Function</h3>
<p>First, we need a function that tells the reflection LLM what went wrong. Weâ€™ll start with <strong>minimal feedback</strong>â€”just the correct answer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> feedback(ground_truth, prediction):</span>
<span id="cb8-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(ground_truth) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> prediction[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>]:</span>
<span id="cb8-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'You got it wrong! The solution is </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>ground_truth<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span></span>
<span id="cb8-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'You got it right!'</span></span></code></pre></div></div>
<p>This is deliberately simple. Later weâ€™ll discuss how richer feedback (like expert solutions) can improve results.</p>
<hr>
</section>
<section id="step-2-the-reflection-prompt" class="level3">
<h3 class="anchored" data-anchor-id="step-2-the-reflection-prompt">Step 2: The Reflection Prompt</h3>
<p>Following GEPAâ€™s structure, we build a prompt that shows the LLM its failures:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">REFLECTION_TEMPLATE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""I provided an assistant with the following instructions:</span></span>
<span id="cb9-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;curr_instructions&gt;</span></span>
<span id="cb9-3"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{current_prompt}</span></span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">The following are examples with assistant's responses and feedback:</span></span>
<span id="cb9-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;inputs_outputs_feedback&gt;</span></span>
<span id="cb9-7"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{examples}</span></span>
<span id="cb9-8"></span>
<span id="cb9-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Your task: write a new instruction for the assistant.</span></span>
<span id="cb9-10"></span>
<span id="cb9-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">- Read inputs carefully and identify the input format and task description</span></span>
<span id="cb9-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">- Read all responses and feedback. Identify niche/domain-specific factual information</span></span>
<span id="cb9-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">- If the assistant used a generalizable strategy, include that in the instruction</span></span>
<span id="cb9-14"></span>
<span id="cb9-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Provide the new instructions.</span></span>
<span id="cb9-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-17"></span>
<span id="cb9-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mk_reflection_prompt(df, curr_prompt):</span>
<span id="cb9-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Build reflection prompt from minibatch results."""</span></span>
<span id="cb9-20">    examples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, row <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.reset_index().iterrows():</span>
<span id="cb9-22">        example <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""# Example </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb9-23"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">## problem</span></span>
<span id="cb9-24"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>row[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'problem'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb9-25"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">## prediction</span></span>
<span id="cb9-26"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>row[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pred'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb9-27"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">## feedback</span></span>
<span id="cb9-28"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>feedback(row.answer, row.pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb9-29"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-30">        examples.append(example)</span>
<span id="cb9-31">    </span>
<span id="cb9-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> REFLECTION_TEMPLATE.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(</span>
<span id="cb9-33">        current_prompt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>curr_prompt,</span>
<span id="cb9-34">        examples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>.join(examples)</span>
<span id="cb9-35">    )</span></code></pre></div></div>
<hr>
</section>
<section id="step-3-the-complete-optimization-loop" class="level3">
<h3 class="anchored" data-anchor-id="step-3-the-complete-optimization-loop">Step 3: The Complete Optimization Loop</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> reflect(mb, curr_prompt):</span>
<span id="cb10-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Ask LLM to reflect on failures and propose improved instruction."""</span></span>
<span id="cb10-3">    refl_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mk_reflection_prompt(mb, curr_prompt)</span>
<span id="cb10-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> _call(refl_prompt, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ReflectionModel)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'new_instruction'</span>]</span>
<span id="cb10-5"></span>
<span id="cb10-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> optimize_prompt(seed_prompt, traindf, valdf, n_iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, mb_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>):</span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Greedy reflective prompt optimization."""</span></span>
<span id="cb10-8">    prompts, train_scores, val_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [seed_prompt], [], []</span>
<span id="cb10-9">    mb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> traindf.sample(mb_size)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fixed minibatch for this run</span></span>
<span id="cb10-10">    </span>
<span id="cb10-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Baseline validation: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eval_val(seed_prompt, valdf)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb10-12">    </span>
<span id="cb10-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_iters):</span>
<span id="cb10-14">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate current prompt on minibatch</span></span>
<span id="cb10-15">        mb_eval, mb_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eval_mb(prompts[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], mb)</span>
<span id="cb10-16">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"ğŸ“Š Minibatch: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mb_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb10-17">        </span>
<span id="cb10-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reflect and propose new instruction</span></span>
<span id="cb10-19">        new_instr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reflect(mb_eval, prompts[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb10-20">        new_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> new_instr  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The new instruction becomes the new prompt</span></span>
<span id="cb10-21">        </span>
<span id="cb10-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate on validation set</span></span>
<span id="cb10-23">        val_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eval_val(new_prompt, valdf)</span>
<span id="cb10-24">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"ğŸ“Š Validation: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>val_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb10-25">        </span>
<span id="cb10-26">        prompts.append(new_prompt)</span>
<span id="cb10-27">        val_scores.append(val_score)</span>
<span id="cb10-28">    </span>
<span id="cb10-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(prompts<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prompts, val_scores<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>val_scores)</span></code></pre></div></div>
<hr>
</section>
<section id="what-actually-happened" class="level3">
<h3 class="anchored" data-anchor-id="what-actually-happened">What Actually Happened</h3>
<p>Running this on AIME problems with Gemini 2.5 Flash:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 19%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Minibatch</th>
<th>Validation</th>
<th>What the reflection learned</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td>â€”</td>
<td>10%</td>
<td>â€”</td>
</tr>
<tr class="even">
<td>1</td>
<td>0%</td>
<td>10%</td>
<td>JSON formatting details, output structure rules</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0%</td>
<td><strong>30%</strong></td>
<td>Systems of equations strategy, remainder/modular arithmetic tips</td>
</tr>
<tr class="even">
<td>3</td>
<td>67%</td>
<td><strong>10%</strong></td>
<td>Over-specialized on number theory, solved #9 but lost generality</td>
</tr>
</tbody>
</table>
<p><strong>The good</strong>: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:</p>
<blockquote class="blockquote">
<p><em>â€œWhen dealing with systems of equations like <img src="https://latex.codecogs.com/png.latex?xy%20+%20Az%20=%20C">, <img src="https://latex.codecogs.com/png.latex?yz%20+%20Ax%20=%20C">, <img src="https://latex.codecogs.com/png.latex?zx%20+%20Ay%20=%20C">, consider subtracting equations pairwise to find relationships between variables, such as <img src="https://latex.codecogs.com/png.latex?(x-z)(y-A)=0">, which implies <img src="https://latex.codecogs.com/png.latex?x=z"> or <img src="https://latex.codecogs.com/png.latex?y=A">. Systematically explore all such cases.â€</em></p>
</blockquote>
<p>This is directly from the actual outputâ€”the reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.</p>
<p><strong>The bad</strong>: Iteration 3 achieved 67% on its minibatch but dropped to <strong>10% validation</strong>. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:</p>
<blockquote class="blockquote">
<p><em>â€œWhen the problem involves number theory and remainders (e.g., <img src="https://latex.codecogs.com/png.latex?n%20%5Cpmod%20x">, <img src="https://latex.codecogs.com/png.latex?n%20%5Cpmod%20y">), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.â€</em></p>
</blockquote>
<p>This over-specialized advice (â€œcan lead to an answer of 0â€) actively hurt performance on non-number-theory problems, dropping validation from 30% back to <strong>10%</strong> (1/10)â€”though notably, it did solve problem #9, which earlier prompts couldnâ€™t.</p>
<hr>
</section>
<section id="the-greedy-selection-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-greedy-selection-problem">The Greedy Selection Problem</h3>
<p>This demonstrates exactly why GEPA uses <strong>Pareto selection</strong> instead of always taking the â€œbestâ€ prompt:</p>
<ol type="1">
<li><strong>Iteration 2â€™s prompt</strong> was a specialistâ€”it learned something valuable about systems of equations</li>
<li><strong>Iteration 3</strong> tried to improve on iteration 2, but the minibatch had different problems</li>
<li>The reflection <strong>overwrote</strong> the systems-of-equations insight while adding number-theory tips that were <em>too specific</em></li>
<li>Result: <strong>catastrophic forgetting</strong></li>
</ol>
<p>With greedy selection, we would have discarded iteration 2â€™s valuable insight. Pareto selection would keep itâ€”because it was <em>best on at least one validation instance</em>.</p>
<hr>
</section>
<section id="the-missing-ingredient-rich-feedback" class="level3">
<h3 class="anchored" data-anchor-id="the-missing-ingredient-rich-feedback">The Missing Ingredient: Rich Feedback</h3>
<p>Our minimal feedback (<code>"You got it wrong! The solution is 349"</code>) only tells the model <em>that</em> it failed, not <em>why</em> or <em>how to fix it</em>.</p>
<p>The AIME dataset includes expert solutions. A richer feedback function could use them:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> feedback_rich(row):</span>
<span id="cb11-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(row.answer) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> row.pred[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>]:</span>
<span id="cb11-3">        sol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> row.solution[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"..."</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(row.solution) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> row.solution</span>
<span id="cb11-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""Wrong! Expected </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>row<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>answer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, got </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>row<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pred[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.</span></span>
<span id="cb11-5"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-6"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">Model's reasoning: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>row<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pred[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'short_reasoning'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb11-7"></span>
<span id="cb11-8"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">Expert solution approach:</span></span>
<span id="cb11-9"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sol<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb11-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Correct!"</span></span></code></pre></div></div>
<p>With rich feedback, the reflection LLM can extract <em>specific strategies</em> from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: <strong>the feedback contains the fix</strong>.</p>
<p>Compare this to RL, which would only see <code>reward = 0</code> and have to statistically infer what went wrong across thousands of trajectories.</p>
<hr>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li><strong>Reflective mutation works</strong> â€” Even with minimal feedback, the LLM extracted useful domain knowledge</li>
<li><strong>Greedy selection fails</strong> â€” Iteration 3â€™s collapse shows why we need to preserve specialist insights</li>
<li><strong>Feedback quality matters</strong> â€” Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with</li>
<li><strong>Sample efficiency is real</strong> â€” We saw meaningful optimization with just 3 iterations on 3 examples each</li>
</ol>
<p>This is the core limitation of greedy optimization: <strong>catastrophic forgetting</strong>. The solution? Pareto selectionâ€”which weâ€™ll build from scratch next.</p>
</section>
<section id="hands-on-building-the-pareto-frontier" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-building-the-pareto-frontier">Hands-On: Building the Pareto Frontier</h3>
<p>In the reflective mutation section, we saw greedy selection failâ€”iteration 3â€™s over-specialized prompt dropped validation from 30% to 10%, losing iteration 2â€™s valuable systems-of-equations insights. The fundamental problem: always improving your â€œbestâ€ prompt discards specialist knowledge.</p>
<p>GEPAâ€™s solution: <strong>Pareto selection</strong>. Instead of keeping one best prompt, maintain a <em>frontier</em> of prompts where each excels at something no other prompt beats.</p>
<hr>
<section id="what-is-pareto-dominance" class="level4">
<h4 class="anchored" data-anchor-id="what-is-pareto-dominance">What is Pareto Dominance?</h4>
<p>A prompt <strong>dominates</strong> another if itâ€™s at least as good everywhere, and strictly better somewhere:</p>
<ul>
<li><strong>â‰¥</strong> on <em>every</em> validation instance, AND<br>
</li>
<li><strong>&gt;</strong> on <em>at least one</em> instance</li>
</ul>
<p>If prompt A dominates prompt B, we can safely discard Bâ€”A is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.</p>
<p><strong>Example</strong>: Consider four prompts evaluated on 10 validation instances. Weâ€™ll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Instances Solved</th>
<th>Aggregate</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P0 (seed)</td>
<td>#0 only</td>
<td>10%</td>
<td>Dominated by P2</td>
</tr>
<tr class="even">
<td>P1 (iter 1)</td>
<td>#0 only</td>
<td>10%</td>
<td>Dominated by P2</td>
</tr>
<tr class="odd">
<td>P2 (iter 2)</td>
<td>#0, #1, #2</td>
<td>30%</td>
<td><strong>Frontier</strong> âœ“</td>
</tr>
<tr class="even">
<td>P3 (iter 3)</td>
<td>#9 only</td>
<td>10%</td>
<td><strong>Frontier</strong> âœ“</td>
</tr>
</tbody>
</table>
<p>P2 dominates both P0 and P1â€”it solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which <em>nothing else could</em>.</p>
<p>The Pareto frontier is <strong>{P2, P3}</strong>. Both contain unique value.</p>
<hr>
</section>
<section id="implementation-dominance-checking" class="level4">
<h4 class="anchored" data-anchor-id="implementation-dominance-checking">Implementation: Dominance Checking</h4>
<p>We represent per-instance scores as boolean arrays (1 = solved, 0 = failed):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb12-2"></span>
<span id="cb12-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> dominates(candidate_scores, other_scores):</span>
<span id="cb12-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)"""</span></span>
<span id="cb12-5">    candidate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(candidate_scores)</span>
<span id="cb12-6">    other <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(other_scores)</span>
<span id="cb12-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (candidate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> other).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span>() <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> (candidate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> other).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>()</span>
<span id="cb12-8"></span>
<span id="cb12-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> is_dominated_by_any(new_scores, frontier_scores):</span>
<span id="cb12-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Is new_scores dominated by ANY prompt in the frontier?"""</span></span>
<span id="cb12-11">    new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(new_scores)</span>
<span id="cb12-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> existing <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> frontier_scores:</span>
<span id="cb12-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> dominates(np.array(existing), new):</span>
<span id="cb12-14">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb12-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb12-16"></span>
<span id="cb12-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_dominated_indices(new_scores, frontier_scores):</span>
<span id="cb12-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Which frontier prompts does new_scores dominate?"""</span></span>
<span id="cb12-19">    new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(new_scores)</span>
<span id="cb12-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> [i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, existing <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(frontier_scores) </span>
<span id="cb12-21">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> dominates(new, np.array(existing))]</span></code></pre></div></div>
<hr>
</section>
<section id="tracing-through-why-p3-survives" class="level4">
<h4 class="anchored" data-anchor-id="tracing-through-why-p3-survives">Tracing Through: Why P3 Survives</h4>
<p>Letâ€™s verify the dominance relationships from our example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">P0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Solves: #0</span></span>
<span id="cb13-2">P1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Solves: #0</span></span>
<span id="cb13-3">P2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Solves: #0, #1, #2</span></span>
<span id="cb13-4">P3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Solves: #9</span></span>
<span id="cb13-5"></span>
<span id="cb13-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Does P2 dominate P0?</span></span>
<span id="cb13-7">dominates(P2, P0)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2</span></span>
<span id="cb13-8"></span>
<span id="cb13-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Does P2 dominate P1?</span></span>
<span id="cb13-10">dominates(P2, P1)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2</span></span>
<span id="cb13-11"></span>
<span id="cb13-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Does P2 dominate P3?</span></span>
<span id="cb13-13">dominates(P2, P3)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># False! P2 loses on #9 (0 &lt; 1)</span></span>
<span id="cb13-14"></span>
<span id="cb13-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Does P3 dominate P2?</span></span>
<span id="cb13-16">dominates(P3, P2)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># False! P3 loses on #0, #1, #2</span></span></code></pre></div></div>
<p>Neither P2 nor P3 dominates the otherâ€”theyâ€™re <strong>Pareto incomparable</strong>. Each solves problems the other canâ€™t. Both stay on the frontier.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.risheekkumar.in/posts/gepa-deepdive/static/pareto_visual.webp" class="img-fluid figure-img"></p>
<figcaption>Pareto Frontier Explained</figcaption>
</figure>
</div>
<hr>
</section>
<section id="the-complete-frontier-manager" class="level4">
<h4 class="anchored" data-anchor-id="the-complete-frontier-manager">The Complete Frontier Manager</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ParetoFrontier:</span>
<span id="cb14-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb14-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scores[i][j] = prompt i's score on instance j</span></span>
<span id="cb14-5">    </span>
<span id="cb14-6">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> add(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, prompt, instance_scores):</span>
<span id="cb14-7">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Try to add a prompt. Returns True if it joins the frontier."""</span></span>
<span id="cb14-8">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reject if dominated by existing frontier member</span></span>
<span id="cb14-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> is_dominated_by_any(instance_scores, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores):</span>
<span id="cb14-10">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb14-11">        </span>
<span id="cb14-12">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove any frontier members this prompt dominates</span></span>
<span id="cb14-13">        dominated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_dominated_indices(instance_scores, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores)</span>
<span id="cb14-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(dominated, reverse<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>):  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove from end first</span></span>
<span id="cb14-15">            <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">del</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts[i]</span>
<span id="cb14-16">            <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">del</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores[i]</span>
<span id="cb14-17">        </span>
<span id="cb14-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add to frontier</span></span>
<span id="cb14-19">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts.append(prompt)</span>
<span id="cb14-20">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores.append(instance_scores)</span>
<span id="cb14-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb14-22">    </span>
<span id="cb14-23">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sample(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb14-24">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Sample a prompt, weighted by unique wins."""</span></span>
<span id="cb14-25">        weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-26">        scores_arr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores)</span>
<span id="cb14-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts)):</span>
<span id="cb14-28">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># How many instances is this prompt *uniquely* best on?</span></span>
<span id="cb14-29">            others_best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.delete(scores_arr, i, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> np.zeros_like(scores_arr[i])</span>
<span id="cb14-30">            unique_wins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (scores_arr[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> others_best).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb14-31">            weights.append(unique_wins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># +1 smoothing</span></span>
<span id="cb14-32">        </span>
<span id="cb14-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.random.choice(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array(weights)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(weights))</span>
<span id="cb14-34">    </span>
<span id="cb14-35">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> best_aggregate(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb14-36">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Return the prompt with highest aggregate score."""</span></span>
<span id="cb14-37">        aggregates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scores]</span>
<span id="cb14-38">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prompts[np.argmax(aggregates)]</span></code></pre></div></div>
<hr>
</section>
<section id="putting-it-together-pareto-guided-optimization" class="level4">
<h4 class="anchored" data-anchor-id="putting-it-together-pareto-guided-optimization">Putting It Together: Pareto-Guided Optimization</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> optimize_with_pareto(seed_prompt, traindf, valdf, n_iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, mb_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>):</span>
<span id="cb15-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Reflective mutation with Pareto frontier selection."""</span></span>
<span id="cb15-3">    frontier <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ParetoFrontier()</span>
<span id="cb15-4">    </span>
<span id="cb15-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize with seed</span></span>
<span id="cb15-6">    seed_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_per_instance(seed_prompt, valdf)</span>
<span id="cb15-7">    frontier.add(seed_prompt, seed_scores)</span>
<span id="cb15-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Baseline: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(seed_scores)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(seed_scores)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-9">    </span>
<span id="cb15-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_iters):</span>
<span id="cb15-11">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'='</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">Iteration </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'='</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb15-12">        </span>
<span id="cb15-13">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Sample parent from frontier (weighted by unique wins)</span></span>
<span id="cb15-14">        parent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> frontier.sample()</span>
<span id="cb15-15">        </span>
<span id="cb15-16">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run on minibatch, reflect, propose mutation</span></span>
<span id="cb15-17">        mb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> traindf.sample(mb_size)</span>
<span id="cb15-18">        mb_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_with_traces(parent, mb)</span>
<span id="cb15-19">        new_prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reflect_and_mutate(parent, mb_results)</span>
<span id="cb15-20">        </span>
<span id="cb15-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate on full validation set</span></span>
<span id="cb15-22">        new_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_per_instance(new_prompt, valdf)</span>
<span id="cb15-23">        new_agg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(new_scores) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(new_scores)</span>
<span id="cb15-24">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"New prompt: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>new_agg<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> aggregate"</span>)</span>
<span id="cb15-25">        </span>
<span id="cb15-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Try to add to frontier</span></span>
<span id="cb15-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> frontier.add(new_prompt, new_scores):</span>
<span id="cb15-28">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"âœ“ Added to frontier (size: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(frontier.prompts)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)"</span>)</span>
<span id="cb15-29">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb15-30">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"âœ— Dominated, rejected"</span>)</span>
<span id="cb15-31">    </span>
<span id="cb15-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> frontier.best_aggregate()</span></code></pre></div></div>
<hr>
</section>
<section id="what-we-observed-on-aime" class="level4">
<h4 class="anchored" data-anchor-id="what-we-observed-on-aime">What We Observed on AIME</h4>
<p>Running this on AIME problems with Gemini 2.5 Flash:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Iteration</th>
<th>Aggregate</th>
<th>Instances Solved</th>
<th>Frontier Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Seed</td>
<td>10%</td>
<td>#0</td>
<td>Initialize</td>
</tr>
<tr class="even">
<td>1</td>
<td>10%</td>
<td>#0</td>
<td>Dominated by seed, rejected</td>
</tr>
<tr class="odd">
<td>2</td>
<td>30%</td>
<td>#0, #1, #2</td>
<td>Added, dominates seed</td>
</tr>
<tr class="even">
<td>3</td>
<td>10%</td>
<td>#9 only</td>
<td><strong>Added</strong> âœ“ (unique win on #9)</td>
</tr>
</tbody>
</table>
<p><strong>The key moment</strong>: Iteration 3 scored only 10%â€”worse than iteration 2â€™s 30%. Greedy selection would discard it entirely.</p>
<p>But it solved <strong>instance #9</strong>, which nothing else could. Pareto selection preserves it.</p>
<p>Our final frontier: <strong>{P2, P3}</strong> - P2: Strong generalist (30%), knows systems-of-equations strategies - P3: Instance-9 specialist (10%), knows whatever cracked that specific problem</p>
<p>Both insights survive. The merge operation (covered later) can combine them.</p>
<hr>
</section>
<section id="why-this-matters-no-more-catastrophic-forgetting" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-no-more-catastrophic-forgetting">Why This Matters: No More Catastrophic Forgetting</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Selection Strategy</th>
<th>What happens to specialists</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Greedy</strong></td>
<td>Discarded whenever aggregate score drops</td>
</tr>
<tr class="even">
<td><strong>Pareto</strong></td>
<td>Preserved if they solve <em>anything</em> unique</td>
</tr>
</tbody>
</table>
<p>Greedy selection caused our iteration 3 collapseâ€”the number-theory prompt overwrote the algebra promptâ€™s insights. Pareto selection prevents this by construction: you canâ€™t remove a prompt from the frontier unless something else does everything it does, <em>plus more</em>.</p>
<p>This is the core of GEPAâ€™s sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier <strong>never loses them in the first place</strong>.</p>
<hr>
<p>We just saw Pareto selection preserve iteration 3â€™s prompt despite its 10% aggregate scoreâ€”because it solved instance #9, which nothing else could. But this raises a question: <em>why</em> does keeping â€œlosersâ€ help optimization? Shouldnâ€™t we focus resources on the best candidates?</p>
<p>The answer comes from <strong>quality-diversity</strong> algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.</p>
</section>
</section>
<section id="why-pareto-works-quality-diversity-and-map-elites" class="level3">
<h3 class="anchored" data-anchor-id="why-pareto-works-quality-diversity-and-map-elites">Why Pareto Works: Quality-Diversity and Map Elites</h3>
<p>Weâ€™ve now built both core mechanisms from scratchâ€”reflective mutation and Pareto selection. Before diving into the full algorithm, letâ€™s briefly step back and understand <em>why</em> this approach works so well.</p>
<p>The Pareto frontier isnâ€™t a novel inventionâ€”it draws from <strong>quality-diversity (QD)</strong> algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.</p>
<section id="the-core-insight" class="level4">
<h4 class="anchored" data-anchor-id="the-core-insight">The Core Insight</h4>
<p>Traditional optimization asks: <em>â€œWhatâ€™s the single best solution?â€</em></p>
<p>Quality-diversity asks: <em>â€œWhatâ€™s the best solution of each <em>type</em>?â€</em></p>
<p>Complex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensivelyâ€”but the best engine knows <em>both</em> styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one â€œbestâ€ approach.</p>
</section>
<section id="map-elites-the-inspiration" class="level4">
<h4 class="anchored" data-anchor-id="map-elites-the-inspiration">Map Elites: The Inspiration</h4>
<p><a href="https://arxiv.org/abs/1504.04909">Map Elites</a> (Mouret &amp; Clune, 2015) maintains an <em>archive</em> organized by behavior:</p>
<ol type="1">
<li><strong>Define behavior dimensions</strong> â€” characteristics describing <em>how</em> a solution works (not just how well)</li>
<li><strong>Discretize into bins</strong> â€” each cell represents a â€œnicheâ€</li>
<li><strong>Keep the best per bin</strong> â€” new solutions compete only within their niche</li>
<li><strong>Mutate from the archive</strong> â€” sample from any occupied bin, mutate, place in appropriate bin</li>
</ol>
<p>The result: diverse specialists, each optimal <em>of its type</em>. <strong>Key insight</strong>: the archive provides <em>stepping stones</em>â€”a mutation from one niche might discover something useful for another. Diversity isnâ€™t just nice to have; itâ€™s a search strategy.</p>
</section>
<section id="gepas-adaptation-validation-instances-as-niches" class="level4">
<h4 class="anchored" data-anchor-id="gepas-adaptation-validation-instances-as-niches">GEPAâ€™s Adaptation: Validation Instances as Niches</h4>
<p>GEPA recognizes that <strong>the validation set itself defines the behavior space</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 66%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Map Elites</th>
<th>GEPA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Behavior = continuous dimensions</td>
<td>Behavior = which validation instances are solved</td>
</tr>
<tr class="even">
<td>Bins = discretized regions</td>
<td>â€œBinsâ€ = individual validation instances</td>
</tr>
<tr class="odd">
<td>Archive = best per bin</td>
<td>Pareto frontier = non-dominated prompts across instances</td>
</tr>
</tbody>
</table>
<p>Each validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the â€œspecialist for that nicheâ€â€”it stays because it demonstrates <em>something works</em>, even with low aggregate score.</p>
<p><strong>Concrete example</strong>: In our AIME experiment, instance #9 was a number theory problem requiring CRT. Prompt P3 (10% aggregate) solved it; P2 (30% aggregate) couldnâ€™t. In Map Elites terms, P3 is the â€œeliteâ€ for the number-theory nicheâ€”it stays in the archive despite low overall score.</p>
<blockquote class="blockquote">
<p><em>â€œBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>This directly motivated GEPAâ€™s design: Pareto selection over per-instance scores naturally implements QDâ€™s â€œbest per nicheâ€ principle, while the merge operation recombines insights across nichesâ€”exactly what stepping-stone search requires.</p>
</section>
<section id="exploration-exploitation-balance" class="level4">
<h4 class="anchored" data-anchor-id="exploration-exploitation-balance">Exploration-Exploitation Balance</h4>
<p>GEPAâ€™s Pareto selection handles the exploration-exploitation tradeoff automatically: candidates are sampled weighted by unique wins, so specialists get attention proportional to their unique value. The frontier self-prunesâ€”noise gets dominated away, genuine diversity persists. No manual tuning required.</p>
<p>With both reflective mutation and Pareto selection in place, thereâ€™s one more operation that makes GEPA powerful: <strong>merge</strong>â€”combining insights from divergent lineages. Letâ€™s look at the complete algorithm.</p>
</section>
</section>
<section id="the-complete-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-complete-algorithm">The Complete Algorithm</h3>
<p>Now that weâ€™ve built reflective mutation and Pareto selection from scratch, and understand <em>why</em> quality-diversity works, letâ€™s see how all the piecesâ€”including lineage tracking and mergeâ€”combine into GEPAâ€™s full optimization loop.</p>
<section id="algorithm-overview" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-overview">Algorithm Overview</h4>
<pre><code>GEPA(base_prompt, trainset, valset, max_iterations):
    
    # Initialize
    candidates = [base_prompt]
    scores = {base_prompt: evaluate_all(base_prompt, valset)}
    pareto_frontier = [base_prompt]
    lineage = {base_prompt: None}  # parent tracking
    
    for iteration in 1..max_iterations:
        
        # 1. SAMPLE: Select candidate from Pareto frontier
        #    Weighted by number of instances where candidate is best
        parent = sample_from_frontier(pareto_frontier, scores)
        
        # 2. PROPOSE: Either mutate or merge
        if should_merge(pareto_frontier, iteration):
            # Select second parent from different lineage
            other_parent = select_divergent_candidate(pareto_frontier, parent, lineage)
            new_prompt = merge(parent, other_parent)
        else:
            # Run rollouts, collect feedback, reflect
            minibatch = sample(trainset, k=3)
            traces, feedback = run_with_feedback(parent, minibatch)
            # See "Hands-On: Building Reflective Mutation" for the reflection prompt structure
            new_prompt = reflect_and_mutate(parent, traces, feedback)
        
        # 3. EVALUATE: Mini-batch gate, then full evaluation
        parent_mb_score = evaluate(parent, minibatch)
        new_mb_score = evaluate(new_prompt, minibatch)
        if new_mb_score &lt;= parent_mb_score:
            continue  # Reject: didn't improve on mini-batch
        
        new_scores = evaluate_all(new_prompt, valset)
        
        # 4. UPDATE: Pareto frontier maintenance
        if is_dominated(new_scores, pareto_frontier):
            continue  # Reject: dominated by existing candidate
        
        # Remove any candidates the new one dominates
        pareto_frontier = [c for c in pareto_frontier 
                          if not dominates(new_scores, scores[c])]
        
        # Add new candidate
        candidates.append(new_prompt)
        scores[new_prompt] = new_scores
        pareto_frontier.append(new_prompt)
        lineage[new_prompt] = parent
    
    # Return best aggregate for deployment; full frontier available via track_stats
    return best_aggregate(pareto_frontier, scores)</code></pre>
<hr>
</section>
<section id="the-key-decision-points" class="level4">
<h4 class="anchored" data-anchor-id="the-key-decision-points">The Key Decision Points</h4>
<p><strong>1. Candidate Sampling (Exploration vs Exploitation)</strong></p>
<p>Rather than always improving the single best prompt (greedy), GEPA samples from the entire Pareto frontier. The sampling weight is proportional to how many validation instances the candidate is <em>uniquely best</em> on:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sample_from_frontier(frontier, scores):</span>
<span id="cb17-2">    weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb17-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> candidate <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> frontier:</span>
<span id="cb17-4">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Count instances where this candidate beats all others</span></span>
<span id="cb17-5">        unique_wins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_instances) </span>
<span id="cb17-6">                        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> scores[candidate][i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(scores[other][i] </span>
<span id="cb17-7">                                                      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> other <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> frontier <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> other <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> candidate))</span>
<span id="cb17-8">        weights.append(unique_wins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># +1 smoothing</span></span>
<span id="cb17-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> random.choices(frontier, weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>weights)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div></div>
<p>This focuses effort on candidates that have demonstrated <em>unique value</em>â€”they solve something nothing else can. Specialists get attention proportional to their specialization.</p>
<p><strong>2. Mutation vs Merge Decision</strong></p>
<p>Early iterations favor mutation (exploring from single candidates). As the frontier diversifies, merge becomes more valuable. The following is a simplified illustration of the decision logic:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> should_merge(frontier, iteration):</span>
<span id="cb18-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(frontier) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb18-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> iteration <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Let lineages diverge first</span></span>
<span id="cb18-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Merge with probability proportional to frontier diversity</span></span>
<span id="cb18-5">    n_lineages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(get_root(c) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> frontier))</span>
<span id="cb18-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> random.random() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> (n_lineages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(frontier)</span></code></pre></div></div>
<p><em>(The actual GEPA implementation may use different thresholdsâ€”see the <a href="https://arxiv.org/abs/2507.19457">paper</a> for details.)</em></p>
<p><strong>3. Mini-Batch Gating</strong></p>
<p>Before expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:</p>
<blockquote class="blockquote">
<p><em>â€œWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p><strong>4. Pareto Update</strong></p>
<p>The frontier update follows the dominance logic we implemented earlier: - <strong>Reject</strong> new candidates dominated by existing ones (they add nothing) - <strong>Remove</strong> existing candidates dominated by the new one (theyâ€™re obsolete) - <strong>Keep</strong> all non-dominated candidates (each offers unique value)</p>
<hr>
</section>
<section id="complexity-analysis" class="level4">
<h4 class="anchored" data-anchor-id="complexity-analysis">Complexity Analysis</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 64%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mutation (3-4 rollouts + reflection)</td>
<td>3-4 LLM calls + 1 reflection call</td>
</tr>
<tr class="even">
<td>Mini-batch evaluation</td>
<td>3-4 metric evaluations</td>
</tr>
<tr class="odd">
<td>Full validation evaluation</td>
<td>N metric evaluations (N = valset size)</td>
</tr>
<tr class="even">
<td>Pareto check</td>
<td>O(F Ã— N) comparisons (F = frontier size)</td>
</tr>
</tbody>
</table>
<p>The mini-batch gate is crucial for efficiency. Most mutations failâ€”they either donâ€™t improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.</p>
<hr>
</section>
<section id="why-each-component-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-each-component-matters">Why Each Component Matters</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 37%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Without it</th>
<th>With it</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Textual feedback</strong></td>
<td>Optimizer sees only <code>score=0.6</code></td>
<td>Optimizer reads â€œwrong answer, expected X, got Y becauseâ€¦â€</td>
</tr>
<tr class="even">
<td><strong>Pareto selection</strong></td>
<td>Specialists discarded when aggregate drops</td>
<td>Specialists preserved if they solve anything unique</td>
</tr>
<tr class="odd">
<td><strong>Lineage tracking</strong></td>
<td>No memory of evolutionary history</td>
<td>Can identify divergent branches for merge</td>
</tr>
<tr class="even">
<td><strong>Merge operation</strong></td>
<td>Insights stay siloed in separate branches</td>
<td>Orthogonal discoveries can combine</td>
</tr>
<tr class="odd">
<td><strong>Mini-batch gating</strong></td>
<td>Evaluate every candidate fully</td>
<td>Reject obvious failures cheaply</td>
</tr>
</tbody>
</table>
<p>The components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.</p>
<hr>
</section>
<section id="what-gets-returned" class="level4">
<h4 class="anchored" data-anchor-id="what-gets-returned">What Gets Returned</h4>
<p>The algorithm returns <code>best_aggregate(pareto_frontier)</code>â€”the prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use <code>track_stats=True</code> to access all candidates and their per-instance scores.</p>
<hr>
<p><em>Next: The merge operation in detailâ€”how GEPA combines insights from divergent lineages.</em></p>
</section>
</section>
<section id="the-lineage-tree-and-system-aware-merge" class="level3">
<h3 class="anchored" data-anchor-id="the-lineage-tree-and-system-aware-merge">The Lineage Tree and System-Aware Merge</h3>
<p>We touched on merge briefly in the algorithm overviewâ€”now letâ€™s see why itâ€™s essential and how it actually works.</p>
<p><strong>What â€œsystem-awareâ€ means</strong>: Unlike genetic algorithm crossover (which blindly swaps segments), GEPAâ€™s merge uses an LLM that <em>understands</em> what itâ€™s combiningâ€”it can resolve contradictions, synthesize complementary strategies, and produce coherent instructions rather than jumbled concatenations.</p>
<p>Pareto selection preserves specialistsâ€”but it creates a new problem: <strong>insights get siloed in separate branches</strong>.</p>
<p>Consider what happens after 10 iterations of GEPA on AIME problems:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.risheekkumar.in/posts/gepa-deepdive/static/Merge_Visual.webp" class="img-fluid figure-img"></p>
<figcaption>Lineage Tree with Merge Operation</figcaption>
</figure>
</div>
<p>The P2â†’P4 lineage accumulated <strong>algebra insights</strong>. The P3â†’P5 lineage accumulated <strong>number theory insights</strong>. Both survive on the Pareto frontier because each solves problems the other canâ€™t.</p>
<p>But what about a problem requiring <em>both</em>?</p>
<hr>
<section id="the-recombination-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-recombination-problem">The Recombination Problem</h4>
<p>Suppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:</p>
<ul>
<li>P4 knows to subtract equations pairwise, but doesnâ€™t think to reduce mod p</li>
<li>P5 knows CRT, but misses the algebraic setup</li>
</ul>
<p>With mutation alone, P4 would need to <em>independently rediscover</em> number theory (which P5 already knows), or vice versa. The knowledge exists in our populationâ€”just in different branches.</p>
<p><strong>Merge</strong> solves this by combining insights from divergent lineages into a single candidate.</p>
<hr>
</section>
<section id="how-merge-works" class="level4">
<h4 class="anchored" data-anchor-id="how-merge-works">How Merge Works</h4>
<p>GEPAâ€™s merge is â€œsystem-awareâ€â€”it understands <em>what</em> itâ€™s combining, not just concatenating strings. The reflection LLM receives:</p>
<ol type="1">
<li><strong>Both parent prompts</strong> with their full instruction text</li>
<li><strong>Lineage context</strong> â€” what types of problems each lineage solved</li>
<li><strong>Conflict guidance</strong> â€” instructions to resolve contradictions, not ignore them</li>
</ol>
<p>The prompt asks the LLM to <em>synthesize</em>, not concatenate:</p>
<blockquote class="blockquote">
<p>â€œCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Donâ€™t simply concatenateâ€”synthesize into coherent guidance.â€</p>
</blockquote>
<p><strong>Concrete example</strong> â€” merging our AIME specialists:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 30%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Parent</th>
<th>Specialty</th>
<th>Key instruction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P4</td>
<td>Algebra</td>
<td>â€œSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.â€</td>
</tr>
<tr class="even">
<td>P5</td>
<td>Number theory</td>
<td>â€œApply CRT for modular constraints. Check for contradictions.â€</td>
</tr>
</tbody>
</table>
<p><strong>Merged offspring P6:</strong> &gt; â€œApproach: (1) For equation systems, subtract pairwise to expose factor relationshipsâ€”enumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.â€</p>
<p>P6 inherits both toolkits AND adds <strong>meta-knowledge</strong> about when to combine them. This is the â€œsystem-awareâ€ partâ€”the merge understands the semantics of what itâ€™s combining.</p>
<hr>
</section>
<section id="when-to-merge-vs.-mutate" class="level4">
<h4 class="anchored" data-anchor-id="when-to-merge-vs.-mutate">When to Merge vs.&nbsp;Mutate</h4>
<p>GEPA alternates between operations based on frontier state:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Condition</th>
<th>Operation</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Early iterations (&lt; 5)</td>
<td>Mutate</td>
<td>Let lineages diverge first; nothing to merge yet</td>
</tr>
<tr class="even">
<td>Frontier has one dominant lineage</td>
<td>Mutate</td>
<td>No orthogonal insights to combine</td>
</tr>
<tr class="odd">
<td>Frontier has divergent specialists</td>
<td>Merge</td>
<td>Recombine discoveries from parallel explorations</td>
</tr>
<tr class="even">
<td>Recent merge succeeded</td>
<td>Mutate</td>
<td>Refine the merged candidate</td>
</tr>
</tbody>
</table>
<p>The paper describes the decision:</p>
<blockquote class="blockquote">
<p><em>â€œAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<hr>
</section>
<section id="merge-vs.-genetic-algorithm-crossover" class="level4">
<h4 class="anchored" data-anchor-id="merge-vs.-genetic-algorithm-crossover">Merge vs.&nbsp;Genetic Algorithm Crossover</h4>
<p>GEPAâ€™s merge is analogous to crossover in genetic algorithms, but smarter:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 62%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Genetic Algorithms</th>
<th>GEPA Merge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Genes = bit positions</td>
<td>Insights = natural language instructions</td>
</tr>
<tr class="even">
<td>Crossover = swap bit segments randomly</td>
<td>Merge = LLM synthesizes with understanding</td>
</tr>
<tr class="odd">
<td>Can create invalid offspring</td>
<td>Can resolve contradictions</td>
</tr>
<tr class="even">
<td>Blind to semantics</td>
<td>Aware of what instructions <em>mean</em></td>
</tr>
</tbody>
</table>
<p>Random crossover might produce: â€œSubtract equations pairwise. Apply CRT. Subtract equations pairwise.â€ (nonsense duplication)</p>
<p>LLM merge produces: â€œSet up algebra first, then apply number-theoretic constraints.â€ (coherent synthesis)</p>
<hr>
</section>
<section id="the-compounding-effect" class="level4">
<h4 class="anchored" data-anchor-id="the-compounding-effect">The Compounding Effect</h4>
<p>The components reinforce each other:</p>
<ol type="1">
<li><strong>Pareto selection</strong> preserves the diversity that makes merge valuable</li>
<li><strong>Lineage tracking</strong> identifies which candidates come from divergent branches<br>
</li>
<li><strong>Merge</strong> recombines orthogonal discoveries into unified candidates</li>
<li><strong>Pareto selection</strong> then preserves successful merges alongside remaining specialists</li>
</ol>
<p>Without Pareto selection, thereâ€™s nothing interesting to mergeâ€”youâ€™d just have variants of one â€œbestâ€ prompt. Without merge, insights stay siloed even when the frontier is diverse.</p>
<p>Mutation explores <em>depth</em>â€”refining one approach through successive reflections.<br>
Merge explores <em>breadth</em>â€”combining orthogonal discoveries from parallel paths.</p>
<p>Together, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.</p>
</section>
</section>
<section id="what-gepa-learns-domain-specific-knowledge-encoding" class="level3">
<h3 class="anchored" data-anchor-id="what-gepa-learns-domain-specific-knowledge-encoding">What GEPA Learns: Domain-Specific Knowledge Encoding</h3>
<p>Weâ€™ve built the full algorithmâ€”reflective mutation, Pareto selection, merge. But what does all this machinery actually <em>produce</em>? Letâ€™s examine the output: prompts that encode domain expertise.</p>
<p>One of GEPAâ€™s most striking capabilities is its ability to <strong>encode domain-specific knowledge directly into prompts</strong>â€”transforming tacit expertise into explicit instructions that persist across examples.</p>
<section id="prompts-as-knowledge-containers" class="level4">
<h4 class="anchored" data-anchor-id="prompts-as-knowledge-containers">Prompts as Knowledge Containers</h4>
<p>Traditional optimization treats prompts as opaque strings to be scored. GEPA treats them as <strong>knowledge containers</strong> that accumulate insights through the reflection loop:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.risheekkumar.in/posts/gepa-deepdive/static/failure_acc_experience.webp" class="img-fluid figure-img"></p>
<figcaption>Failure Accumulation Experience</figcaption>
</figure>
</div>
<p>Each iteration doesnâ€™t just fix one errorâ€”it extracts the <em>lesson</em> behind the error.</p>
<p><strong>Concrete example from our AIME experiments:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Prompt excerpt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Seed</strong></td>
<td>â€œYou are given a problem and you have to give the answer along with reasoning.â€</td>
</tr>
<tr class="even">
<td><strong>After iter 2</strong></td>
<td>â€œâ€¦For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.â€</td>
</tr>
<tr class="odd">
<td><strong>After iter 3</strong></td>
<td>â€œâ€¦When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.â€</td>
</tr>
</tbody>
</table>
<p>The prompt evolved from generic instruction to encoding <strong>competition math heuristics</strong>. The LLM didnâ€™t invent theseâ€”it extracted them from its training knowledge, triggered by seeing specific failure modes.</p>
</section>
<section id="three-categories-of-captured-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="three-categories-of-captured-knowledge">Three Categories of Captured Knowledge</h4>
<p>We observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):</p>
<p><strong>1. Format and Interface Knowledge</strong> - Output schemas (â€œreturn JSON with keys: answer, reasoningâ€) - API conventions (â€œuse library.method(), not library_method()â€) - Parsing requirements (â€œintegers only, no leading zerosâ€)</p>
<p>This is easiest to extractâ€”format errors produce explicit feedback.</p>
<p><strong>2. Strategic Knowledge</strong> - Problem-solving heuristics (â€œtry small cases firstâ€) - Domain patterns (â€œcompetition problems often combine algebra and number theoryâ€) - Failure mode awareness (â€œwatch for off-by-one errors in countingâ€)</p>
<p>This emerges from reflecting on <em>why</em> approaches failed, not just <em>that</em> they failed.</p>
<p><strong>3. Factual Domain Knowledge</strong> - API names and signatures (â€œtorch.einsum, not torch.einstein_sumâ€) - Domain constants (â€œstandard gravity = 9.81 m/sÂ²â€) - Constraint relationships (â€œin valid Sudoku, each row/column/box contains 1-9 exactly onceâ€)</p>
<p>The LLM already knows thisâ€”reflection surfaces it into the prompt where itâ€™s consistently applied.</p>
</section>
<section id="why-prompts-beat-weights-sometimes" class="level4">
<h4 class="anchored" data-anchor-id="why-prompts-beat-weights-sometimes">Why Prompts Beat Weights (Sometimes)</h4>
<p>Fine-tuning encodes knowledge in model weightsâ€”opaque, distributed, hard to inspect. GEPA encodes knowledge in natural languageâ€”readable, editable, composable.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>Fine-tuning</th>
<th>GEPA prompts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Inspectability</strong></td>
<td>Black box</td>
<td>Human-readable instructions</td>
</tr>
<tr class="even">
<td><strong>Editability</strong></td>
<td>Requires retraining</td>
<td>Edit the text directly</td>
</tr>
<tr class="odd">
<td><strong>Composability</strong></td>
<td>Train new model</td>
<td>Merge prompt sections</td>
</tr>
<tr class="even">
<td><strong>Sample efficiency</strong></td>
<td>Thousands of examples</td>
<td>Tens of examples</td>
</tr>
</tbody>
</table>
<p>A GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.</p>
</section>
<section id="the-preservation-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-preservation-problem">The Preservation Problem</h4>
<p>Knowledge accumulation has a failure mode: <strong>new insights can overwrite old ones</strong>. We saw this in our hands-on experimentâ€”iteration 3â€™s number theory insights overwrote iteration 2â€™s algebra insights, causing catastrophic forgetting.</p>
<p>This is precisely why GEPA uses <strong>Pareto selection</strong>. By preserving prompts that are best on <em>any</em> validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontierâ€”and the merge operation can later combine their insights.</p>
<blockquote class="blockquote">
<p><em>Without Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.</em></p>
</blockquote>
</section>
<section id="limitation-knowledge-must-be-triggerable" class="level4">
<h4 class="anchored" data-anchor-id="limitation-knowledge-must-be-triggerable">Limitation: Knowledge Must Be Triggerable</h4>
<p>GEPA can only surface knowledge the base LLM already has. If the model doesnâ€™t know that â€œCRTâ€ means Chinese Remainder Theorem, no amount of reflection will discover it. The optimization extracts and organizes existing knowledgeâ€”it doesnâ€™t create new knowledge.</p>
<p>This is why GEPA works better with stronger base models: more latent knowledge to extract. And why, for domains requiring knowledge the LLM lacks, youâ€™ll need to inject it through few-shot examples, retrieval augmentation, or explicit instructions.</p>
<blockquote class="blockquote">
<p><em>â€œLanguage models already have built a lot of prior knowledgeâ€¦ we can use all of this natural language information to make the LLM itself reflect on its mistakes and improve.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<hr>
<p><em>Now that we understand the algorithm and what it produces, letâ€™s look at how to use GEPA in practice.</em></p>
</section>
</section>
</section>
<section id="beyond-training-gepa-for-inference-time-search" class="level2">
<h2 class="anchored" data-anchor-id="beyond-training-gepa-for-inference-time-search">Beyond Training: GEPA for Inference-Time Search</h2>
<p>Everything weâ€™ve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.</p>
<section id="two-paradigms-of-operation" class="level3">
<h3 class="anchored" data-anchor-id="two-paradigms-of-operation">Two Paradigms of Operation</h3>
<p><strong>Train-then-generalize</strong> (what weâ€™ve built so far): - Optimize prompts on a training set - Select the best-aggregate prompt from the Pareto frontier - Deploy that prompt on new, unseen tasks - Goal: learn <em>generalizable</em> lessons that transfer</p>
<p><strong>Test-time search</strong> (inference-time optimization): - You have a batch of hard tasks you need to solve <em>now</em> - Optimize directly on the tasks themselves - GEPA searches for solutions, storing the best prompt <em>per task</em> - Goal: maximize performance on <em>these specific instances</em></p>
<p>The mechanics are identicalâ€”reflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, youâ€™re using GEPA as a <strong>search algorithm</strong> over the solution space. This aligns with the broader trend of <a href="https://openai.com/index/learning-to-reason-with-llms/">inference-time compute scaling</a>â€”investing more computation at inference to solve harder problems.</p>
<p><strong>The key mechanical change</strong>: pass <code>valset=trainset</code> (or equivalently, omit <code>valset</code> and set <code>trainset</code> to your target tasks). This tells GEPA to optimize directly on the problems youâ€™re trying to solve rather than holding out a separate validation set. See the <a href="https://dspy.ai/api/optimizers/GEPA/overview/">GEPA API documentation</a> for full parameter details.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Test-time search: optimize directly on the tasks you want to solve</span></span>
<span id="cb19-2">optimized <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GEPA(metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>metric_with_feedback, auto<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"medium"</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(</span>
<span id="cb19-3">    program,</span>
<span id="cb19-4">    trainset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>hard_problems,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The actual tasks you need solved</span></span>
<span id="cb19-5">    valset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>hard_problems,    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Same setâ€”no held-out validation</span></span>
<span id="cb19-6">)</span></code></pre></div></div>
<blockquote class="blockquote">
<p><em>â€œGiven a batch of tasks that we want to solve and given some budgetâ€¦ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<hr>
</section>
<section id="why-gepa-beats-high-temperature-sampling" class="level3">
<h3 class="anchored" data-anchor-id="why-gepa-beats-high-temperature-sampling">Why GEPA Beats High-Temperature Sampling</h3>
<p>Traditional inference-time compute strategies generate many candidates by sampling at high temperature, then use a verifier or LLM-as-judge to pick the best one. But these samples tend to be <em>similar</em>â€”variations on the same approach.</p>
<p>Recent research on <a href="https://arxiv.org/abs/2408.03314">test-time compute scaling</a> (Snell et al., 2024) shows that â€œcompute-optimalâ€ strategiesâ€”which adaptively allocate inference budget based on problem difficultyâ€”can improve efficiency by <strong>more than 4x</strong> compared to traditional best-of-N sampling. In some cases, smaller models with optimized test-time compute outperform models <strong>14x larger</strong> that donâ€™t use additional inference-time computation.</p>
<p><strong>The intuition</strong>: High-temperature sampling produces variations <em>around</em> the same approachâ€”different variable names, slightly different loop bounds, minor syntactic choices. GEPAâ€™s reflective mutation proposes <em>structurally different</em> strategies based on what went wrong. When the feedback says â€œmemory bandwidth bottleneck,â€ the next candidate might switch from a naive loop to shared-memory tilingâ€”a qualitative change that temperature variation rarely discovers.</p>
<p>The Pareto frontier then preserves these diverse strategies rather than converging on a single â€œbestâ€ approach.</p>
<p>GEPA induces <strong>genuine diversity</strong> through two mechanisms:</p>
<ol type="1">
<li><strong>Pareto tracking</strong> â€” maintains candidates that each excel at <em>something different</em>, not just the single highest scorer</li>
<li><strong>Reflective mutation</strong> â€” proposes structurally different approaches based on <em>what went wrong</em>, not random perturbations</li>
</ol>
<blockquote class="blockquote">
<p><em>â€œDue to the way GEPA operates where itâ€™s doing the Pareto candidate tracking and itâ€™s doing reflective mutation, we find that GEPA is itself inducing a huge amount of diversity in the kinds of solutions that it generates.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>Rather than 100 slight variations of one approach, GEPA maintains a frontier of genuinely different strategies.</p>
<p><strong>Concrete results</strong>: On the <a href="https://arxiv.org/abs/2103.03874">MATH benchmark</a>, GEPA achieves <strong>93% accuracy</strong> compared to 67% with basic DSPy ChainOfThoughtâ€”a 26 percentage point improvement from prompt optimization alone, no fine-tuning required.</p>
<hr>
</section>
<section id="self-bootstrapping-how-iteration-compounds" class="level3">
<h3 class="anchored" data-anchor-id="self-bootstrapping-how-iteration-compounds">Self-Bootstrapping: How Iteration Compounds</h3>
<p>GEPAâ€™s iterative process creates a self-bootstrapping dynamic:</p>
<ol type="1">
<li><strong>Round 1</strong>: Generate rollouts â†’ get feedback (e.g., compiler error) â†’ reflect â†’ propose fix</li>
<li><strong>Round 2</strong>: The code compiles, but now thereâ€™s a runtime error (division by zero) â†’ reflect â†’ propose fix</li>
<li><strong>Round 3</strong>: Runtime works, but output is wrong â†’ reflect â†’ propose fix</li>
</ol>
<blockquote class="blockquote">
<p><em>â€œIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So itâ€™s kind of like self-bootstrapping data to train itself.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>Each iteration surfaces a new failure mode and proposes a solutionâ€”generating increasingly challenging training signal from the task itself.</p>
<hr>
</section>
<section id="cross-task-transfer-within-a-batch" class="level3">
<h3 class="anchored" data-anchor-id="cross-task-transfer-within-a-batch">Cross-Task Transfer Within a Batch</h3>
<p>When solving related tasks (e.g., a batch of <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">CUDA kernels</a>), insights compound across the batch:</p>
<blockquote class="blockquote">
<p><em>â€œAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>The frontier maintains multiple specialized prompts simultaneously:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Specialization</th>
<th>Problems solved</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P_conv</td>
<td>Convolutional operators</td>
<td>#1, #4, #7</td>
</tr>
<tr class="even">
<td>P_reduce</td>
<td>Reduction/summation operators</td>
<td>#2, #5, #8</td>
</tr>
<tr class="odd">
<td>P_matmul</td>
<td>Matrix multiplication</td>
<td>#3, #6</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><em>â€œOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>When a new problem arrives, GEPA tries candidates from across the frontierâ€”the convolution specialist might crack it, or the reduction specialist, or insights from both might merge.</p>
<hr>
</section>
<section id="what-gepa-stores" class="level3">
<h3 class="anchored" data-anchor-id="what-gepa-stores">What GEPA Stores</h3>
<p>For each task in the batch, GEPA tracks both artifacts:</p>
<blockquote class="blockquote">
<p><em>â€œGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<ul>
<li><strong>Best outputs</strong> â€” the actual solutions, ready to use</li>
<li><strong>Best prompts</strong> â€” specialized strategies representing different subdomains of your problem space</li>
</ul>
<p>You can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.</p>
<hr>
</section>
<section id="feedback-design-for-each-paradigm" class="level3">
<h3 class="anchored" data-anchor-id="feedback-design-for-each-paradigm">Feedback Design for Each Paradigm</h3>
<p>The paradigm choice shapes how you design feedback:</p>
<p><strong>For train-then-generalize</strong> â€” extract transferable lessons:</p>
<blockquote class="blockquote">
<p><em>â€œYou will ensure that there is no task-specific insights that the prompt captures. So you will write in your feedback something along the lines of â€˜try to extract lessons out of this.â€™â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> metric_generalizable(gold, pred, trace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb20-2">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(gold.answer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> pred.answer)</span>
<span id="cb20-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb20-4">        feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb20-5">            <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Failed: expected </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>gold<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>answer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, got </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>pred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>answer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">. "</span></span>
<span id="cb20-6">            <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Extract a GENERAL lesson that would help on similar problems."</span></span>
<span id="cb20-7">        )</span>
<span id="cb20-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb20-9">        feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Correct. What general strategy led to success?"</span></span>
<span id="cb20-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"score"</span>: score, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"feedback"</span>: feedback}</span></code></pre></div></div>
<p><strong>For test-time search</strong> â€” hyper-specialize to the task:</p>
<blockquote class="blockquote">
<p><em>â€œIf youâ€™re doing simply an inference time search where you just care about the final outputs, then you will try to provide feedback which is as hyper-specialized to the task as possible.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> metric_specialized(gold, pred, trace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb21-2">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(gold.answer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> pred.answer)</span>
<span id="cb21-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb21-4">        feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb21-5">            <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Your code failed with this particular compiler error: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>pred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>error<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">. "</span></span>
<span id="cb21-6">            <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Try to improve on this specific thing."</span></span>
<span id="cb21-7">        )</span>
<span id="cb21-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb21-9">        feedback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Correct."</span></span>
<span id="cb21-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"score"</span>: score, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"feedback"</span>: feedback}</span></code></pre></div></div>
<hr>
</section>
<section id="the-tutor-analogy-background-optimization-loops" class="level3">
<h3 class="anchored" data-anchor-id="the-tutor-analogy-background-optimization-loops">The Tutor Analogy: Background Optimization Loops</h3>
<p>This iterative pattern mirrors how students already work with AI tutoring systems:</p>
<ol type="1">
<li>Attempt problem â†’ get solution</li>
<li>See an error â†’ receive feedback</li>
<li>Tutor explains â†’ improved understanding</li>
<li>Repeat until mastered</li>
</ol>
<p>This suggests an intriguing application: <strong>background GEPA loops for personalization</strong>. Similar patterns are emerging in tools like <a href="https://www.cursor.com/">Cursor</a> and other AI-assisted development environments.</p>
<blockquote class="blockquote">
<p><em>â€œFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing promptâ€¦ cursor can learn a user-specific prompt that works well specifically for you.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>Every time you provide feedback on an AI-generated solution, a background loop could update not just the current response but a <em>user-specific prompt</em> capturing your preferences and patterns.</p>
<hr>
</section>
<section id="when-not-to-use-test-time-gepa" class="level3">
<h3 class="anchored" data-anchor-id="when-not-to-use-test-time-gepa">When NOT to Use Test-Time GEPA</h3>
<ul>
<li><strong>Sparse feedback</strong> â€” If your metric only returns pass/fail with no explanation, GEPA canâ€™t extract lessons</li>
<li><strong>Tasks outside LLM knowledge</strong> â€” GEPA surfaces knowledge the model already has; genuinely novel information wonâ€™t emerge from reflection. For such domains, consider <a href="https://arxiv.org/abs/2005.11401">retrieval-augmented generation</a> to inject external knowledge</li>
<li><strong>Tight latency constraints</strong> â€” Test-time GEPA typically runs 5-20 iterations, each involving LLM calls for rollout + reflection. Budget 30 seconds to 5 minutes per task depending on model latency and iteration count. Interactive use cases need a different approach.</li>
<li><strong>Identical tasks</strong> â€” If your batch contains <em>identical</em> problems (not just related ones), optimize once and reuse. Cross-task transfer helps when problems are <em>related but distinct</em>â€”like different CUDA kernels that share optimization patterns but have different structures.</li>
</ul>
<hr>
<p>When the conditions <em>are</em> rightâ€”rich feedback, high-value tasks worth the compute, domains where the LLM has strong priorsâ€”test-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.</p>
</section>
<section id="case-study-code-optimization-for-novel-hardware" class="level3">
<h3 class="anchored" data-anchor-id="case-study-code-optimization-for-novel-hardware">Case Study: Code Optimization for Novel Hardware</h3>
<p>The GEPA paper demonstrates test-time search on domains where GEPAâ€™s strengths shine brightest: <strong>code optimization for hardware with limited pre-training data</strong>.</p>
<section id="amd-npu-kernels-optimization-without-pre-training-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="amd-npu-kernels-optimization-without-pre-training-knowledge">AMD NPU Kernels: Optimization Without Pre-Training Knowledge</h4>
<p>AMDâ€™s NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The <a href="https://arxiv.org/abs/2507.14403">NPUEval benchmark</a> reveals just how challenging this is: even with compiler feedback and retrieval-augmented generation (RAG), state-of-the-art LLMs achieve only ~10% mean vectorization score across the dataset.</p>
<p>GEPA succeeds here because it doesnâ€™t need prior examples. Instead, it:</p>
<ol type="1">
<li><strong>Generates an initial kernel</strong> based on generic programming knowledge</li>
<li><strong>Compiles and runs</strong> â†’ receives compiler errors or performance metrics</li>
<li><strong>Reflects on feedback</strong> â†’ proposes targeted improvements</li>
<li><strong>Iterates</strong> until the kernel compiles, runs correctly, and performs well</li>
</ol>
<p><strong>Why GEPA fits</strong>: The GEPA paper demonstrates results on NPUEval, showing that reflective mutation can extract optimization patterns even for hardware with minimal pre-training coverageâ€”without requiring retraining or RAG.</p>
<blockquote class="blockquote">
<p><em>â€œGEPA can be used to generate optimized kernels for AMDâ€™s NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>The compiler error messages contain the fix. â€œSymbol not found: <code>npu_matmul</code>â€ triggers reflection that surfaces the correct API. â€œMemory alignment error at line 47â€ points directly to what needs fixing.</p>
</section>
<section id="cuda-kernels-outperforming-human-baselines" class="level4">
<h4 class="anchored" data-anchor-id="cuda-kernels-outperforming-human-baselines">CUDA Kernels: Outperforming Human Baselines</h4>
<p><a href="https://scalingintelligence.stanford.edu/blogs/kernelbench/">KernelBench</a> (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernelsâ€”a task where even frontier models struggle. The benchmark reveals a sobering baseline: <strong>frontier reasoning models match the PyTorch baseline on less than 20% of tasks</strong> using the fastâ‚ metric (kernels that are both correct <em>and</em> faster than PyTorch). This isnâ€™t a matter of promptingâ€”efficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models havenâ€™t learned to apply reliably.</p>
<p><strong>The impact of iterative feedback</strong>: The KernelBench paper demonstrates that feedback-driven refinement dramatically improves results. By providing execution results and profiler feedback in context, fastâ‚ scores improved from 12%, 36%, and 12% to <strong>43%, 72%, and 18%</strong> respectively across their test configurations. This is exactly the mechanism GEPA exploitsâ€”but with systematic Pareto tracking and reflective mutation rather than ad-hoc iteration.</p>
<blockquote class="blockquote">
<p><em>â€œFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p><strong>Related evolutionary approaches show further gains</strong>: Stanfordâ€™s separate test-time evolutionary search (distinct from GEPA, published in their <a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html">Fast Kernels blog post</a>) produces kernels achieving <strong>103-133% of PyTorch reference performance</strong> on foundational operators like Conv2Dâ€”demonstrating that structured search with execution feedback can exceed human-optimized baselines.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 44%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Performance</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base LLM (one-shot)</td>
<td>~3-15% fastâ‚</td>
<td>KernelBench paper</td>
</tr>
<tr class="even">
<td>Frontier reasoning (o1, R1)</td>
<td>Match baseline on &lt;20% of tasks</td>
<td>KernelBench paper</td>
</tr>
<tr class="odd">
<td>With execution + profiler feedback</td>
<td>43-72% fastâ‚ (3-6x improvement)</td>
<td>KernelBench paper</td>
</tr>
<tr class="even">
<td>Evolutionary test-time search</td>
<td>103-133% of PyTorch on select kernels</td>
<td>Stanford CRFM (separate from GEPA)</td>
</tr>
<tr class="odd">
<td>GEPA</td>
<td>â€œSignificant speedups over PyTorch-eagerâ€</td>
<td>GEPA paper Fig. 11</td>
</tr>
</tbody>
</table>
<p>The gap matters commercially: <a href="https://scalingintelligence.stanford.edu/blogs/kernelbench/">efficient compilers often lag behind new GPU architectures by over two years</a>â€”approximately one year for CUDA experts to develop optimized implementations and another year to generalize into compilers. GEPA-style optimization could bridge that gap by generating optimized kernels for new hardware before traditional toolchains catch up.</p>
<p><strong>Why code optimization is ideal for GEPA:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 41%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Why it helps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Rich textual feedback</strong></td>
<td>Compiler errors, profiler output, runtime exceptions all explain <em>why</em> something failed</td>
</tr>
<tr class="even">
<td><strong>Verifiable correctness</strong></td>
<td>Unit tests and benchmarks provide unambiguous signal</td>
</tr>
<tr class="odd">
<td><strong>Iterative refinement</strong></td>
<td>Each failed compilation reveals the next fix to try</td>
</tr>
<tr class="even">
<td><strong>Cross-task transfer</strong></td>
<td>Insights about memory coalescing on one kernel help others</td>
</tr>
</tbody>
</table>
</section>
<section id="the-self-bootstrapping-dynamic" class="level4">
<h4 class="anchored" data-anchor-id="the-self-bootstrapping-dynamic">The Self-Bootstrapping Dynamic</h4>
<p>A typical GEPA optimization trajectory for CUDA kernels follows this pattern:</p>
<p><strong>Iteration 1</strong>: Generate naive kernel â†’ compiler error â€œundeclared identifier <code>threadIdx</code>â€ - Reflection: â€œCUDA kernels require explicit thread indexing. Add <code>threadIdx.x</code> and <code>blockIdx.x</code>.â€</p>
<p><strong>Iteration 2</strong>: Compiles, but runtime error (out of bounds) - Reflection: â€œNeed bounds checking. Add <code>if (idx &lt; n)</code> guard.â€</p>
<p><strong>Iteration 3</strong>: Runs correctly, but 10x slower than baseline - Profiler feedback: â€œMemory bandwidth: 12% of peak. Non-coalesced access pattern.â€ - Reflection: â€œReorganize memory access for coalescing. Use shared memory for reductions.â€</p>
<p><strong>Iteration 4</strong>: 1.2x faster than PyTorch baseline</p>
<p>Each iteration surfaced a new challenge <em>and</em> its solutionâ€”generating increasingly sophisticated training signal from the task itself.</p>
</section>
<section id="cross-kernel-transfer" class="level4">
<h4 class="anchored" data-anchor-id="cross-kernel-transfer">Cross-Kernel Transfer</h4>
<p>When optimizing a batch of related kernels, insights compound across tasks:</p>
<blockquote class="blockquote">
<p><em>â€œIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p>GEPA exploits the relatedness of kernels in a batch. As it optimizes each kernel, the Pareto frontier accumulates specialized promptsâ€”one excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. When a new kernel arrives, candidates from across the frontier are tried. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax.</p>
<p>This cross-task transfer is why batch optimization outperforms solving each kernel independentlyâ€”the accumulated optimization knowledge benefits later tasks.</p>
</section>
<section id="beyond-kernels" class="level4">
<h4 class="anchored" data-anchor-id="beyond-kernels">Beyond Kernels</h4>
<p>The same pattern extends to other code optimization domains where execution produces rich textual feedback:</p>
<blockquote class="blockquote">
<p><em>â€œThere are many other tasks where it can be usedâ€”for example, with unit tests to generate code patches.â€</em> â€” Lakshya A Agrawal</p>
</blockquote>
<p><strong>Bug fixing</strong> (test failures explain whatâ€™s wrong), <strong>performance optimization</strong> (profiler output identifies bottlenecks), and <strong>API migration</strong> (deprecation warnings specify changes) all fit GEPAâ€™s feedback-driven model.</p>
</section>
</section>
</section>
<section id="conclusion-when-to-reach-for-gepa" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-when-to-reach-for-gepa">Conclusion: When to Reach for GEPA</h2>
<p>We opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculusâ€”matching RLâ€™s optimization performance at a fraction of the sample cost by doing something RL canâ€™t: <em>reading the feedback</em>.</p>
<p>The results speak for themselves: 46.6% â†’ 56.6% on AIME math competition problems. 67% â†’ 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.</p>
<p>This represents a genuinely new point in the optimization design spaceâ€”one that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. Thatâ€™s not incremental improvementâ€”itâ€™s a 100x reduction in sample requirements.</p>
<hr>
<section id="use-gepa-for-train-then-generalize-when" class="level3">
<h3 class="anchored" data-anchor-id="use-gepa-for-train-then-generalize-when">Use GEPA for Train-Then-Generalize When:</h3>
<ul>
<li>You have <strong>rich textual feedback</strong> (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)</li>
<li>Your evaluation budget is <strong>limited</strong> (50-500 examples, not 5,000)</li>
<li>Youâ€™re optimizing <strong>compound AI systems</strong> where prompts orchestrate multi-step pipelines</li>
<li>You need <strong>interpretable results</strong>â€”prompts you can read, edit, and reason about</li>
</ul>
</section>
<section id="use-gepa-for-test-time-search-when" class="level3">
<h3 class="anchored" data-anchor-id="use-gepa-for-test-time-search-when">Use GEPA for Test-Time Search When:</h3>
<ul>
<li>You have a <strong>batch of high-value tasks</strong> worth the compute investment</li>
<li>Each task produces <strong>execution feedback</strong> (tests, profilers, validators)</li>
<li>Tasks are <strong>related enough</strong> for cross-task transfer to help</li>
</ul>
</section>
<section id="stick-with-traditional-approaches-when" class="level3">
<h3 class="anchored" data-anchor-id="stick-with-traditional-approaches-when">Stick with Traditional Approaches When:</h3>
<ul>
<li>You have <strong>abundant labeled data</strong> and compute budget for fine-tuning</li>
<li>Feedback is <strong>purely scalar</strong> with no explanatory signal</li>
<li>The task is <strong>already solved</strong> by few-shot prompting</li>
<li>You need <strong>sub-second latency</strong></li>
</ul>
<hr>
</section>
<section id="known-limitations" class="level3">
<h3 class="anchored" data-anchor-id="known-limitations">Known Limitations</h3>
<p>GEPA isnâ€™t a silver bullet:</p>
<ul>
<li><p><strong>Reflection quality varies by domain</strong> â€” GEPA excels when the LLM has strong prior knowledge. On highly specialized domains where the base model is weak, reflection produces generic advice rather than actionable fixes.</p></li>
<li><p><strong>Feedback bottleneck</strong> â€” The optimizer is only as good as its feedback. If your metric returns â€œwrongâ€ without explaining <em>why</em>, GEPA degrades to expensive random search.</p></li>
<li><p><strong>Validation set size</strong> â€” With fewer than 30 validation examples, prompts can overfit to idiosyncrasies of those specific instances.</p></li>
</ul>
<hr>
</section>
<section id="get-started" class="level3">
<h3 class="anchored" data-anchor-id="get-started">Get Started</h3>
<p>Ready to try GEPA on your own pipelines?</p>
<ul>
<li><strong><a href="https://dspy.ai/tutorials/gepa_aime/">GEPA for AIME Tutorial</a></strong> â€” Complete walkthrough from setup to optimized results</li>
<li><strong><a href="https://dspy.ai/api/optimizers/GEPA/overview/">GEPA API Reference</a></strong> â€” Full parameter documentation</li>
<li><strong><a href="https://arxiv.org/abs/2507.19457">Paper</a></strong> â€” Algorithm details and experimental methodology</li>
</ul>
<p>The core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads itâ€”and learns.</p>


</section>
</section>

 ]]></description>
  <category>code</category>
  <category>research paper</category>
  <category>analysis</category>
  <category>reimplementation</category>
  <guid>https://blog.risheekkumar.in/posts/gepa-deepdive/gepa_final_article.html</guid>
  <pubDate>Sat, 20 Dec 2025 18:30:00 GMT</pubDate>
  <media:content url="https://blog.risheekkumar.in/posts/gepa-deepdive/static/blogpost_image.webp" medium="image" type="image/webp"/>
</item>
</channel>
</rss>
