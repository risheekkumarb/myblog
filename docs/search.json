[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/itches/itches.html",
    "href": "posts/itches/itches.html",
    "title": "fastai example",
    "section": "",
    "text": "Importing Dataset\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\nimg = path.ls()[0]\nimg\n\nPath('/Users/risheekkumar/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg')\n\n\n\nimg = PILImage.create(img)\nimg\n\n\n\n\n\n\n\n\n\nimg.to_thumb(192)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "",
    "text": "Write and speak about AI-powered product, engineering and evals.\nPrototype apps like VLM Self-healing Agent; GEPA reimplementation"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html",
    "href": "posts/gepa-deepdive/gepa_final_article.html",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "This article was made possible by the research papers referenced throughout, the Weaviate discussion with Lakshya A Agrawal, and guidance from Kerem Turgutlu at answer.ai. Created using the solveit platform by fast.ai.\nYou‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.\nBut what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) achieves exactly this‚Äîby exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.\nadd in credit note about the solveit platform used to create this. This article was possible by all the research papers mentioned and also from weaviate article with Lakshya A Agrawal. embed the link - https://www.youtube.com/watch?v=fREQrxhBSk0 Also credit Kerem Turgutlu from answer.ai for the guide on the article."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#introduction-the-case-for-sample-efficient-prompt-optimization",
    "href": "posts/gepa-deepdive/gepa_final_article.html#introduction-the-case-for-sample-efficient-prompt-optimization",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "You‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.\nBut what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) achieves exactly this‚Äîby exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.\n\n\nTraditional AI optimization techniques‚Äîreinforcement learning and fine-tuning‚Äîhave achieved remarkable results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?\nConsider: - Agentic pipelines that invoke simulations, query rate-limited APIs, or run multi-step tool chains - Code generation for novel hardware, where each evaluation requires compiling for custom silicon and executing on the device - Complex reasoning tasks with expensive verification steps\nCollecting thousands of rollouts simply isn‚Äôt feasible in these settings.\nThe core issue: RL learns by comparison. A 500-step trajectory collapses to reward = 0.73. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectories‚Äîthis one scored 0.8, that one scored 0.4, what differed?‚Äîrequiring sample counts that expensive domains can‚Äôt support.\n\nRL and fine-tuning require generating large amounts of rollouts to gather scalar learning signals‚Äîsample inefficient by design.\n\nWhen each rollout costs minutes (or dollars), this approach breaks down.\n\n\n\nModern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:\n\nReasoning traces ‚Äî Chain-of-Thought and ReAct logs expose the model‚Äôs explicit thought process. When a multi-hop QA system fails, you can see where the reasoning went wrong: ‚ÄúThe capital of France is Paris. Paris is in Germany‚Ä¶‚Äù The failure mode is visible in the text.\nEnvironment feedback ‚Äî Compiler errors don‚Äôt just say ‚Äúfailed.‚Äù They say cannot find symbol 'x', did you mean 'y'? API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.\nEvaluation rubrics ‚Äî LLM-as-judge systems don‚Äôt just score 3/5. They explain: ‚ÄúResponse was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.‚Äù\n\nEach of these is dramatically richer than reward = 0.73.\nThe key realization: these traces aren‚Äôt just logs for debugging‚Äîthey‚Äôre potential input to the optimizer. A compiler error that says ‚Äúdid you mean ‚Äòy‚Äô?‚Äù contains the fix. A rubric that says ‚Äútoo verbose‚Äù specifies exactly what to change.\nTraditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.\nBut what if we could just‚Ä¶ read the feedback?\nThis is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?\n\n\n\nHere‚Äôs the key insight enabling a new optimization paradigm: LLMs already have prior knowledge about the domains they‚Äôre working in, and they‚Äôre increasingly capable of self-reflection.\nConsider what happens with different types of feedback:\nCompiler errors ‚Äî When the compiler returns cannot find symbol 'x', did you mean 'y'?, the LLM doesn‚Äôt need thousands of examples to learn the fix. It already knows the library‚Äôs API. One error message is enough.\n\n‚ÄúThe language model already knows that x is not a valid API name in the library but y is. Next time I should try this.‚Äù ‚Äî Lakshya A Agrawal\n\nLLM-as-judge feedback ‚Äî When a judge says ‚Äúyour summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,‚Äù the model can directly incorporate ‚Äúbe concise, match casual tone‚Äù into its next attempt. No statistical signal extraction required.\nReasoning trace failures ‚Äî When a multi-hop QA trace shows the model correctly retrieved ‚ÄúParis is the capital of France‚Äù but then hallucinated ‚ÄúParis is in Germany,‚Äù the failure point is visible in the text. You can see exactly where the reasoning derailed.\nPrivacy-aware rewriting (PUPA task) ‚Äî In the paper‚Äôs experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains why a rewrite failed‚Äî‚Äúleaked the user‚Äôs company name‚Äù or ‚Äúremoved too much context, degrading response quality‚Äù‚Äîgiving the optimizer actionable signal from each example.\n\n\nThis is the fundamental difference GEPA exploits:\n\n\n\n\n\n\n\nApproach\nHow it learns\n\n\n\n\nRL\nCompare thousands of trajectories statistically: ‚ÄúThese 500 scored 0.8, those 500 scored 0.4‚Äîwhat differed?‚Äù\n\n\nReflection\nRead the feedback directly: ‚ÄúThe compiler said use y, so use y.‚Äù\n\n\n\nRL would need hundreds of rollouts to statistically isolate that x‚Üíy is the fix. The LLM gets it from one error message.\n\n\n\nBetter still, modern LLMs don‚Äôt just extract point fixes‚Äîthey can derive generalizable lessons:\n\nNot just ‚Äúuse y instead of x‚Äù ‚Üí but ‚Äúalways verify symbol names against the library‚Äôs namespace before generating code‚Äù\nNot just ‚Äúresponse was too long‚Äù ‚Üí but ‚Äúfor Slack contexts, limit responses to 150 words and use bullet points‚Äù\nNot just ‚Äúleaked company name‚Äù ‚Üí but ‚Äúscan for proper nouns and replace with generic placeholders‚Äù\n\n\n‚ÄúLLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.‚Äù ‚Äî Lakshya A Agrawal\n\nThese rules get folded directly into the prompt as instructions‚Äîcompounding improvements across examples rather than treating each failure in isolation.\nThis capability unlock‚ÄîLLMs that can genuinely reflect and generalize‚Äîis what makes GEPA viable now when it wouldn‚Äôt have been two years ago.\n\n\n\n\nThis approach wasn‚Äôt viable with earlier LLMs. In March 2023, roboticist Eric Jang observed that self-reflection capability ‚Äúseems to be emergent in GPT-4 but not GPT-3.5 or Claude.‚Äù When asked to write a non-rhyming poem, GPT-4 produced rhymes‚Äîbut when prompted ‚Äúdid the poem meet the assignment?‚Äù it apologized and corrected itself. GPT-3.5 and Claude couldn‚Äôt recognize their errors.\nThe Reflexion paper (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4‚Äôs baseline 80%‚Äîwithout any weight updates. Similarly, Self-Refine showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.\nBut there‚Äôs a crucial nuance. A comprehensive 2024 survey found that pure ‚Äúintrinsic‚Äù self-correction‚Äîwhere the LLM reflects with no external signal‚Äîrarely helps, and can even degrade performance. What does work is self-correction with reliable external feedback: compiler errors, test results, structured rubrics.\nThis is precisely what GEPA exploits. The CRITIC paper (ICLR 2024) highlights that external feedback is ‚Äúcrucial‚Äù for successful self-improvement. GEPA doesn‚Äôt ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedback‚Äîthe compiler said this, the profiler showed that, the judge flagged this rubric‚Äîand asks it to reflect on that.\nThe capability unlock isn‚Äôt ‚ÄúLLMs can now introspect perfectly.‚Äù It‚Äôs ‚ÄúLLMs can now process feedback and generalize lessons effectively.‚Äù\n\n‚ÄúEarlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessons‚Ä¶ But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nTo understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessor‚Äîand GEPA represents the latest capability unlock.\n\n\n\nOptimizer Evolution\n\n\n\n\nThe original insight: you don‚Äôt need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as few-shot demonstrations for future runs. The system bootstraps its own examples from successful executions.\nExample: Your QA system correctly answers ‚ÄúWhat‚Äôs the capital of France?‚Äù ‚Üí ‚ÄúParis‚Äù. That (question, answer) pair becomes a demonstration shown to the model on future queries.\nLimitation: Demonstrations are static snapshots. Once selected, they don‚Äôt adapt when new failure modes emerge. And there‚Äôs no instruction optimization‚Äîthe system prompt stays identical whether you‚Äôre handling edge cases or common inputs.\n\n\n\nOPRO (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.\nExample: The optimizer sees: - \"Solve the math problem step by step\" ‚Üí score 0.65 - \"Show your work and verify the answer\" ‚Üí score 0.72 - \"Break the problem into cases and check each\" ‚Üí score 0.78\nIt proposes: \"Systematically enumerate cases and verify each solution\" ‚Üí score 0.81\nLimitation: Score-only signal. The optimizer sees that prompt_v3 scored 0.72 but not why. Did it make algebraic errors? Miss edge cases? The number alone doesn‚Äôt say.\n\n\n\nMiPRO recognized that instructions and demonstrations interact‚Äîthe same instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.\nThe search space problem: Say you have 10 candidate instructions and 5 possible demo sets. That‚Äôs 50 combinations. Now add instruction variants (‚ÄúBe concise‚Äù vs ‚ÄúBe brief‚Äù vs ‚ÄúAnswer in one sentence‚Äù)‚Äîsuddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.\nMiPRO‚Äôs solution: a cheap surrogate model. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you have run. The predictor learns patterns like ‚Äúinstructions mentioning ‚Äòstep-by-step‚Äô tend to score higher‚Äù or ‚Äúdemos with longer reasoning traces correlate with better performance.‚Äù\nThe workflow:\n\nBootstrap: Run a small random sample of combinations (say, 30 out of 500)\nTrain surrogate: Fit the predictor on those 30 (instruction, demos) ‚Üí score pairs\nPredict cheaply: Score all 500 combinations using the surrogate (milliseconds, not dollars)\nEvaluate selectively: Only run full evaluation on the top-predicted candidates\nRepeat: Add new results to training data, retrain surrogate, sample again\n\nExample: After 30 random evaluations, the surrogate learns: - Instructions with ‚Äústep-by-step‚Äù ‚Üí +0.08 average - Demo set B (which has chain-of-thought examples) ‚Üí +0.05 average - Combining both ‚Üí predicted 0.79\nMiPRO focuses budget on high-predicted combinations rather than exhaustive search.\nLimitation: The surrogate learns correlations, not causation. It knows ‚Äústep-by-step instructions score higher‚Äù but not why‚Äîmaybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt that‚Äôs 0.75 on everything beats one that‚Äôs 0.95 on hard cases but 0.60 overall‚Äîeven though that hard-case specialist might contain crucial insights.\n\n\n\nSIMBA reframes prompt optimization as a multi-armed bandit problem‚Äîa classic framework for sequential decision-making under uncertainty.\nThe bandit analogy: Imagine you‚Äôre in a casino with 100 slot machines. Each has a different (unknown) payout rate. You have 50 tokens. How do you maximize winnings?\n\nPure exploitation: Find one machine that seems good, play it 50 times. Problem: maybe you got lucky early‚Äîanother machine is actually better.\nPure exploration: Try each machine once, then‚Ä¶ you‚Äôre out of tokens before you learn anything useful.\nSmart balance: Track your uncertainty about each machine. Play machines where you‚Äôre uncertain (might be great!) more than machines you‚Äôre confident are mediocre.\n\nSIMBA applies this to prompts. Each candidate prompt is a ‚Äúslot machine.‚Äù Each evaluation is a ‚Äúpull.‚Äù The score is the ‚Äúpayout.‚Äù\nHow it works:\n\nInitialize: Start with a pool of candidate prompts (maybe generated by an LLM or hand-written)\nTrack statistics: For each prompt, maintain: average score so far, number of times evaluated, and a confidence interval (range of plausible true scores)\nSample strategically: Use Upper Confidence Bound (UCB) to pick which prompt to evaluate next‚Äîfavoring prompts with high uncertainty OR high average\nUpdate beliefs: After evaluation, narrow the confidence interval for that prompt\nRepeat: Eventually, confidence intervals separate‚Äîyou know which prompts are best\n\nExample in action: You have 20 candidate prompts, budget for 50 evaluations.\n\n\n\nPrompt\nEvaluations\nAvg Score\nConfidence Interval\n\n\n\n\nA\n8\n0.74\n[0.70, 0.78]\n\n\nB\n2\n0.71\n[0.55, 0.87]\n\n\nC\n5\n0.68\n[0.62, 0.74]\n\n\n\nWhich to evaluate next? - Prompt A: probably ~0.74, we‚Äôre confident - Prompt B: could be 0.55 (bad) or 0.87 (best!)‚Äîhigh uncertainty - Prompt C: probably ~0.68, we‚Äôre fairly confident it‚Äôs worse than A\nSIMBA picks Prompt B‚Äîthe uncertainty is valuable. If B turns out great, we found a winner. If bad, we‚Äôve ruled it out cheaply.\nWhy this beats random search: Random would waste evaluations on prompts we already know are bad. SIMBA focuses budget on decisions that matter‚Äîresolving uncertainty between plausibly-good candidates.\nLimitation: SIMBA efficiently finds the best prompt but doesn‚Äôt understand why it works. The bandit framework treats prompts as black boxes with hidden payout rates‚Äîit can‚Äôt reason about ‚Äúthis prompt works because it specifies output format.‚Äù And like MiPRO, it optimizes aggregate score: a prompt scoring 0.75 uniformly beats one scoring 0.95 on hard cases but 0.60 elsewhere‚Äîeven if the hard-case specialist contains insights worth preserving.\n\n\n\nGEPA breaks from this trajectory in two fundamental ways:\n\n\n\n\n\n\n\n\nWhat changed\nBefore GEPA\nWith GEPA\n\n\n\n\nLearning signal\nscore = 0.6\n‚ÄúExceeded word limit. Missing keyword. Compiler error: use y not x.‚Äù\n\n\nSelection strategy\nBest aggregate score\nPareto frontier of diverse specialists\n\n\n\n1. From scalar scores to textual feedback ‚Äî Instead of just knowing that a prompt scored 0.6, GEPA sees why: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.\nExample: OPRO sees score = 0.6. GEPA sees: &gt; ‚ÄúFailed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword ‚Äòdisclaimer‚Äô. Passed examples 1-6, 8-11.‚Äù\nThe LLM reflects: ‚ÄúI should add an instruction about word limits and required keywords.‚Äù\n2. From greedy to Pareto selection ‚Äî Instead of always promoting the highest-scoring candidate, GEPA maintains a Pareto frontier: candidates that each excel at something no other candidate beats.\nExample: Three candidates evaluated on 10 examples: - prompt_A: 8/10 overall, but fails hard cases #7 and #9 - prompt_B: 6/10 overall, but nails hard cases #7 and #9\n- prompt_C: 7/10 overall, no unique strengths\nGreedy selection keeps only prompt_A. Pareto selection keeps both prompt_A and prompt_B‚Äîbecause B‚Äôs insights about hard cases might combine with A‚Äôs general strength. prompt_C gets dropped (dominated by A on everything).\nThe contrast with prior optimizers is stark: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains why‚Äîand proposes the fix directly."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "href": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "title": "GEPA Deepdive",
    "section": "How GEPA Works: Building It From Scratch",
    "text": "How GEPA Works: Building It From Scratch\nGEPA combines two key innovations: reflective prompt mutation (learning from textual feedback) and Pareto selection (preserving diverse specialists). Each is powerful alone; together they compound.\nWe‚Äôll build them from scratch in this section:\n\nReflective mutation ‚Äî How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.\nPareto selection ‚Äî Why always improving your ‚Äúbest‚Äù prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.\nMerge ‚Äî How GEPA combines insights from divergent lineages (covered after the complete algorithm).\n\nBy the end, you‚Äôll see how these mechanisms combine into GEPA‚Äôs full evolutionary loop.\n\n\nüöÄ Quick Start: Using GEPA in 30 Seconds\n\nüìñ Full tutorial: GEPA for AIME (Math) ‚Äî optimizing GPT-4.1 Mini from 46.6% ‚Üí 56.6% on AIME 2025.\n\nBefore diving deep, here‚Äôs what using GEPA looks like in practice:\nStep 1: Configure your language model\nimport dspy\n\nlm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, max_tokens=32000)\ndspy.configure(lm=lm)\nStep 2: Define your program\nprogram = dspy.ChainOfThought(\"problem -&gt; answer\")\nStep 3: Define a metric that returns feedback (not just a score)\nThis is the key difference from other optimizers‚Äîyour metric explains why something failed:\ndef metric_with_feedback(example, prediction, trace=None, **kwargs):\n    correct_answer = example.answer\n    pred_answer = prediction.answer\n    \n    score = int(correct_answer == pred_answer)\n    \n    if score == 1:\n        feedback = f\"Correct! The answer is '{correct_answer}'.\"\n    else:\n        feedback = f\"Incorrect. Expected '{correct_answer}', got '{pred_answer}'.\"\n        # Add any additional context that could help improvement:\n        if hasattr(example, 'solution'):\n            feedback += f\" Solution: {example.solution}\"\n    \n    return dspy.Prediction(score=score, feedback=feedback)\nStep 4: Optimize with GEPA\nfrom dspy import GEPA\n\noptimizer = GEPA(\n    metric=metric_with_feedback,\n    auto=\"light\",           # Budget preset: \"light\", \"medium\", or \"heavy\"\n    num_threads=32,         # Parallel evaluation threads\n)\n\noptimized_program = optimizer.compile(\n    program,\n    trainset=train_set,\n    valset=val_set,\n)\nThat‚Äôs it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.\n\nWhat‚Äôs happening under the hood?\n\n\n\n\n\n\n\nComponent\nWhat it does\n\n\n\n\nTextual feedback\nYour metric returns why something failed, not just a score\n\n\nReflective mutation\nAn LLM reads the feedback and proposes improved instructions\n\n\nPareto selection\nDiverse specialists are preserved, not just the ‚Äúbest‚Äù prompt\n\n\nMerge operations\nInsights from divergent lineages get combined\n\n\n\nThe rest of this section explains why each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you need‚Äîsee the DSPy GEPA API Reference for full parameter details.\nNow let‚Äôs make this concrete by building the core mechanism from scratch.\n\n\nHands-On: Building Reflective Mutation from Scratch\nLet‚Äôs make this concrete by implementing GEPA‚Äôs core mechanism on a real task.\nThe Problem: AIME Math Competition\nWe‚Äôll optimize prompts for solving AIME (American Invitational Mathematics Examination) problems ‚Äî challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.\nfrom datasets import load_dataset\ndset = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n# 90 problems with solutions and integer answers\nWhy AIME for testing prompt optimization?\n\nClear ground truth ‚Äî Every answer is an integer (0-999), so evaluation is unambiguous\nRich failure modes ‚Äî Wrong answers come from algebraic errors, missed cases, misread constraints\nDomain knowledge helps ‚Äî Prompts that encode strategies (‚Äúsubtract equations pairwise‚Äù, ‚Äúenumerate all cases‚Äù) measurably improve performance\nSmall dataset ‚Äî Only 90 problems, so sample efficiency matters\n\nThe Setup\n# Split: 10 train, 10 validation (simulating scarce labeled data)\ntdf = df.sample(45).iloc[:10]  # Training mini-batches drawn from here\nvdf = df.drop(tdf.index).iloc[:10]  # Held-out validation\n\n# Base model: Gemini 2.5 Flash via LiteLLM\n# Metric: Exact match (predicted integer == ground truth)\ndef metric(ground_truth, prediction):\n    return int(ground_truth) == prediction['answer']\nSeed Prompt\nWe start with a minimal instruction:\nseed_prompt = \"\"\"You are given a problem and you have to give the answer \nalong with reasoning. Do not return anything apart from json. \nIt should be parsable by json.loads()\"\"\"\nBaseline validation accuracy: 10% (1/10 correct)\nCan reflective mutation improve this? Let‚Äôs find out.\n\n\n\nStep 1: The Feedback Function\nFirst, we need a function that tells the reflection LLM what went wrong. We‚Äôll start with minimal feedback‚Äîjust the correct answer:\ndef feedback(ground_truth, prediction):\n    if int(ground_truth) != prediction['answer']:\n        return f'You got it wrong! The solution is {ground_truth}'\n    return 'You got it right!'\nThis is deliberately simple. Later we‚Äôll discuss how richer feedback (like expert solutions) can improve results.\n\n\n\nStep 2: The Reflection Prompt\nFollowing GEPA‚Äôs structure, we build a prompt that shows the LLM its failures:\nREFLECTION_TEMPLATE = \"\"\"I provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\n{current_prompt}\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n{examples}\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\"\"\"\n\ndef mk_reflection_prompt(df, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\n\n\n\nStep 3: The Complete Optimization Loop\ndef reflect(mb, curr_prompt):\n    \"\"\"Ask LLM to reflect on failures and propose improved instruction.\"\"\"\n    refl_prompt = mk_reflection_prompt(mb, curr_prompt)\n    return _call(refl_prompt, format=ReflectionModel)['new_instruction']\n\ndef optimize_prompt(seed_prompt, traindf, valdf, n_iters=3, mb_size=3):\n    \"\"\"Greedy reflective prompt optimization.\"\"\"\n    prompts, train_scores, val_scores = [seed_prompt], [], []\n    mb = traindf.sample(mb_size)  # Fixed minibatch for this run\n    \n    print(f'Baseline validation: {eval_val(seed_prompt, valdf):.2%}')\n    \n    for i in range(n_iters):\n        # Evaluate current prompt on minibatch\n        mb_eval, mb_score = eval_mb(prompts[-1], mb)\n        print(f\"üìä Minibatch: {mb_score:.2%}\")\n        \n        # Reflect and propose new instruction\n        new_instr = reflect(mb_eval, prompts[-1])\n        new_prompt = new_instr  # The new instruction becomes the new prompt\n        \n        # Evaluate on validation set\n        val_score = eval_val(new_prompt, valdf)\n        print(f\"üìä Validation: {val_score:.2%}\")\n        \n        prompts.append(new_prompt)\n        val_scores.append(val_score)\n    \n    return dict(prompts=prompts, val_scores=val_scores)\n\n\n\nWhat Actually Happened\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\n\n\n\n\n\n\nIteration\nMinibatch\nValidation\nWhat the reflection learned\n\n\n\n\nBaseline\n‚Äî\n10%\n‚Äî\n\n\n1\n0%\n10%\nJSON formatting details, output structure rules\n\n\n2\n0%\n30%\nSystems of equations strategy, remainder/modular arithmetic tips\n\n\n3\n67%\n10%\nOver-specialized on number theory, solved #9 but lost generality\n\n\n\nThe good: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:\n\n‚ÄúWhen dealing with systems of equations like \\(xy + Az = C\\), \\(yz + Ax = C\\), \\(zx + Ay = C\\), consider subtracting equations pairwise to find relationships between variables, such as \\((x-z)(y-A)=0\\), which implies \\(x=z\\) or \\(y=A\\). Systematically explore all such cases.‚Äù\n\nThis is directly from the actual output‚Äîthe reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.\nThe bad: Iteration 3 achieved 67% on its minibatch but dropped to 10% validation. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:\n\n‚ÄúWhen the problem involves number theory and remainders (e.g., \\(n \\pmod x\\), \\(n \\pmod y\\)), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.‚Äù\n\nThis over-specialized advice (‚Äúcan lead to an answer of 0‚Äù) actively hurt performance on non-number-theory problems, dropping validation from 30% back to 10% (1/10)‚Äîthough notably, it did solve problem #9, which earlier prompts couldn‚Äôt.\n\n\n\nThe Greedy Selection Problem\nThis demonstrates exactly why GEPA uses Pareto selection instead of always taking the ‚Äúbest‚Äù prompt:\n\nIteration 2‚Äôs prompt was a specialist‚Äîit learned something valuable about systems of equations\nIteration 3 tried to improve on iteration 2, but the minibatch had different problems\nThe reflection overwrote the systems-of-equations insight while adding number-theory tips that were too specific\nResult: catastrophic forgetting\n\nWith greedy selection, we would have discarded iteration 2‚Äôs valuable insight. Pareto selection would keep it‚Äîbecause it was best on at least one validation instance.\n\n\n\nThe Missing Ingredient: Rich Feedback\nOur minimal feedback (\"You got it wrong! The solution is 349\") only tells the model that it failed, not why or how to fix it.\nThe AIME dataset includes expert solutions. A richer feedback function could use them:\ndef feedback_rich(row):\n    if int(row.answer) != row.pred['answer']:\n        sol = row.solution[:500] + \"...\" if len(row.solution) &gt; 500 else row.solution\n        return f\"\"\"Wrong! Expected {row.answer}, got {row.pred['answer']}.\n        \nModel's reasoning: {row.pred['short_reasoning']}\n\nExpert solution approach:\n{sol}\"\"\"\n    return \"Correct!\"\nWith rich feedback, the reflection LLM can extract specific strategies from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: the feedback contains the fix.\nCompare this to RL, which would only see reward = 0 and have to statistically infer what went wrong across thousands of trajectories.\n\n\n\nKey Takeaways\n\nReflective mutation works ‚Äî Even with minimal feedback, the LLM extracted useful domain knowledge\nGreedy selection fails ‚Äî Iteration 3‚Äôs collapse shows why we need to preserve specialist insights\nFeedback quality matters ‚Äî Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with\nSample efficiency is real ‚Äî We saw meaningful optimization with just 3 iterations on 3 examples each\n\nThis is the core limitation of greedy optimization: catastrophic forgetting. The solution? Pareto selection‚Äîwhich we‚Äôll build from scratch next.\n\n\nHands-On: Building the Pareto Frontier\nIn the reflective mutation section, we saw greedy selection fail‚Äîiteration 3‚Äôs over-specialized prompt dropped validation from 30% to 10%, losing iteration 2‚Äôs valuable systems-of-equations insights. The fundamental problem: always improving your ‚Äúbest‚Äù prompt discards specialist knowledge.\nGEPA‚Äôs solution: Pareto selection. Instead of keeping one best prompt, maintain a frontier of prompts where each excels at something no other prompt beats.\n\n\nWhat is Pareto Dominance?\nA prompt dominates another if it‚Äôs at least as good everywhere, and strictly better somewhere:\n\n‚â• on every validation instance, AND\n\n&gt; on at least one instance\n\nIf prompt A dominates prompt B, we can safely discard B‚ÄîA is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.\nExample: Consider four prompts evaluated on 10 validation instances. We‚Äôll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:\n\n\n\nPrompt\nInstances Solved\nAggregate\nStatus\n\n\n\n\nP0 (seed)\n#0 only\n10%\nDominated by P2\n\n\nP1 (iter 1)\n#0 only\n10%\nDominated by P2\n\n\nP2 (iter 2)\n#0, #1, #2\n30%\nFrontier ‚úì\n\n\nP3 (iter 3)\n#9 only\n10%\nFrontier ‚úì\n\n\n\nP2 dominates both P0 and P1‚Äîit solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which nothing else could.\nThe Pareto frontier is {P2, P3}. Both contain unique value.\n\n\n\nImplementation: Dominance Checking\nWe represent per-instance scores as boolean arrays (1 = solved, 0 = failed):\nimport numpy as np\n\ndef dominates(candidate_scores, other_scores):\n    \"\"\"Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)\"\"\"\n    candidate = np.array(candidate_scores)\n    other = np.array(other_scores)\n    return (candidate &gt;= other).all() and (candidate &gt; other).any()\n\ndef is_dominated_by_any(new_scores, frontier_scores):\n    \"\"\"Is new_scores dominated by ANY prompt in the frontier?\"\"\"\n    new = np.array(new_scores)\n    for existing in frontier_scores:\n        if dominates(np.array(existing), new):\n            return True\n    return False\n\ndef get_dominated_indices(new_scores, frontier_scores):\n    \"\"\"Which frontier prompts does new_scores dominate?\"\"\"\n    new = np.array(new_scores)\n    return [i for i, existing in enumerate(frontier_scores) \n            if dominates(new, np.array(existing))]\n\n\n\nTracing Through: Why P3 Survives\nLet‚Äôs verify the dominance relationships from our example:\nP0 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP1 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP2 = [1,1,1,0,0,0,0,0,0,0]  # Solves: #0, #1, #2\nP3 = [0,0,0,0,0,0,0,0,0,1]  # Solves: #9\n\n# Does P2 dominate P0?\ndominates(P2, P0)  # True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2\n\n# Does P2 dominate P1?\ndominates(P2, P1)  # True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2\n\n# Does P2 dominate P3?\ndominates(P2, P3)  # False! P2 loses on #9 (0 &lt; 1)\n\n# Does P3 dominate P2?\ndominates(P3, P2)  # False! P3 loses on #0, #1, #2\nNeither P2 nor P3 dominates the other‚Äîthey‚Äôre Pareto incomparable. Each solves problems the other can‚Äôt. Both stay on the frontier.\n\n\n\nPareto Frontier Explained\n\n\n\n\n\nThe Complete Frontier Manager\nclass ParetoFrontier:\n    def __init__(self):\n        self.prompts = []\n        self.scores = []  # scores[i][j] = prompt i's score on instance j\n    \n    def add(self, prompt, instance_scores):\n        \"\"\"Try to add a prompt. Returns True if it joins the frontier.\"\"\"\n        # Reject if dominated by existing frontier member\n        if is_dominated_by_any(instance_scores, self.scores):\n            return False\n        \n        # Remove any frontier members this prompt dominates\n        dominated = get_dominated_indices(instance_scores, self.scores)\n        for i in sorted(dominated, reverse=True):  # Remove from end first\n            del self.prompts[i]\n            del self.scores[i]\n        \n        # Add to frontier\n        self.prompts.append(prompt)\n        self.scores.append(instance_scores)\n        return True\n    \n    def sample(self):\n        \"\"\"Sample a prompt, weighted by unique wins.\"\"\"\n        weights = []\n        scores_arr = np.array(self.scores)\n        for i in range(len(self.prompts)):\n            # How many instances is this prompt *uniquely* best on?\n            others_best = np.delete(scores_arr, i, axis=0).max(axis=0) if len(self.prompts) &gt; 1 else np.zeros_like(scores_arr[i])\n            unique_wins = (scores_arr[i] &gt; others_best).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        \n        return np.random.choice(self.prompts, p=np.array(weights)/sum(weights))\n    \n    def best_aggregate(self):\n        \"\"\"Return the prompt with highest aggregate score.\"\"\"\n        aggregates = [sum(s) for s in self.scores]\n        return self.prompts[np.argmax(aggregates)]\n\n\n\nPutting It Together: Pareto-Guided Optimization\ndef optimize_with_pareto(seed_prompt, traindf, valdf, n_iters=5, mb_size=3):\n    \"\"\"Reflective mutation with Pareto frontier selection.\"\"\"\n    frontier = ParetoFrontier()\n    \n    # Initialize with seed\n    seed_scores = evaluate_per_instance(seed_prompt, valdf)\n    frontier.add(seed_prompt, seed_scores)\n    print(f'Baseline: {sum(seed_scores)/len(seed_scores):.1%}')\n    \n    for i in range(n_iters):\n        print(f\"\\n{'='*40}\\nIteration {i+1}\\n{'='*40}\")\n        \n        # Sample parent from frontier (weighted by unique wins)\n        parent = frontier.sample()\n        \n        # Run on minibatch, reflect, propose mutation\n        mb = traindf.sample(mb_size)\n        mb_results = evaluate_with_traces(parent, mb)\n        new_prompt = reflect_and_mutate(parent, mb_results)\n        \n        # Evaluate on full validation set\n        new_scores = evaluate_per_instance(new_prompt, valdf)\n        new_agg = sum(new_scores) / len(new_scores)\n        print(f\"New prompt: {new_agg:.1%} aggregate\")\n        \n        # Try to add to frontier\n        if frontier.add(new_prompt, new_scores):\n            print(f\"‚úì Added to frontier (size: {len(frontier.prompts)})\")\n        else:\n            print(f\"‚úó Dominated, rejected\")\n    \n    return frontier.best_aggregate()\n\n\n\nWhat We Observed on AIME\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\nIteration\nAggregate\nInstances Solved\nFrontier Action\n\n\n\n\nSeed\n10%\n#0\nInitialize\n\n\n1\n10%\n#0\nDominated by seed, rejected\n\n\n2\n30%\n#0, #1, #2\nAdded, dominates seed\n\n\n3\n10%\n#9 only\nAdded ‚úì (unique win on #9)\n\n\n\nThe key moment: Iteration 3 scored only 10%‚Äîworse than iteration 2‚Äôs 30%. Greedy selection would discard it entirely.\nBut it solved instance #9, which nothing else could. Pareto selection preserves it.\nOur final frontier: {P2, P3} - P2: Strong generalist (30%), knows systems-of-equations strategies - P3: Instance-9 specialist (10%), knows whatever cracked that specific problem\nBoth insights survive. The merge operation (covered later) can combine them.\n\n\n\nWhy This Matters: No More Catastrophic Forgetting\n\n\n\nSelection Strategy\nWhat happens to specialists\n\n\n\n\nGreedy\nDiscarded whenever aggregate score drops\n\n\nPareto\nPreserved if they solve anything unique\n\n\n\nGreedy selection caused our iteration 3 collapse‚Äîthe number-theory prompt overwrote the algebra prompt‚Äôs insights. Pareto selection prevents this by construction: you can‚Äôt remove a prompt from the frontier unless something else does everything it does, plus more.\nThis is the core of GEPA‚Äôs sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier never loses them in the first place.\n\nWe just saw Pareto selection preserve iteration 3‚Äôs prompt despite its 10% aggregate score‚Äîbecause it solved instance #9, which nothing else could. But this raises a question: why does keeping ‚Äúlosers‚Äù help optimization? Shouldn‚Äôt we focus resources on the best candidates?\nThe answer comes from quality-diversity algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.\n\n\n\nWhy Pareto Works: Quality-Diversity and Map Elites\nWe‚Äôve now built both core mechanisms from scratch‚Äîreflective mutation and Pareto selection. Before diving into the full algorithm, let‚Äôs briefly step back and understand why this approach works so well.\nThe Pareto frontier isn‚Äôt a novel invention‚Äîit draws from quality-diversity (QD) algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.\n\nThe Core Insight\nTraditional optimization asks: ‚ÄúWhat‚Äôs the single best solution?‚Äù\nQuality-diversity asks: ‚ÄúWhat‚Äôs the best solution of each type?‚Äù\nComplex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensively‚Äîbut the best engine knows both styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one ‚Äúbest‚Äù approach.\n\n\nMap Elites: The Inspiration\nMap Elites (Mouret & Clune, 2015) maintains an archive organized by behavior:\n\nDefine behavior dimensions ‚Äî characteristics describing how a solution works (not just how well)\nDiscretize into bins ‚Äî each cell represents a ‚Äúniche‚Äù\nKeep the best per bin ‚Äî new solutions compete only within their niche\nMutate from the archive ‚Äî sample from any occupied bin, mutate, place in appropriate bin\n\nThe result: diverse specialists, each optimal of its type. Key insight: the archive provides stepping stones‚Äîa mutation from one niche might discover something useful for another. Diversity isn‚Äôt just nice to have; it‚Äôs a search strategy.\n\n\nGEPA‚Äôs Adaptation: Validation Instances as Niches\nGEPA recognizes that the validation set itself defines the behavior space:\n\n\n\n\n\n\n\nMap Elites\nGEPA\n\n\n\n\nBehavior = continuous dimensions\nBehavior = which validation instances are solved\n\n\nBins = discretized regions\n‚ÄúBins‚Äù = individual validation instances\n\n\nArchive = best per bin\nPareto frontier = non-dominated prompts across instances\n\n\n\nEach validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the ‚Äúspecialist for that niche‚Äù‚Äîit stays because it demonstrates something works, even with low aggregate score.\nConcrete example: In our AIME experiment, instance #9 was a number theory problem requiring CRT. Prompt P3 (10% aggregate) solved it; P2 (30% aggregate) couldn‚Äôt. In Map Elites terms, P3 is the ‚Äúelite‚Äù for the number-theory niche‚Äîit stays in the archive despite low overall score.\n\n‚ÄúBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.‚Äù ‚Äî Lakshya A Agrawal\n\nThis directly motivated GEPA‚Äôs design: Pareto selection over per-instance scores naturally implements QD‚Äôs ‚Äúbest per niche‚Äù principle, while the merge operation recombines insights across niches‚Äîexactly what stepping-stone search requires.\n\n\nExploration-Exploitation Balance\nGEPA‚Äôs Pareto selection handles the exploration-exploitation tradeoff automatically: candidates are sampled weighted by unique wins, so specialists get attention proportional to their unique value. The frontier self-prunes‚Äînoise gets dominated away, genuine diversity persists. No manual tuning required.\nWith both reflective mutation and Pareto selection in place, there‚Äôs one more operation that makes GEPA powerful: merge‚Äîcombining insights from divergent lineages. Let‚Äôs look at the complete algorithm.\n\n\n\nThe Complete Algorithm\nNow that we‚Äôve built reflective mutation and Pareto selection from scratch, and understand why quality-diversity works, let‚Äôs see how all the pieces‚Äîincluding lineage tracking and merge‚Äîcombine into GEPA‚Äôs full optimization loop.\n\nAlgorithm Overview\nGEPA(base_prompt, trainset, valset, max_iterations):\n    \n    # Initialize\n    candidates = [base_prompt]\n    scores = {base_prompt: evaluate_all(base_prompt, valset)}\n    pareto_frontier = [base_prompt]\n    lineage = {base_prompt: None}  # parent tracking\n    \n    for iteration in 1..max_iterations:\n        \n        # 1. SAMPLE: Select candidate from Pareto frontier\n        #    Weighted by number of instances where candidate is best\n        parent = sample_from_frontier(pareto_frontier, scores)\n        \n        # 2. PROPOSE: Either mutate or merge\n        if should_merge(pareto_frontier, iteration):\n            # Select second parent from different lineage\n            other_parent = select_divergent_candidate(pareto_frontier, parent, lineage)\n            new_prompt = merge(parent, other_parent)\n        else:\n            # Run rollouts, collect feedback, reflect\n            minibatch = sample(trainset, k=3)\n            traces, feedback = run_with_feedback(parent, minibatch)\n            # See \"Hands-On: Building Reflective Mutation\" for the reflection prompt structure\n            new_prompt = reflect_and_mutate(parent, traces, feedback)\n        \n        # 3. EVALUATE: Mini-batch gate, then full evaluation\n        parent_mb_score = evaluate(parent, minibatch)\n        new_mb_score = evaluate(new_prompt, minibatch)\n        if new_mb_score &lt;= parent_mb_score:\n            continue  # Reject: didn't improve on mini-batch\n        \n        new_scores = evaluate_all(new_prompt, valset)\n        \n        # 4. UPDATE: Pareto frontier maintenance\n        if is_dominated(new_scores, pareto_frontier):\n            continue  # Reject: dominated by existing candidate\n        \n        # Remove any candidates the new one dominates\n        pareto_frontier = [c for c in pareto_frontier \n                          if not dominates(new_scores, scores[c])]\n        \n        # Add new candidate\n        candidates.append(new_prompt)\n        scores[new_prompt] = new_scores\n        pareto_frontier.append(new_prompt)\n        lineage[new_prompt] = parent\n    \n    # Return best aggregate for deployment; full frontier available via track_stats\n    return best_aggregate(pareto_frontier, scores)\n\n\n\nThe Key Decision Points\n1. Candidate Sampling (Exploration vs Exploitation)\nRather than always improving the single best prompt (greedy), GEPA samples from the entire Pareto frontier. The sampling weight is proportional to how many validation instances the candidate is uniquely best on:\ndef sample_from_frontier(frontier, scores):\n    weights = []\n    for candidate in frontier:\n        # Count instances where this candidate beats all others\n        unique_wins = sum(1 for i in range(n_instances) \n                        if scores[candidate][i] &gt; max(scores[other][i] \n                                                      for other in frontier if other != candidate))\n        weights.append(unique_wins + 1)  # +1 smoothing\n    return random.choices(frontier, weights=weights)[0]\nThis focuses effort on candidates that have demonstrated unique value‚Äîthey solve something nothing else can. Specialists get attention proportional to their specialization.\n2. Mutation vs Merge Decision\nEarly iterations favor mutation (exploring from single candidates). As the frontier diversifies, merge becomes more valuable. The following is a simplified illustration of the decision logic:\ndef should_merge(frontier, iteration):\n    if len(frontier) &lt; 2: return False\n    if iteration &lt; 5: return False  # Let lineages diverge first\n    # Merge with probability proportional to frontier diversity\n    n_lineages = len(set(get_root(c) for c in frontier))\n    return random.random() &lt; (n_lineages - 1) / len(frontier)\n(The actual GEPA implementation may use different thresholds‚Äîsee the paper for details.)\n3. Mini-Batch Gating\nBefore expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:\n\n‚ÄúWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.‚Äù ‚Äî Lakshya A Agrawal\n\n4. Pareto Update\nThe frontier update follows the dominance logic we implemented earlier: - Reject new candidates dominated by existing ones (they add nothing) - Remove existing candidates dominated by the new one (they‚Äôre obsolete) - Keep all non-dominated candidates (each offers unique value)\n\n\n\nComplexity Analysis\n\n\n\n\n\n\n\nOperation\nCost\n\n\n\n\nMutation (3-4 rollouts + reflection)\n3-4 LLM calls + 1 reflection call\n\n\nMini-batch evaluation\n3-4 metric evaluations\n\n\nFull validation evaluation\nN metric evaluations (N = valset size)\n\n\nPareto check\nO(F √ó N) comparisons (F = frontier size)\n\n\n\nThe mini-batch gate is crucial for efficiency. Most mutations fail‚Äîthey either don‚Äôt improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.\n\n\n\nWhy Each Component Matters\n\n\n\n\n\n\n\n\nComponent\nWithout it\nWith it\n\n\n\n\nTextual feedback\nOptimizer sees only score=0.6\nOptimizer reads ‚Äúwrong answer, expected X, got Y because‚Ä¶‚Äù\n\n\nPareto selection\nSpecialists discarded when aggregate drops\nSpecialists preserved if they solve anything unique\n\n\nLineage tracking\nNo memory of evolutionary history\nCan identify divergent branches for merge\n\n\nMerge operation\nInsights stay siloed in separate branches\nOrthogonal discoveries can combine\n\n\nMini-batch gating\nEvaluate every candidate fully\nReject obvious failures cheaply\n\n\n\nThe components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.\n\n\n\nWhat Gets Returned\nThe algorithm returns best_aggregate(pareto_frontier)‚Äîthe prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use track_stats=True to access all candidates and their per-instance scores.\n\nNext: The merge operation in detail‚Äîhow GEPA combines insights from divergent lineages.\n\n\n\nThe Lineage Tree and System-Aware Merge\nWe touched on merge briefly in the algorithm overview‚Äînow let‚Äôs see why it‚Äôs essential and how it actually works.\nWhat ‚Äúsystem-aware‚Äù means: Unlike genetic algorithm crossover (which blindly swaps segments), GEPA‚Äôs merge uses an LLM that understands what it‚Äôs combining‚Äîit can resolve contradictions, synthesize complementary strategies, and produce coherent instructions rather than jumbled concatenations.\nPareto selection preserves specialists‚Äîbut it creates a new problem: insights get siloed in separate branches.\nConsider what happens after 10 iterations of GEPA on AIME problems:\n\n\n\nLineage Tree with Merge Operation\n\n\nThe P2‚ÜíP4 lineage accumulated algebra insights. The P3‚ÜíP5 lineage accumulated number theory insights. Both survive on the Pareto frontier because each solves problems the other can‚Äôt.\nBut what about a problem requiring both?\n\n\nThe Recombination Problem\nSuppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:\n\nP4 knows to subtract equations pairwise, but doesn‚Äôt think to reduce mod p\nP5 knows CRT, but misses the algebraic setup\n\nWith mutation alone, P4 would need to independently rediscover number theory (which P5 already knows), or vice versa. The knowledge exists in our population‚Äîjust in different branches.\nMerge solves this by combining insights from divergent lineages into a single candidate.\n\n\n\nHow Merge Works\nGEPA‚Äôs merge is ‚Äúsystem-aware‚Äù‚Äîit understands what it‚Äôs combining, not just concatenating strings. The reflection LLM receives:\n\nBoth parent prompts with their full instruction text\nLineage context ‚Äî what types of problems each lineage solved\nConflict guidance ‚Äî instructions to resolve contradictions, not ignore them\n\nThe prompt asks the LLM to synthesize, not concatenate:\n\n‚ÄúCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don‚Äôt simply concatenate‚Äîsynthesize into coherent guidance.‚Äù\n\nConcrete example ‚Äî merging our AIME specialists:\n\n\n\n\n\n\n\n\nParent\nSpecialty\nKey instruction\n\n\n\n\nP4\nAlgebra\n‚ÄúSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.‚Äù\n\n\nP5\nNumber theory\n‚ÄúApply CRT for modular constraints. Check for contradictions.‚Äù\n\n\n\nMerged offspring P6: &gt; ‚ÄúApproach: (1) For equation systems, subtract pairwise to expose factor relationships‚Äîenumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.‚Äù\nP6 inherits both toolkits AND adds meta-knowledge about when to combine them. This is the ‚Äúsystem-aware‚Äù part‚Äîthe merge understands the semantics of what it‚Äôs combining.\n\n\n\nWhen to Merge vs.¬†Mutate\nGEPA alternates between operations based on frontier state:\n\n\n\n\n\n\n\n\nCondition\nOperation\nRationale\n\n\n\n\nEarly iterations (&lt; 5)\nMutate\nLet lineages diverge first; nothing to merge yet\n\n\nFrontier has one dominant lineage\nMutate\nNo orthogonal insights to combine\n\n\nFrontier has divergent specialists\nMerge\nRecombine discoveries from parallel explorations\n\n\nRecent merge succeeded\nMutate\nRefine the merged candidate\n\n\n\nThe paper describes the decision:\n\n‚ÄúAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nMerge vs.¬†Genetic Algorithm Crossover\nGEPA‚Äôs merge is analogous to crossover in genetic algorithms, but smarter:\n\n\n\n\n\n\n\nGenetic Algorithms\nGEPA Merge\n\n\n\n\nGenes = bit positions\nInsights = natural language instructions\n\n\nCrossover = swap bit segments randomly\nMerge = LLM synthesizes with understanding\n\n\nCan create invalid offspring\nCan resolve contradictions\n\n\nBlind to semantics\nAware of what instructions mean\n\n\n\nRandom crossover might produce: ‚ÄúSubtract equations pairwise. Apply CRT. Subtract equations pairwise.‚Äù (nonsense duplication)\nLLM merge produces: ‚ÄúSet up algebra first, then apply number-theoretic constraints.‚Äù (coherent synthesis)\n\n\n\nThe Compounding Effect\nThe components reinforce each other:\n\nPareto selection preserves the diversity that makes merge valuable\nLineage tracking identifies which candidates come from divergent branches\n\nMerge recombines orthogonal discoveries into unified candidates\nPareto selection then preserves successful merges alongside remaining specialists\n\nWithout Pareto selection, there‚Äôs nothing interesting to merge‚Äîyou‚Äôd just have variants of one ‚Äúbest‚Äù prompt. Without merge, insights stay siloed even when the frontier is diverse.\nMutation explores depth‚Äîrefining one approach through successive reflections.\nMerge explores breadth‚Äîcombining orthogonal discoveries from parallel paths.\nTogether, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.\n\n\n\nWhat GEPA Learns: Domain-Specific Knowledge Encoding\nWe‚Äôve built the full algorithm‚Äîreflective mutation, Pareto selection, merge. But what does all this machinery actually produce? Let‚Äôs examine the output: prompts that encode domain expertise.\nOne of GEPA‚Äôs most striking capabilities is its ability to encode domain-specific knowledge directly into prompts‚Äîtransforming tacit expertise into explicit instructions that persist across examples.\n\nPrompts as Knowledge Containers\nTraditional optimization treats prompts as opaque strings to be scored. GEPA treats them as knowledge containers that accumulate insights through the reflection loop:\n\n\n\nFailure Accumulation Experience\n\n\nEach iteration doesn‚Äôt just fix one error‚Äîit extracts the lesson behind the error.\nConcrete example from our AIME experiments:\n\n\n\n\n\n\n\nStage\nPrompt excerpt\n\n\n\n\nSeed\n‚ÄúYou are given a problem and you have to give the answer along with reasoning.‚Äù\n\n\nAfter iter 2\n‚Äú‚Ä¶For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.‚Äù\n\n\nAfter iter 3\n‚Äú‚Ä¶When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.‚Äù\n\n\n\nThe prompt evolved from generic instruction to encoding competition math heuristics. The LLM didn‚Äôt invent these‚Äîit extracted them from its training knowledge, triggered by seeing specific failure modes.\n\n\nThree Categories of Captured Knowledge\nWe observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):\n1. Format and Interface Knowledge - Output schemas (‚Äúreturn JSON with keys: answer, reasoning‚Äù) - API conventions (‚Äúuse library.method(), not library_method()‚Äù) - Parsing requirements (‚Äúintegers only, no leading zeros‚Äù)\nThis is easiest to extract‚Äîformat errors produce explicit feedback.\n2. Strategic Knowledge - Problem-solving heuristics (‚Äútry small cases first‚Äù) - Domain patterns (‚Äúcompetition problems often combine algebra and number theory‚Äù) - Failure mode awareness (‚Äúwatch for off-by-one errors in counting‚Äù)\nThis emerges from reflecting on why approaches failed, not just that they failed.\n3. Factual Domain Knowledge - API names and signatures (‚Äútorch.einsum, not torch.einstein_sum‚Äù) - Domain constants (‚Äústandard gravity = 9.81 m/s¬≤‚Äù) - Constraint relationships (‚Äúin valid Sudoku, each row/column/box contains 1-9 exactly once‚Äù)\nThe LLM already knows this‚Äîreflection surfaces it into the prompt where it‚Äôs consistently applied.\n\n\nWhy Prompts Beat Weights (Sometimes)\nFine-tuning encodes knowledge in model weights‚Äîopaque, distributed, hard to inspect. GEPA encodes knowledge in natural language‚Äîreadable, editable, composable.\n\n\n\nAspect\nFine-tuning\nGEPA prompts\n\n\n\n\nInspectability\nBlack box\nHuman-readable instructions\n\n\nEditability\nRequires retraining\nEdit the text directly\n\n\nComposability\nTrain new model\nMerge prompt sections\n\n\nSample efficiency\nThousands of examples\nTens of examples\n\n\n\nA GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.\n\n\nThe Preservation Problem\nKnowledge accumulation has a failure mode: new insights can overwrite old ones. We saw this in our hands-on experiment‚Äîiteration 3‚Äôs number theory insights overwrote iteration 2‚Äôs algebra insights, causing catastrophic forgetting.\nThis is precisely why GEPA uses Pareto selection. By preserving prompts that are best on any validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier‚Äîand the merge operation can later combine their insights.\n\nWithout Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.\n\n\n\nLimitation: Knowledge Must Be Triggerable\nGEPA can only surface knowledge the base LLM already has. If the model doesn‚Äôt know that ‚ÄúCRT‚Äù means Chinese Remainder Theorem, no amount of reflection will discover it. The optimization extracts and organizes existing knowledge‚Äîit doesn‚Äôt create new knowledge.\nThis is why GEPA works better with stronger base models: more latent knowledge to extract. And why, for domains requiring knowledge the LLM lacks, you‚Äôll need to inject it through few-shot examples, retrieval augmentation, or explicit instructions.\n\n‚ÄúLanguage models already have built a lot of prior knowledge‚Ä¶ we can use all of this natural language information to make the LLM itself reflect on its mistakes and improve.‚Äù ‚Äî Lakshya A Agrawal\n\n\nNow that we understand the algorithm and what it produces, let‚Äôs look at how to use GEPA in practice."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "href": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "title": "GEPA Deepdive",
    "section": "Beyond Training: GEPA for Inference-Time Search",
    "text": "Beyond Training: GEPA for Inference-Time Search\nEverything we‚Äôve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.\n\nTwo Paradigms of Operation\nTrain-then-generalize (what we‚Äôve built so far): - Optimize prompts on a training set - Select the best-aggregate prompt from the Pareto frontier - Deploy that prompt on new, unseen tasks - Goal: learn generalizable lessons that transfer\nTest-time search (inference-time optimization): - You have a batch of hard tasks you need to solve now - Optimize directly on the tasks themselves - GEPA searches for solutions, storing the best prompt per task - Goal: maximize performance on these specific instances\nThe mechanics are identical‚Äîreflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you‚Äôre using GEPA as a search algorithm over the solution space. This aligns with the broader trend of inference-time compute scaling‚Äîinvesting more computation at inference to solve harder problems.\nThe key mechanical change: pass valset=trainset (or equivalently, omit valset and set trainset to your target tasks). This tells GEPA to optimize directly on the problems you‚Äôre trying to solve rather than holding out a separate validation set. See the GEPA API documentation for full parameter details.\n# Test-time search: optimize directly on the tasks you want to solve\noptimized = GEPA(metric=metric_with_feedback, auto=\"medium\").compile(\n    program,\n    trainset=hard_problems,  # The actual tasks you need solved\n    valset=hard_problems,    # Same set‚Äîno held-out validation\n)\n\n‚ÄúGiven a batch of tasks that we want to solve and given some budget‚Ä¶ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nWhy GEPA Beats High-Temperature Sampling\nTraditional inference-time compute strategies generate many candidates by sampling at high temperature, then use a verifier or LLM-as-judge to pick the best one. But these samples tend to be similar‚Äîvariations on the same approach.\nRecent research on test-time compute scaling (Snell et al., 2024) shows that ‚Äúcompute-optimal‚Äù strategies‚Äîwhich adaptively allocate inference budget based on problem difficulty‚Äîcan improve efficiency by more than 4x compared to traditional best-of-N sampling. In some cases, smaller models with optimized test-time compute outperform models 14x larger that don‚Äôt use additional inference-time computation.\nThe intuition: High-temperature sampling produces variations around the same approach‚Äîdifferent variable names, slightly different loop bounds, minor syntactic choices. GEPA‚Äôs reflective mutation proposes structurally different strategies based on what went wrong. When the feedback says ‚Äúmemory bandwidth bottleneck,‚Äù the next candidate might switch from a naive loop to shared-memory tiling‚Äîa qualitative change that temperature variation rarely discovers.\nThe Pareto frontier then preserves these diverse strategies rather than converging on a single ‚Äúbest‚Äù approach.\nGEPA induces genuine diversity through two mechanisms:\n\nPareto tracking ‚Äî maintains candidates that each excel at something different, not just the single highest scorer\nReflective mutation ‚Äî proposes structurally different approaches based on what went wrong, not random perturbations\n\n\n‚ÄúDue to the way GEPA operates where it‚Äôs doing the Pareto candidate tracking and it‚Äôs doing reflective mutation, we find that GEPA is itself inducing a huge amount of diversity in the kinds of solutions that it generates.‚Äù ‚Äî Lakshya A Agrawal\n\nRather than 100 slight variations of one approach, GEPA maintains a frontier of genuinely different strategies.\nConcrete results: On the MATH benchmark, GEPA achieves 93% accuracy compared to 67% with basic DSPy ChainOfThought‚Äîa 26 percentage point improvement from prompt optimization alone, no fine-tuning required.\n\n\n\nSelf-Bootstrapping: How Iteration Compounds\nGEPA‚Äôs iterative process creates a self-bootstrapping dynamic:\n\nRound 1: Generate rollouts ‚Üí get feedback (e.g., compiler error) ‚Üí reflect ‚Üí propose fix\nRound 2: The code compiles, but now there‚Äôs a runtime error (division by zero) ‚Üí reflect ‚Üí propose fix\nRound 3: Runtime works, but output is wrong ‚Üí reflect ‚Üí propose fix\n\n\n‚ÄúIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it‚Äôs kind of like self-bootstrapping data to train itself.‚Äù ‚Äî Lakshya A Agrawal\n\nEach iteration surfaces a new failure mode and proposes a solution‚Äîgenerating increasingly challenging training signal from the task itself.\n\n\n\nCross-Task Transfer Within a Batch\nWhen solving related tasks (e.g., a batch of CUDA kernels), insights compound across the batch:\n\n‚ÄúAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.‚Äù ‚Äî Lakshya A Agrawal\n\nThe frontier maintains multiple specialized prompts simultaneously:\n\n\n\nPrompt\nSpecialization\nProblems solved\n\n\n\n\nP_conv\nConvolutional operators\n#1, #4, #7\n\n\nP_reduce\nReduction/summation operators\n#2, #5, #8\n\n\nP_matmul\nMatrix multiplication\n#3, #6\n\n\n\n\n‚ÄúOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.‚Äù ‚Äî Lakshya A Agrawal\n\nWhen a new problem arrives, GEPA tries candidates from across the frontier‚Äîthe convolution specialist might crack it, or the reduction specialist, or insights from both might merge.\n\n\n\nWhat GEPA Stores\nFor each task in the batch, GEPA tracks both artifacts:\n\n‚ÄúGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.‚Äù ‚Äî Lakshya A Agrawal\n\n\nBest outputs ‚Äî the actual solutions, ready to use\nBest prompts ‚Äî specialized strategies representing different subdomains of your problem space\n\nYou can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.\n\n\n\nFeedback Design for Each Paradigm\nThe paradigm choice shapes how you design feedback:\nFor train-then-generalize ‚Äî extract transferable lessons:\n\n‚ÄúYou will ensure that there is no task-specific insights that the prompt captures. So you will write in your feedback something along the lines of ‚Äòtry to extract lessons out of this.‚Äô‚Äù ‚Äî Lakshya A Agrawal\n\ndef metric_generalizable(gold, pred, trace=None):\n    score = int(gold.answer == pred.answer)\n    if score == 0:\n        feedback = (\n            f\"Failed: expected {gold.answer}, got {pred.answer}. \"\n            f\"Extract a GENERAL lesson that would help on similar problems.\"\n        )\n    else:\n        feedback = \"Correct. What general strategy led to success?\"\n    return {\"score\": score, \"feedback\": feedback}\nFor test-time search ‚Äî hyper-specialize to the task:\n\n‚ÄúIf you‚Äôre doing simply an inference time search where you just care about the final outputs, then you will try to provide feedback which is as hyper-specialized to the task as possible.‚Äù ‚Äî Lakshya A Agrawal\n\ndef metric_specialized(gold, pred, trace=None):\n    score = int(gold.answer == pred.answer)\n    if score == 0:\n        feedback = (\n            f\"Your code failed with this particular compiler error: {pred.error}. \"\n            f\"Try to improve on this specific thing.\"\n        )\n    else:\n        feedback = \"Correct.\"\n    return {\"score\": score, \"feedback\": feedback}\n\n\n\nThe Tutor Analogy: Background Optimization Loops\nThis iterative pattern mirrors how students already work with AI tutoring systems:\n\nAttempt problem ‚Üí get solution\nSee an error ‚Üí receive feedback\nTutor explains ‚Üí improved understanding\nRepeat until mastered\n\nThis suggests an intriguing application: background GEPA loops for personalization. Similar patterns are emerging in tools like Cursor and other AI-assisted development environments.\n\n‚ÄúFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt‚Ä¶ cursor can learn a user-specific prompt that works well specifically for you.‚Äù ‚Äî Lakshya A Agrawal\n\nEvery time you provide feedback on an AI-generated solution, a background loop could update not just the current response but a user-specific prompt capturing your preferences and patterns.\n\n\n\nWhen NOT to Use Test-Time GEPA\n\nSparse feedback ‚Äî If your metric only returns pass/fail with no explanation, GEPA can‚Äôt extract lessons\nTasks outside LLM knowledge ‚Äî GEPA surfaces knowledge the model already has; genuinely novel information won‚Äôt emerge from reflection. For such domains, consider retrieval-augmented generation to inject external knowledge\nTight latency constraints ‚Äî Test-time GEPA typically runs 5-20 iterations, each involving LLM calls for rollout + reflection. Budget 30 seconds to 5 minutes per task depending on model latency and iteration count. Interactive use cases need a different approach.\nIdentical tasks ‚Äî If your batch contains identical problems (not just related ones), optimize once and reuse. Cross-task transfer helps when problems are related but distinct‚Äîlike different CUDA kernels that share optimization patterns but have different structures.\n\n\nWhen the conditions are right‚Äîrich feedback, high-value tasks worth the compute, domains where the LLM has strong priors‚Äîtest-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.\n\n\nCase Study: Code Optimization for Novel Hardware\nThe GEPA paper demonstrates test-time search on domains where GEPA‚Äôs strengths shine brightest: code optimization for hardware with limited pre-training data.\n\nAMD NPU Kernels: Optimization Without Pre-Training Knowledge\nAMD‚Äôs NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The NPUEval benchmark reveals just how challenging this is: even with compiler feedback and retrieval-augmented generation (RAG), state-of-the-art LLMs achieve only ~10% mean vectorization score across the dataset.\nGEPA succeeds here because it doesn‚Äôt need prior examples. Instead, it:\n\nGenerates an initial kernel based on generic programming knowledge\nCompiles and runs ‚Üí receives compiler errors or performance metrics\nReflects on feedback ‚Üí proposes targeted improvements\nIterates until the kernel compiles, runs correctly, and performs well\n\nWhy GEPA fits: The GEPA paper demonstrates results on NPUEval, showing that reflective mutation can extract optimization patterns even for hardware with minimal pre-training coverage‚Äîwithout requiring retraining or RAG.\n\n‚ÄúGEPA can be used to generate optimized kernels for AMD‚Äôs NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.‚Äù ‚Äî Lakshya A Agrawal\n\nThe compiler error messages contain the fix. ‚ÄúSymbol not found: npu_matmul‚Äù triggers reflection that surfaces the correct API. ‚ÄúMemory alignment error at line 47‚Äù points directly to what needs fixing.\n\n\nCUDA Kernels: Outperforming Human Baselines\nKernelBench (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels‚Äîa task where even frontier models struggle. The benchmark reveals a sobering baseline: frontier reasoning models match the PyTorch baseline on less than 20% of tasks using the fast‚ÇÅ metric (kernels that are both correct and faster than PyTorch). This isn‚Äôt a matter of prompting‚Äîefficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven‚Äôt learned to apply reliably.\nThe impact of iterative feedback: The KernelBench paper demonstrates that feedback-driven refinement dramatically improves results. By providing execution results and profiler feedback in context, fast‚ÇÅ scores improved from 12%, 36%, and 12% to 43%, 72%, and 18% respectively across their test configurations. This is exactly the mechanism GEPA exploits‚Äîbut with systematic Pareto tracking and reflective mutation rather than ad-hoc iteration.\n\n‚ÄúFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.‚Äù ‚Äî Lakshya A Agrawal\n\nRelated evolutionary approaches show further gains: Stanford‚Äôs separate test-time evolutionary search (distinct from GEPA, published in their Fast Kernels blog post) produces kernels achieving 103-133% of PyTorch reference performance on foundational operators like Conv2D‚Äîdemonstrating that structured search with execution feedback can exceed human-optimized baselines.\n\n\n\n\n\n\n\n\nMethod\nPerformance\nSource\n\n\n\n\nBase LLM (one-shot)\n~3-15% fast‚ÇÅ\nKernelBench paper\n\n\nFrontier reasoning (o1, R1)\nMatch baseline on &lt;20% of tasks\nKernelBench paper\n\n\nWith execution + profiler feedback\n43-72% fast‚ÇÅ (3-6x improvement)\nKernelBench paper\n\n\nEvolutionary test-time search\n103-133% of PyTorch on select kernels\nStanford CRFM (separate from GEPA)\n\n\nGEPA\n‚ÄúSignificant speedups over PyTorch-eager‚Äù\nGEPA paper Fig. 11\n\n\n\nThe gap matters commercially: efficient compilers often lag behind new GPU architectures by over two years‚Äîapproximately one year for CUDA experts to develop optimized implementations and another year to generalize into compilers. GEPA-style optimization could bridge that gap by generating optimized kernels for new hardware before traditional toolchains catch up.\nWhy code optimization is ideal for GEPA:\n\n\n\n\n\n\n\nProperty\nWhy it helps\n\n\n\n\nRich textual feedback\nCompiler errors, profiler output, runtime exceptions all explain why something failed\n\n\nVerifiable correctness\nUnit tests and benchmarks provide unambiguous signal\n\n\nIterative refinement\nEach failed compilation reveals the next fix to try\n\n\nCross-task transfer\nInsights about memory coalescing on one kernel help others\n\n\n\n\n\nThe Self-Bootstrapping Dynamic\nA typical GEPA optimization trajectory for CUDA kernels follows this pattern:\nIteration 1: Generate naive kernel ‚Üí compiler error ‚Äúundeclared identifier threadIdx‚Äù - Reflection: ‚ÄúCUDA kernels require explicit thread indexing. Add threadIdx.x and blockIdx.x.‚Äù\nIteration 2: Compiles, but runtime error (out of bounds) - Reflection: ‚ÄúNeed bounds checking. Add if (idx &lt; n) guard.‚Äù\nIteration 3: Runs correctly, but 10x slower than baseline - Profiler feedback: ‚ÄúMemory bandwidth: 12% of peak. Non-coalesced access pattern.‚Äù - Reflection: ‚ÄúReorganize memory access for coalescing. Use shared memory for reductions.‚Äù\nIteration 4: 1.2x faster than PyTorch baseline\nEach iteration surfaced a new challenge and its solution‚Äîgenerating increasingly sophisticated training signal from the task itself.\n\n\nCross-Kernel Transfer\nWhen optimizing a batch of related kernels, insights compound across tasks:\n\n‚ÄúIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.‚Äù ‚Äî Lakshya A Agrawal\n\nGEPA exploits the relatedness of kernels in a batch. As it optimizes each kernel, the Pareto frontier accumulates specialized prompts‚Äîone excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. When a new kernel arrives, candidates from across the frontier are tried. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax.\nThis cross-task transfer is why batch optimization outperforms solving each kernel independently‚Äîthe accumulated optimization knowledge benefits later tasks.\n\n\nBeyond Kernels\nThe same pattern extends to other code optimization domains where execution produces rich textual feedback:\n\n‚ÄúThere are many other tasks where it can be used‚Äîfor example, with unit tests to generate code patches.‚Äù ‚Äî Lakshya A Agrawal\n\nBug fixing (test failures explain what‚Äôs wrong), performance optimization (profiler output identifies bottlenecks), and API migration (deprecation warnings specify changes) all fit GEPA‚Äôs feedback-driven model."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "href": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "title": "GEPA Deepdive",
    "section": "Conclusion: When to Reach for GEPA",
    "text": "Conclusion: When to Reach for GEPA\nWe opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculus‚Äîmatching RL‚Äôs optimization performance at a fraction of the sample cost by doing something RL can‚Äôt: reading the feedback.\nThe results speak for themselves: 46.6% ‚Üí 56.6% on AIME math competition problems. 67% ‚Üí 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.\nThis represents a genuinely new point in the optimization design space‚Äîone that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. That‚Äôs not incremental improvement‚Äîit‚Äôs a 100x reduction in sample requirements.\n\n\nUse GEPA for Train-Then-Generalize When:\n\nYou have rich textual feedback (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)\nYour evaluation budget is limited (50-500 examples, not 5,000)\nYou‚Äôre optimizing compound AI systems where prompts orchestrate multi-step pipelines\nYou need interpretable results‚Äîprompts you can read, edit, and reason about\n\n\n\nUse GEPA for Test-Time Search When:\n\nYou have a batch of high-value tasks worth the compute investment\nEach task produces execution feedback (tests, profilers, validators)\nTasks are related enough for cross-task transfer to help\n\n\n\nStick with Traditional Approaches When:\n\nYou have abundant labeled data and compute budget for fine-tuning\nFeedback is purely scalar with no explanatory signal\nThe task is already solved by few-shot prompting\nYou need sub-second latency\n\n\n\n\nKnown Limitations\nGEPA isn‚Äôt a silver bullet:\n\nReflection quality varies by domain ‚Äî GEPA excels when the LLM has strong prior knowledge. On highly specialized domains where the base model is weak, reflection produces generic advice rather than actionable fixes.\nFeedback bottleneck ‚Äî The optimizer is only as good as its feedback. If your metric returns ‚Äúwrong‚Äù without explaining why, GEPA degrades to expensive random search.\nValidation set size ‚Äî With fewer than 30 validation examples, prompts can overfit to idiosyncrasies of those specific instances.\n\n\n\n\nGet Started\nReady to try GEPA on your own pipelines?\n\nGEPA for AIME Tutorial ‚Äî Complete walkthrough from setup to optimized results\nGEPA API Reference ‚Äî Full parameter documentation\nPaper ‚Äî Algorithm details and experimental methodology\n\nThe core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads it‚Äîand learns."
  }
]