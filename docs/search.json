[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Risheekkumar, a Data Scientist. I build AI systems and AI-powered experiences that serve customers at scale at American Express.\nI write about AI-powered products, engineering, and evaluations. Outside of work, I like playing chess."
  },
  {
    "objectID": "posts/gepa-impact/gepa_impact_final.html",
    "href": "posts/gepa-impact/gepa_impact_final.html",
    "title": "Programmatic Prompt Optimization: Replacing Intuition with Algorithms",
    "section": "",
    "text": "I‚Äôd built plenty of successful POCs with LLM prompts. Impressive in demos, but when it came time to deploy at scale, precision wasn‚Äôt good enough‚Ä¶ recall wasn‚Äôt high. I‚Äôd tried methods from many prompt tutorials from openai, claude etc.\nManual prompt engineering is fundamentally trial and error. What works today might fail tomorrow and there‚Äôs no systematic way to improve it.\nMy task at hand was getting insights from unstructured customer interaction data. I‚Äôd spent three weeks fine-tuning a single prompt by hand, iterating through variations, hoping to stumble on something that worked consistently. It wasn‚Äôt cutting it. Even if this worked now, what about model upgrades? What about maintenance?\nI searched for tools that could achieve this out of pure desperation. That‚Äôs when I stumbled upon DSPy and the concept of prompt optimizers: systems that treat prompt engineering as an optimization problem rather than an art. When GEPA was released, I knew I had to test it.\nThink of it like compilation: you write high-level code (your task definition), and the compiler transforms it into optimized machine instructions (a prompt that actually works). You don‚Äôt hand-tune assembly, so why hand-tune prompts?"
  },
  {
    "objectID": "posts/gepa-impact/gepa_impact_final.html#the-pilot",
    "href": "posts/gepa-impact/gepa_impact_final.html#the-pilot",
    "title": "Programmatic Prompt Optimization: Replacing Intuition with Algorithms",
    "section": "The Pilot",
    "text": "The Pilot\nI needed a simple test to see if GEPA could work for my use case. So I created a synthetic dataset of 27 sales call transcripts that represented a real challenge we face: detecting presence of required behaviors and predicting call quality (good/bad). The transcripts were hand-labeled across 7 behavior categories (introduction, needs, value proposition, objection handling, benefit reinforcement, risk reduction, and closing). Small enough to iterate fast, realistic enough to validate the approach‚Äîand representative of a problem I‚Äôd hit repeatedly: intent extraction and call evaluation look easy for a few cases, but precision and recall tank at scale.\nI expected weeks of iteration. Instead, I got meaningful results in a single run. Usually a show piece like this would be carefully selected sample to show the power of the approach. Here this is literally first attempt, that in itself tells the power of the approach.\n\n\n\n  \n    \n      üìû Input: Call Transcript\n      agent: Hi, good afternoon! This is Maya calling from Citi Corp. Am I speaking with Jordan Lee?\n\ncustomer: Yes, this is Jordan.\n\nagent: Great, Jordan. How are you doing today?\n\ncustomer: I'm good, thanks. Busy afternoon, but I have a few minutes.\n\nagent: I appreciate you taking the time...\n    \n    \n      üìä Output: Analysis\n      \n        Call Quality\n        ‚úì Good\n      \n      \n        Detected Categories\n        \n          Introduction/Rapport ‚úì\n          Need Assessment ‚úì\n          Value Proposition ‚úó\n          Objection Handling ‚úì\n          Benefit Reinforcement ‚úì\n          Risk Reduction ‚úì\n          Call to Action ‚úì\n        \n      \n    \n  \n\n\n\n\nResults\n\n\n\n\n\n\n\n\n\nApproach\nCost\nTime\nAccuracy\n\n\n\n\nManual prompt engineering\n$100-1000 (engineer time)\nDays to weeks\n72%\n\n\nGEPA\n~$2\n10 hours\n81%\n\n\nGEPA with error analysis\n~$0.5\n3 hours\n90%\n\n\n\nThe optimizer ran for about 10 hours, cost roughly $2, and explored over 200 prompt variants. Through genetic mutation and Pareto selection, it whittled those down to 9 ‚Äúsurvivors‚Äù‚Äîprompts that excelled at different subsets of the problem. The best performer jumped from 72% to 81% accuracy, a lift I hadn‚Äôt achieved in months of manual tuning.\nThe intermediate prompts evolving caught my eye. I could see the optimizer discovering nuances I‚Äôd never thought to include: explicit definitions for each category, step-by-step rules for edge cases, domain-specific guidance about soft pulls versus hard pulls. The quality of the reasoning it produced while iterating was genuinely impressive.\nHopefully I have convinced you that this method is powerful, lets see how i did it and you can follow similar steps for yours as well."
  },
  {
    "objectID": "posts/gepa-impact/gepa_impact_final.html#detailed-implementation-of-gepa",
    "href": "posts/gepa-impact/gepa_impact_final.html#detailed-implementation-of-gepa",
    "title": "Programmatic Prompt Optimization: Replacing Intuition with Algorithms",
    "section": "Detailed Implementation of GEPA",
    "text": "Detailed Implementation of GEPA\nLet‚Äôs first understand what GEPA does, then dive into code for this specific usecase. If you want deeper dive on how GEPA works, i have previously written a detailed piece here: GEPA\nCode-first folks: here‚Äôs the notebook: github link\n\nWe‚Äôll use DSPy to run GEPA. If you‚Äôre new to DSPy, it‚Äôs a framework that treats prompts as code you can optimize programmatically. For background, see The Data Quarry‚Äôs guide.\n\n\n\nHow GEPA Works\nGEPA (Genetic-Pareto Algorithm) differs from traditional optimization in three key ways:\n\nReflective Mutation: The LLM reads failure feedback and proposes targeted improvements. It‚Äôs not random guessing‚Äîit‚Äôs reasoning about what went wrong.\nPareto Selection: Instead of keeping only the single best prompt, GEPA maintains a ‚Äúfrontier‚Äù of diverse specialists. One prompt might excel at detecting objection handling; another at predicting outcomes. This prevents catastrophic forgetting.\nText-as-Feedback: Traditional RL uses scalar rewards. GEPA exploits rich textual feedback (‚ÄúYou incorrectly marked this as rapport-building because‚Ä¶‚Äù) to guide mutations precisely.\n\n\n\n\nPrerequisites\nTo Use GEPA, we need 3 components.\n\n\n\n\n\n\n\n\nComponent\nWhat it does\nWhy it matters\n\n\n\n\nDSPy Signature\nYour baseline prompt defining the task\nThe ‚Äúprompt‚Äù being optimized\n\n\nMetric & Feedback\nReturns score + textual feedback\nTells optimizer what ‚Äúgood‚Äù looks like and why\n\n\nDataset\nLabeled examples (train/val/test)\nGround truth for evaluation\n\n\n\n\n\nThe DSPy Signature\nIn DSPy, A Signature defines input/output schema; the instructions in the docstring become part of the prompt. My initial prompt was embarrassingly simple‚Äîjust two lines:\nclass CallAnalysis(dspy.Signature):\n    \"\"\"\n    Read the provided call transcript and analyze it comprehensively.\n    Determine both: (1) which categories the agent displayed, and \n    (2) whether the call will lead to conversion or customer retention.\n    \"\"\"\n    message: str = dspy.InputField()\n    categories: List[Literal[\"introduction_rapport_building\", \"need_assessment_qualification\", \n                             \"value_proposition_feature_mapping\", \"objection_handling\", \n                             \"benefit_reinforcement\", \"risk_reduction_trust_building\", \n                             \"call_to_action_closing\"]] = dspy.OutputField()\n    final_result: Literal['good', 'bad'] = dspy.OutputField()\n\nprogram = dspy.ChainOfThought(CallAnalysis)\n\n\n\nMetric Function\nA Metric tells us whether we are moving in the right direction. In this case, accuracy of categories detected and final prediction that whether call was good or bad - both were important. Hence metric will be mean of both the entities\ndef call_qual_metric(gold, pred):\n    return 1.0 if gold == pred else 0.0\n\ndef category_qual_metric(gold, pred):\n    \"\"\"Compute score for categories using set operations.\"\"\"\n    pred_set = set(pred)\n    gold_true = {k for k, v in gold.items() if v}\n    gold_false = {k for k, v in gold.items() if not v}\n    \n    correct = len(gold_true & pred_set) + len(gold_false - pred_set)\n    return correct / len(gold)\n\ndef comb_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n    \"\"\"Overall metric combining both scores.\"\"\"\n    call_qual = call_qual_metric(gold.final_result, pred.final_result)\n    category_qual = category_qual_metric(gold.categories, pred.categories)\n    return (call_qual + category_qual) / 2\n\n\n\nAdding Feedback\nThis is the key enabler. A basic metric only returns a score but not why it failed. With feedback, the optimizer can reason about failures and propose targeted fixes.\n\n\n\n\n\n  \n    ‚ùå Metric (Score Only)\n    def comb_metric(example, pred):\n    gold_cat = json.loads(example['answer'])\n    gold_final = example['final_result']\n\n    # Category score\n    correct = sum(1 for k, v in gold_cat.items() \n                  if (v and k in pred.categories) or \n                     (not v and k not in pred.categories))\n    cat_score = correct / len(gold_cat)\n\n    # Final result score\n    final_score = 1.0 if gold_final == pred.final_result else 0.0\n\n    return (cat_score + final_score) / 2\n    \n    Problem: Optimizer only knows \"0.7\" ‚Äî no idea why it failed.\n  \n  \n    ‚úÖ Metric with Feedback\n    def comb_metric_with_feedback(example, pred, pred_name=None):\n    # ... same scoring logic as above ...\n\n    # Generate textual feedback\n    if gold_final != pred.final_result:\n        fb = f\"Incorrect: predicted {pred.final_result}, actual {gold_final}\"\n    else:\n        fb = f\"Correct: {gold_final}\"\n\n    if incorrectly_included:\n        fb += f\"\\nFalse positives: {incorrectly_included}\"\n    if incorrectly_excluded:\n        fb += f\"\\nMissed categories: {incorrectly_excluded}\"\n\n    return dspy.Prediction(score=score, feedback=fb)\n    \n    Benefit: Optimizer sees \"Missed: intro_rapport\" ‚Üí can propose targeted fix.\n  \n\n\n\nCode example:\ndef call_qual_feedback(gold, pred):\n    \"\"\" Generate feedback for final result module. \"\"\"\n    if gold == pred:\n        fb = f\"You correctly classified the sales call as `{gold}`. This sales call is indeed `{gold}`.\"\n    else:\n        fb = f\"You incorrectly classified the sales call as `{pred}`. The correct sales call is `{gold}`. Think about how you could have reasoned to get the correct sales call label.\"\n    return fb\n\ndef category_qual_feedback(gold, pred):\n    \"\"\"Generate feedback using set operations.\"\"\"\n    pred_set = set(pred)\n    gold_true = {k for k, v in gold.items() if v}\n    gold_false = {k for k, v in gold.items() if not v}\n    \n    correctly_included = gold_true & pred_set\n    incorrectly_included = gold_false & pred_set\n    incorrectly_excluded = gold_true - pred_set\n    correctly_excluded = gold_false - pred_set\n    \n    score = (len(correctly_included) + len(correctly_excluded)) / len(gold)\n    \n    if score == 1.0:\n        return f\"Perfect. Correctly identified: `{correctly_included}`.\", score\n    \n    fb = f\"Correctly identified: `{correctly_included}`.\\n\"\n    if incorrectly_included:\n        fb += f\"False positives: `{incorrectly_included}`.\\n\"\n    if incorrectly_excluded:\n        fb += f\"Missed: `{incorrectly_excluded}`.\\n\"\n    return fb\n\ndef comb_metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):\n    \"\"\"\n    Computes a score and provides feedback for the call analysis prediction.\n    Returns total score if pred_name is None, otherwise returns dspy.Prediction with score and feedback.\n    \"\"\"\n    # Compute feedback and scores\n    cal_fb = call_qual_feedback(gold.final_result, pred.final_result)\n    cat_fb = category_qual_feedback(gold.categories, pred.categories)\n    fb = cal_fb + '\\n' + cat_fb\n    score = comb_metric(gold, pred)\n    return dspy.Prediction(score=score, feedback=fb)\n\n\n\n\nRunning GEPA\nWith prerequisites in place, optimization is straightforward:\nfrom dspy import GEPA\n\noptimizer = GEPA(\n    metric=comb_metric_with_feedback,\n    auto=\"light\",\n)\n\noptimized_program = optimizer.compile(program, trainset=tset, valset=vset)\n\n\n\nPost GEPA run\nGEPA ran for 10 hrs on my PC. I saw the 2-line prompt evolve into ~1,500 words of instruction discovering nuances like ‚Äúa bare greeting isn‚Äôt rapport-building; look for warmth and time acknowledgment‚Äù.\n\n\n\n\n\n\nNotePay Attention\n\n\n\nIn some cases, it came up with better instructions and heuristics than me. Kindah felt fearful yet exciting.\n\n\n\n\n\n\n  \n    Initial Prompt\n    Read the provided call transcript and analyze it comprehensively.\nDetermine both: (1) which categories the agent displayed, and (2) whether the call will lead to conversion or customer retention.\n  \n  \n    Optimized Prompt (GEPA)\n    New Instructions for Analyzing Banking/Card Transaction Call Transcripts\n\nOverview\nYou are an analysis assistant whose job is to evaluate sales/transact‚Äëion-focused call transcripts in the banking/credit-card domain. For each transcript, produce a compact, structured analysis with two main objectives:\n  (a) identify the agent behavior categories demonstrated (from the seven pillars below), and\n  (b) judge whether the call outcome is good or bad.\n\nInputs you will receive\n- A complete transcript of a single call between an agent and a customer. Transcripts may include labels such as \"agent:\" and \"customer:\" and may cover topics like card offers, fees, rewards, security, and next steps.\n\nWhat you must produce (three sections exactly)\n1) reasoning\n   - Provide a concise, bullets-style justification for every pillar category you detected in the transcript.\n   - Include short quotes or paraphrases from the transcript to illustrate why the category applies. Do not introduce facts or assumptions beyond what is in the transcript.\n   - If you detect a strength/weakness signal about the outcome, include a brief, one- to two-sentence note here describing how strong the signal is and what would push it toward conversion or toward retention.\n   - This section may contain a small, optional note about outcome strength, but must not introduce information outside the transcript.\n\n2) categories\n   - Output a Python-like list of the detected pillar categories in the exact order they first appeared in the transcript.\n   - Example format: ['introduction_rapport_building', 'need_assessment_qualification', ...]\n\n3) final_result\n   - A single word indicating the call outcome:\n     - good ‚Äî the call demonstrates strong agent performance and is likely to lead to conversion or retention.\n     - bad ‚Äî the call shows weak agent performance or missed opportunities.\n   - Do not add any qualifiers in this field; use exactly one of the two keywords above.\n\nOptional but encouraged: assess the strength of the outcome\n- If you include it (recommendation), place this assessment only in reasoning as the optional strength_of_outcome note. Keep it concise (one or two sentences). It should address:\n  - How strong is the good/bad signal?\n  - What would most likely push the outcome toward good or toward bad?\n\nPillar definitions (seven bank/card-specific categories)\n- introduction_rapport_building\n  - Includes opening greetings, courtesy, acknowledgment of time, and attempts to establish rapport.\n  - Examples: greetings, confirming time, polite introductions, small talk about fit or time constraints.\n- need_assessment_qualification\n  - Involves asking about customer needs, usage, spend patterns, eligibility checks, and whether the product fits (e.g., business vs personal, employee cards, annual fees, soft vs hard pulls).\n- value_proposition_feature_mapping\n  - Linking card features to tangible, customer-relevant benefits (rewards, protections, credits) and showing how those features align with stated needs.\n- objection_handling\n  - Addressing concerns about price, complexity, trust, enrollment, or process obstacles. Includes acknowledging concerns and offering clarifications or mitigations.\n- benefit_reinforcement\n  - Reiterating concrete benefits and value after objections or hesitations, often tying back to the customer's stated needs.\n- risk_reduction_trust_building\n  - Providing security assurances, privacy protections, non-hard-pull options, guarantees, terms clarity, or brand trust signals.\n- call_to_action_closing\n  - Concrete next steps or commitments: soft checks, secure links, email/mail options, scheduling follow-ups, or instructions to apply/get more information.\n\nHow to apply the rules\n- For every transcript, read from start to finish. Mark each pillar as soon as its criteria are clearly demonstrated.\n- If a single utterance clearly satisfies more than one pillar, count it under all applicable pillars.\n- If a pillar is not clearly demonstrated anywhere in the transcript, do not include it in the categories list.\n- Record the detected pillars in the exact order of their first appearance in the transcript.\n- The final_result should reflect the overall trajectory of the call as described above.\n\nOutput constraints and format\n- Do not introduce any facts not present in the transcript.\n- Do not insert subjective opinions beyond what is grounded in the transcript.\n- Use the exact section headings and formatting:\n  reasoning\n  categories\n  final_result\n- Do not include extraneous content beyond the three sections above.\n\nDomain-specific considerations\n- You may encounter references to soft pulls vs hard pulls, online applications, secure links, email follow-ups, or scheduled follow-ups. Treat these as legitimate \"call_to_action_closing\" or \"risk_reduction_trust_building\" elements as appropriate.\n- When quoting or paraphrasing, keep quotes brief and focused on the reason for the pillar.\n- If PII appears in the transcript (e.g., partial SSN or addresses), quote minimally and do not reveal full sensitive data in your justification. You may paraphrase or reference the presence of sensitive data without reproducing it.\n\nExample behavior (not to reproduce here)\n- A transcript with strong agent behaviors and clear next steps is more likely good; a transcript with missed opportunities, customer hesitation, or weak closing is more likely bad.\n\nEnd result\n- Return exactly three sections for every transcript analyzed, with the content governed by the rules above. This format enables consistent, comparable, and transparent analysis across transcripts.\n  \n\n\n\nResult: 72% ‚Üí 81% accuracy. More importantly, the process was repeatable.\nFor a deeper technical dive: GEPA Deepdive"
  },
  {
    "objectID": "posts/gepa-impact/gepa_impact_final.html#breaking-through-the-81-ceiling-with-error-analysis",
    "href": "posts/gepa-impact/gepa_impact_final.html#breaking-through-the-81-ceiling-with-error-analysis",
    "title": "Programmatic Prompt Optimization: Replacing Intuition with Algorithms",
    "section": "Breaking Through the 81% Ceiling with Error Analysis",
    "text": "Breaking Through the 81% Ceiling with Error Analysis\nWith the above proof, I deployed it internally and it worked reliably wherever it was implemented properly. But I wasn‚Äôt satisfied. When I manually reviewed the failing cases, something bothered me: these weren‚Äôt hard examples. Given a hint, the LLM could easily get them right. So why was it failing?\nSo I dug in and exported the misclassified examples to a spreadsheet and studied them systematically:\n\n\n\n\n\n\n\n\n\n\nInput (truncated)\nActual\nPredicted\nWhat Went Wrong\n\n\n\n\nTyler calling from dominos‚Ä¶ ‚Äúsounds good, I‚Äôll send that link‚Ä¶‚Äù\nbad\ngood\nCustomer showed hesitation (‚ÄúI‚Äôm really not sure‚Ä¶‚Äù) but agent rushed to close\n\n\nMark from JP calling about credit solutions‚Ä¶\nrapport = false\nrapport = true\nAgent said ‚ÄúHi, this is Mark from JP‚Äù with no warmth or rapport signals\n\n\n\n\nThe pattern became clear: the model was over-detecting introduction_rapport_building‚Äîtreating any greeting as rapport. It also sometimes marked calls as ‚Äúgood‚Äù just because next steps existed, even when the customer showed clear hesitation.\n\n\n\n\nüéØ GEPA Run 1\n\n\n72% ‚Üí 81%\n\n\n‚Üí\n\n\nüîç Error Analysis\n\n\nExport failures, find patterns\n\n\n‚Üí\n\n\n‚úèÔ∏è Add Feedback\n\n\nTargeted hints for failures\n\n\n‚Üí\n\n\nüöÄ GEPA Run 2\n\n\n81% ‚Üí 90%\n\n\n\n\nThe Solution: Targeted Feedback in the Metric\nMy hunch was simple: if I could teach the optimizer why these specific cases failed, it could learn the distinctions. I added a feedback column to my dataset and filled it only for mistagged cases‚Äîwriting out precisely why my label was correct. For example:\n\n‚ÄúThe agent said ‚ÄòHi, this is Mark from JP‚Äô without warmth, time acknowledgment, or rapport-building. A bare introduction doesn‚Äôt qualify as introduction_rapport_building.‚Äù\n\n\n\n\n\n\nExample\n\n\nIter 1\n\n\nFeedback\n\n\nIter 2\n\n\nFeedback\n\n\nIter 3\n\n\n\n\nTyler\n\n\n‚ùå\n\n\nLook for hesitation signals\n\n\n‚úÖ\n\n\n‚Äî\n\n\n‚Äî\n\n\n\n\nMark\n\n\n‚ùå\n\n\nGreeting ‚â† rapport\n\n\n‚ùå\n\n\nNeed warmth/courtesy\n\n\n‚úÖ\n\n\n\n\nJohn\n\n\n‚ùå\n\n\nGreeting ‚â† rapport\n\n\n‚úÖ\n\n\n‚Äî\n\n\n‚Äî\n\n\n\n\nThen I passed this feedback directly to GEPA through the metric function:\ndef comb_metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):\n    \"\"\"Metric that returns score + feedback, including targeted hints for known failure cases.\"\"\"\n    # Compute scores\n    call_qual = call_qual_metric(gold.final_result, pred.final_result)\n    category_qual = category_qual_metric(gold.categories, pred.categories)\n    score = (call_qual + category_qual) / 2\n    \n    # Generate base feedback\n    cal_fb = call_qual_feedback(gold.final_result, pred.final_result)\n    cat_fb = category_qual_feedback(gold.categories, pred.categories)\n    fb = cal_fb + '\\n' + cat_fb\n\n    # Append targeted feedback from the dataset (if present)\n    fb += gold.feedback\n\n    return dspy.Prediction(score=score, feedback=fb)\nNow when GEPA‚Äôs reflective mutations analyze failures, they see specific guidance like ‚Äúlook for warmth signals, not just greetings‚Äù instead of generic ‚Äúwrong answer‚Äù feedback.\n\n\n\n\n\n\nTipüí° Pro tip:\n\n\n\nYou could use an LLM to generate this feedback automatically, but doing it manually gives you fine-grained control over how the final prompt evolves. The LLM might miss the exact nuance you care about. In short, you‚Äôre writing the prompt through feedback.\n\n\n\n\nThe Result\n81% ‚Üí 90% accuracy in just 3 hours and ~$0.50.\nThe key insight: GEPA‚Äôs genetic mutations work best when they have precise feedback to reason about. Generic ‚Äúwrong answer‚Äù feedback produces generic improvements. Targeted feedback like ‚Äúyou‚Äôre conflating greetings with rapport‚Äù produces targeted fixes."
  },
  {
    "objectID": "posts/gepa-impact/gepa_impact_final.html#lessons-learned-recommended-workflow",
    "href": "posts/gepa-impact/gepa_impact_final.html#lessons-learned-recommended-workflow",
    "title": "Programmatic Prompt Optimization: Replacing Intuition with Algorithms",
    "section": "Lessons Learned & Recommended Workflow",
    "text": "Lessons Learned & Recommended Workflow\nThe biggest lesson wasn‚Äôt technical, it was psychological. I stopped thinking of prompts as things I write and started thinking of them as things I evolve. My job shifted from ‚Äúcraft the perfect prompt‚Äù to ‚Äúdefine what good looks like and let the system find it.‚Äù\n\nWhen NOT to Use GEPA\nGEPA only works when you can clearly define ‚Äúcorrect.‚Äù If you can‚Äôt reliably label examples yourself or if the target is fuzzy or subjective, the optimizer will chase noise. I tried it on a sentiment task where human annotators disagreed 30% of the time. The results were inconsistent. Rule: if you can‚Äôt do it clearly as a human, don‚Äôt expect GEPA to figure it out.\n\n\nMy Recommended Workflow\n\nStart with a clear metric and a personally-validated dataset. 20-50 examples you‚Äôve labeled yourself, understanding the edge cases.\nRun GEPA ‚Üí Review failures ‚Üí Add targeted feedback. Study misclassified examples. Write specific explanations of why your label is correct.\nRun GEPA again ‚Üí Stop when acceptable ‚Üí Deploy. One or two iterations usually suffice. Don‚Äôt chase perfection.\nPost-deployment: monitor, collect failures, iterate. Production surfaces edge cases. Add them with feedback and re-run as needed.\n\n\nThe complete Jupyter notebook: github link"
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html",
    "href": "posts/gepa-deepdive/gepa_final_article.html",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "This article was made possible by the research papers referenced throughout, the Weaviate discussion with Lakshya A Agrawal, and guidance from Kerem Turgutlu at answer.ai. Created using the solveit platform by fast.ai.\nYou‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.\nBut what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) does exactly this‚Äîby exploiting something traditional optimization ignores: the detailed textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "href": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "title": "GEPA Deepdive",
    "section": "How GEPA Works: Building It From Scratch",
    "text": "How GEPA Works: Building It From Scratch\nGEPA combines two key innovations: reflective prompt mutation (learning from textual feedback) and Pareto selection (preserving diverse specialists). Each helps on its own; combined, they reinforce each other.\nWe‚Äôll build them from scratch in this section:\n\nReflective mutation ‚Äî How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.\nPareto selection ‚Äî Why always improving your ‚Äúbest‚Äù prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.\nMerge ‚Äî How GEPA combines insights from divergent lineages (covered after the complete algorithm).\n\nBy the end, you‚Äôll see how these mechanisms combine into GEPA‚Äôs full evolutionary loop.\n\n\nQuick Start: Using GEPA in 30 Seconds\n\nüìñ Full tutorial: GEPA for AIME (Math) ‚Äî optimizing GPT-4.1 Mini from 46.6% ‚Üí 56.6% on AIME 2025.\n\nBefore diving deep, here‚Äôs what using GEPA looks like in practice:\nStep 1: Configure your language model\nimport dspy\n\nlm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, max_tokens=32000)\ndspy.configure(lm=lm)\nStep 2: Define your program\nprogram = dspy.ChainOfThought(\"problem -&gt; answer\")\nStep 3: Define a metric that returns feedback (not just a score)\nThis is the key difference from other optimizers‚Äîyour metric explains why something failed:\ndef metric_with_feedback(example, prediction, trace=None, **kwargs):\n    correct_answer = example.answer\n    pred_answer = prediction.answer\n    \n    score = int(correct_answer == pred_answer)\n    \n    if score == 1:\n        feedback = f\"Correct! The answer is '{correct_answer}'.\"\n    else:\n        feedback = f\"Incorrect. Expected '{correct_answer}', got '{pred_answer}'.\"\n        # Add any additional context that could help improvement:\n        if hasattr(example, 'solution'):\n            feedback += f\" Solution: {example.solution}\"\n    \n    return dspy.Prediction(score=score, feedback=feedback)\nStep 4: Optimize with GEPA\nfrom dspy import GEPA\n\noptimizer = GEPA(\n    metric=metric_with_feedback,\n    auto=\"light\",           # Budget preset: \"light\", \"medium\", or \"heavy\"\n    num_threads=32,         # Parallel evaluation threads\n)\n\noptimized_program = optimizer.compile(\n    program,\n    trainset=train_set,\n    valset=val_set,\n)\nGEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally. If you just want to use it, the code above is sufficient‚Äîsee the DSPy GEPA API Reference for full parameter details.\nBut what‚Äôs actually happening? Your metric returns why something failed (not just a score), an LLM reads that feedback and proposes improved instructions, Pareto selection preserves diverse specialists rather than just the highest-scoring prompt, and merge operations combine insights from divergent lineages.\nThe rest of this section builds the core mechanism from scratch.\n\n\nGEPA: REFLECTIVE PROMPT EVOLUTION (Flow diagram from paper)\n\n\n\nGEPA Flowchart\n\n\nThe diagram above shows GEPA‚Äôs loop. Each prompt is evaluated on every training task, producing a per-instance score matrix. Pareto filtering preserves prompts that excel at something no other prompt beats. New candidates come from reflective mutation (analyzing textual feedback) or merge (combining two specialists‚Äô insights). Only candidates passing a minibatch screen get full evaluation.\nThe next section implements reflective mutation: the ‚ÄúReflect and Propose New Prompt‚Äù step in the diagram.\n\n\nHands-On: Building Reflective Mutation from Scratch\nTo see how this works by implementing GEPA‚Äôs core mechanism on a real task.\nThe Problem: AIME Math Competition\nWe‚Äôll optimize prompts for solving AIME (American Invitational Mathematics Examination) problems ‚Äî challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.\nfrom datasets import load_dataset\ndset = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n# 90 problems with solutions and integer answers\n\n\n\n\n\n\n\n\nproblem\nsolution\nanswer\n\n\n\n\nQuadratic polynomials \\(P(x)\\) and \\(Q(x)\\) have l‚Ä¶\nLet \\(R(x)=P(x)+Q(x).\\) Since the \\(x^2\\)-terms of‚Ä¶\n116\n\n\nThree spheres with radii \\(11\\), \\(13\\), and \\(19\\) ‚Ä¶\nThis solution refers to the Diagram section‚Ä¶\n756\n\n\n\nWhy AIME for testing prompt optimization?\n\nClear ground truth ‚Äî Every answer is an integer (0-999), so evaluation is unambiguous\nRich failure modes ‚Äî Wrong answers come from algebraic errors, missed cases, misread constraints\nDomain knowledge helps ‚Äî Prompts that encode strategies (‚Äúsubtract equations pairwise‚Äù, ‚Äúenumerate all cases‚Äù) measurably improve performance\nSmall dataset ‚Äî Only 90 problems, so sample efficiency matters\n\nThe Setup\n# Split: 10 train, 10 validation (simulating scarce labeled data)\ntdf = df.sample(45).iloc[:10]  # Training mini-batches drawn from here\nvdf = df.drop(tdf.index).iloc[:10]  # Held-out validation\n\n# Base model: Gemini 2.5 Flash via LiteLLM\n# Metric: Exact match (predicted integer == ground truth)\ndef metric(ground_truth, prediction):\n    return int(ground_truth) == prediction['answer']\nSeed Prompt\nWe start with a minimal instruction:\nseed_prompt = \"\"\"You are given a problem and you have to give the answer \nalong with reasoning. Do not return anything apart from json. \nIt should be parsable by json.loads()\"\"\"\nBaseline validation accuracy: 10% (1/10 correct)\nCan reflective mutation improve this? Let‚Äôs find out.\n\n\n\nStep 1: The Feedback Function\nFirst, we need a function that tells the reflection LLM what went wrong. We‚Äôll start with minimal feedback‚Äîjust the correct answer:\ndef feedback(ground_truth, prediction):\n    if int(ground_truth) != prediction['answer']:\n        return f'You got it wrong! The solution is {ground_truth}'\n    return 'You got it right!'\nThis is deliberately simple. Later we‚Äôll discuss how richer feedback (like expert solutions) can improve results.\n\n\n\nStep 2: The Reflection Prompt\nFollowing GEPA‚Äôs structure, we build a prompt that shows the LLM its failures:\nREFLECTION_TEMPLATE = \"\"\"I provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\n{current_prompt}\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n{examples}\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\"\"\"\n\ndef mk_reflection_prompt(df, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\nExample filled-in reflection prompt:\nI provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\nYou are given a problem and you have to give the answer along with reasoning. \nDo not return anything apart from json. It should be parsable by json.loads()\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n# Example 1\n## problem\nQuadratic polynomials P(x) and Q(x) have leading coefficient 1. The sum of the roots of P(x) is 7...\n## prediction\n{\"answer\": 42, \"reasoning\": \"I solved the system and got x=7, y=6\"}\n## feedback\nYou got it wrong! The solution is 116\n\n# Example 2\n## problem\nThree spheres with radii 11, 13, and 19 are mutually externally tangent...\n## prediction\n{\"answer\": 756, \"reasoning\": \"Using the tangent sphere formula...\"}\n## feedback\nYou got it right!\n\n# Example 3\n## problem\nFind the remainder when 2^2024 is divided by 1000...\n## prediction\n{\"answer\": 16, \"reasoning\": \"I computed powers of 2 mod 1000...\"}\n## feedback\nYou got it wrong! The solution is 896\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\n\n\nStep 3: The Complete Optimization Loop\ndef mk_reflection_prompt(mb, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\n\ndef reflect(mb, curr_prompt):\n    \"\"\"Ask LLM to reflect on failures and propose improved instruction.\"\"\"\n    refl_prompt = mk_reflection_prompt(mb, curr_prompt)\n    return _call(refl_prompt, format=ReflectionModel)['new_instruction']\n\ndef optimize_prompt(seed_prompt, traindf, valdf, n_iters=3, mb_size=3):\n    \"\"\"Greedy reflective prompt optimization.\"\"\"\n    prompts, train_scores, val_scores = [seed_prompt], [], []\n    mb = traindf.sample(mb_size)  # Fixed minibatch for this run\n    \n    print(f'Baseline validation: {eval_val(seed_prompt, valdf):.2%}')\n    \n    for i in range(n_iters):\n        # Evaluate current prompt on minibatch\n        mb_eval, mb_score = eval_mb(prompts[-1], mb)\n        print(f\"üìä Minibatch: {mb_score:.2%}\")\n        \n        # Reflect and propose new instruction\n        new_instr = reflect(mb_eval, prompts[-1])\n        new_prompt = new_instr  # The new instruction becomes the new prompt\n        \n        # Evaluate on validation set\n        val_score = eval_val(new_prompt, valdf)\n        print(f\"üìä Validation: {val_score:.2%}\")\n        \n        prompts.append(new_prompt)\n        val_scores.append(val_score)\n    \n    return dict(prompts=prompts, val_scores=val_scores)\n\n\n\nWhat Actually Happened\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\n\n\n\n\n\n\nIteration\nMinibatch\nValidation\nWhat the reflection learned\n\n\n\n\nBaseline\n‚Äî\n10%\n‚Äî\n\n\n1\n0%\n10%\nJSON formatting details, output structure rules\n\n\n2\n0%\n30%\nSystems of equations strategy, remainder/modular arithmetic tips\n\n\n3\n67%\n10%\nOver-specialized on number theory, solved #9 but lost generality\n\n\n\nThe good: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:\n\n‚ÄúWhen dealing with systems of equations like \\(xy + Az = C\\), \\(yz + Ax = C\\), \\(zx + Ay = C\\), consider subtracting equations pairwise to find relationships between variables, such as \\((x-z)(y-A)=0\\), which implies \\(x=z\\) or \\(y=A\\). Systematically explore all such cases.‚Äù\n\nThis is directly from the actual output‚Äîthe reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.\nThe bad: Iteration 3 achieved 67% on its minibatch but dropped to 10% validation. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:\n\n‚ÄúWhen the problem involves number theory and remainders (e.g., \\(n \\pmod x\\), \\(n \\pmod y\\)), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.‚Äù\n\nThis over-specialized advice (‚Äúcan lead to an answer of 0‚Äù) actively hurt performance on non-number-theory problems, dropping validation from 30% back to 10% (1/10)‚Äîthough notably, it did solve problem #9, which earlier prompts couldn‚Äôt.\n\n\n\nThe Greedy Selection Problem\nThis demonstrates exactly why GEPA uses Pareto selection instead of always taking the ‚Äúbest‚Äù prompt:\n\nIteration 2‚Äôs prompt was a specialist‚Äîit learned something valuable about systems of equations\nIteration 3 tried to improve on iteration 2, but the minibatch had different problems\nThe reflection overwrote the systems-of-equations insight while adding number-theory tips that were too specific\nResult: catastrophic forgetting\n\nWith greedy selection, we would have discarded iteration 2‚Äôs valuable insight. Pareto selection would keep it‚Äîbecause it was best on at least one validation instance.\n\n\n\nThe Missing Ingredient: Rich Feedback\nOur minimal feedback (\"You got it wrong! The solution is 349\") only tells the model that it failed, not why or how to fix it.\nThe AIME dataset includes expert solutions. A richer feedback function could use them:\ndef feedback_rich(row):\n    if int(row.answer) != row.pred['answer']:\n        sol = row.solution[:500] + \"...\" if len(row.solution) &gt; 500 else row.solution\n        return f\"\"\"Wrong! Expected {row.answer}, got {row.pred['answer']}.\n        \nModel's reasoning: {row.pred['short_reasoning']}\n\nExpert solution approach:\n{sol}\"\"\"\n    return \"Correct!\"\nExample output for a wrong answer:\nWrong! Expected 116, got 42.\n\nModel's reasoning: I set up the system of equations and solved for x=7, y=6, giving 7*6=42.\n\nExpert solution approach:\nLet R(x)=P(x)+Q(x). Since the x¬≤-terms of P and Q have leading coefficient 1, \nR(x) is quadratic with leading coefficient 2. Given the roots condition, we can \nwrite R(x) = 2(x-r‚ÇÅ)(x-r‚ÇÇ). Expanding and comparing coefficients...\nWith rich feedback, the reflection LLM can extract specific strategies from the expert solution rather than having to guess what went wrong. This is what makes GEPA sample-efficient: the feedback contains the fix.\nCompare this to RL, which would only see reward = 0 and have to statistically infer what went wrong across thousands of trajectories.\n\n\n\nWhat We Learned\n\nReflective mutation works ‚Äî Even with minimal feedback, the LLM extracted useful domain knowledge\nGreedy selection fails ‚Äî Iteration 3‚Äôs collapse shows why we need to preserve specialist insights\nFeedback quality matters ‚Äî Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with\nSample efficiency is real ‚Äî We saw meaningful optimization with just 3 iterations on 3 examples each\n\nThis is the core limitation of greedy optimization: catastrophic forgetting. The solution? Pareto selection‚Äîwhich we‚Äôll build from scratch next.\n\n\nHands-On: Building the Pareto Frontier\nIn the reflective mutation section, we saw greedy selection fail‚Äîiteration 3‚Äôs over-specialized prompt dropped validation from 30% to 10%, losing iteration 2‚Äôs valuable systems-of-equations insights. The fundamental problem: always improving your ‚Äúbest‚Äù prompt discards specialist knowledge.\nGEPA‚Äôs solution: Pareto selection. Instead of keeping one best prompt, maintain a frontier of prompts where each excels at something no other prompt beats.\n\n\nWhat is Pareto Dominance?\nA prompt dominates another if it‚Äôs at least as good everywhere, and strictly better somewhere:\n\n‚â• on every validation instance, AND\n\n&gt; on at least one instance\n\nIf prompt A dominates prompt B, we can safely discard B‚ÄîA is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.\nExample: Consider four prompts evaluated on 10 validation instances. We‚Äôll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:\n\n\n\nPrompt\nInstances Solved\nAggregate\nStatus\n\n\n\n\nP0 (seed)\n#0 only\n10%\nDominated by P2\n\n\nP1 (iter 1)\n#0 only\n10%\nDominated by P2\n\n\nP2 (iter 2)\n#0, #1, #2\n30%\nFrontier ‚úì\n\n\nP3 (iter 3)\n#9 only\n10%\nFrontier ‚úì\n\n\n\nP2 dominates both P0 and P1‚Äîit solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which nothing else could.\nThe Pareto frontier is {P2, P3}. Both contain unique value.\n\n\n\nImplementation: Dominance Checking\nWe represent per-instance scores as boolean arrays (1 = solved, 0 = failed):\nimport numpy as np\n\ndef dominates(candidate_scores, other_scores):\n    \"\"\"Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)\"\"\"\n    candidate = np.array(candidate_scores)\n    other = np.array(other_scores)\n    return (candidate &gt;= other).all() and (candidate &gt; other).any()\n\ndef is_dominated_by_any(new_scores, frontier_scores):\n    \"\"\"Is new_scores dominated by ANY prompt in the frontier?\"\"\"\n    new = np.array(new_scores)\n    for existing in frontier_scores:\n        if dominates(np.array(existing), new):\n            return True\n    return False\n\ndef get_dominated_indices(new_scores, frontier_scores):\n    \"\"\"Which frontier prompts does new_scores dominate?\"\"\"\n    new = np.array(new_scores)\n    return [i for i, existing in enumerate(frontier_scores) \n            if dominates(new, np.array(existing))]\n\n\n\nTracing Through: Why P3 Survives\nLet‚Äôs verify the dominance relationships from our example:\nP0 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP1 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP2 = [1,1,1,0,0,0,0,0,0,0]  # Solves: #0, #1, #2\nP3 = [0,0,0,0,0,0,0,0,0,1]  # Solves: #9\n\n# Does P2 dominate P0?\ndominates(P2, P0)  # True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2\n\n# Does P2 dominate P1?\ndominates(P2, P1)  # True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2\n\n# Does P2 dominate P3?\ndominates(P2, P3)  # False! P2 loses on #9 (0 &lt; 1)\n\n# Does P3 dominate P2?\ndominates(P3, P2)  # False! P3 loses on #0, #1, #2\nNeither P2 nor P3 dominates the other‚Äîthey‚Äôre Pareto incomparable. Each solves problems the other can‚Äôt. Both stay on the frontier.\n\n\n\nPareto Frontier Explained\n\n\n\n\n\nThe Complete Frontier Manager\nclass ParetoFrontier:\n    def __init__(self):\n        self.prompts = []\n        self.scores = []  # scores[i][j] = prompt i's score on instance j\n    \n    def add(self, prompt, instance_scores):\n        \"\"\"Try to add a prompt. Returns True if it joins the frontier.\"\"\"\n        # Reject if dominated by existing frontier member\n        if is_dominated_by_any(instance_scores, self.scores):\n            return False\n        \n        # Remove any frontier members this prompt dominates\n        dominated = get_dominated_indices(instance_scores, self.scores)\n        for i in sorted(dominated, reverse=True):  # Remove from end first\n            del self.prompts[i]\n            del self.scores[i]\n        \n        # Add to frontier\n        self.prompts.append(prompt)\n        self.scores.append(instance_scores)\n        return True\n    \n    def sample(self):\n        \"\"\"Sample a prompt, weighted by unique wins.\"\"\"\n        weights = []\n        scores_arr = np.array(self.scores)\n        for i in range(len(self.prompts)):\n            # How many instances is this prompt *uniquely* best on?\n            others_best = np.delete(scores_arr, i, axis=0).max(axis=0) if len(self.prompts) &gt; 1 else np.zeros_like(scores_arr[i])\n            unique_wins = (scores_arr[i] &gt; others_best).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        \n        return np.random.choice(self.prompts, p=np.array(weights)/sum(weights))\n    \n    def best_aggregate(self):\n        \"\"\"Return the prompt with highest aggregate score.\"\"\"\n        aggregates = [sum(s) for s in self.scores]\n        return self.prompts[np.argmax(aggregates)]\n\n\n\nPutting It Together: Pareto-Guided Optimization\ndef optimize_with_pareto(seed_prompt, traindf, valdf, n_iters=5, mb_size=3):\n    \"\"\"Reflective mutation with Pareto frontier selection.\"\"\"\n    frontier = ParetoFrontier()\n    \n    # Initialize with seed\n    seed_scores = evaluate_per_instance(seed_prompt, valdf)\n    frontier.add(seed_prompt, seed_scores)\n    print(f'Baseline: {sum(seed_scores)/len(seed_scores):.1%}')\n    \n    for i in range(n_iters):\n        print(f\"\\n{'='*40}\\nIteration {i+1}\\n{'='*40}\")\n        \n        # Sample parent from frontier (weighted by unique wins)\n        parent = frontier.sample()\n        \n        # Run on minibatch, reflect, propose mutation\n        mb = traindf.sample(mb_size)\n        mb_results = evaluate_with_traces(parent, mb)\n        new_prompt = reflect_and_mutate(parent, mb_results)\n        \n        # Evaluate on full validation set\n        new_scores = evaluate_per_instance(new_prompt, valdf)\n        new_agg = sum(new_scores) / len(new_scores)\n        print(f\"New prompt: {new_agg:.1%} aggregate\")\n        \n        # Try to add to frontier\n        if frontier.add(new_prompt, new_scores):\n            print(f\"‚úì Added to frontier (size: {len(frontier.prompts)})\")\n        else:\n            print(f\"‚úó Dominated, rejected\")\n    \n    return frontier.best_aggregate()\n\n\n\nWhat We Observed on AIME\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\nIteration\nAggregate\nInstances Solved\nFrontier Action\n\n\n\n\nSeed\n10%\n#0\nInitialize\n\n\n1\n10%\n#0\nDominated by seed, rejected\n\n\n2\n30%\n#0, #1, #2\nAdded, dominates seed\n\n\n3\n10%\n#9 only\nAdded ‚úì (unique win on #9)\n\n\n\nThe key moment: Iteration 3 scored only 10%‚Äîworse than iteration 2‚Äôs 30%. Greedy selection would discard it entirely.\nBut it solved instance #9, which nothing else could. Pareto selection preserves it.\nOur final frontier: {P2, P3}\n\nP2: Strong generalist (30%), knows systems-of-equations strategies\nP3: Instance-9 specialist (10%), knows whatever cracked that specific problem\n\nBoth insights survive. The merge operation (covered later) can combine them.\n\n\n\nWhy This Matters: No More Catastrophic Forgetting\n\n\n\nSelection Strategy\nWhat happens to specialists\n\n\n\n\nGreedy\nDiscarded whenever aggregate score drops\n\n\nPareto\nPreserved if they solve anything unique\n\n\n\nGreedy selection caused our iteration 3 collapse‚Äîthe number-theory prompt overwrote the algebra prompt‚Äôs insights. Pareto selection prevents this by construction: you can‚Äôt remove a prompt from the frontier unless something else does everything it does, plus more.\nThis drives GEPA‚Äôs sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier retains them automatically.\n\nPareto selection preserved iteration 3‚Äôs prompt despite its 10% aggregate score‚Äîbecause it solved instance #9, which nothing else could. The intuition for why keeping ‚Äúlosers‚Äù helps comes from quality-diversity algorithms in evolutionary computation.\n\n\n\nWhy Pareto Works: Quality-Diversity and Map Elites\nWe‚Äôve now built both core mechanisms from scratch‚Äîreflective mutation and Pareto selection. Before diving into the full algorithm, let‚Äôs step back and understand why this approach works so well.\nThe Pareto frontier isn‚Äôt a novel invention‚Äîit draws from quality-diversity (QD) algorithms, a family of techniques from evolutionary computation that work well when diversity itself is valuable.\nTraditional optimization asks: ‚ÄúWhat‚Äôs the single best solution?‚Äù\nQuality-diversity asks: ‚ÄúWhat‚Äôs the best solution of each type?‚Äù\nComplex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensively‚Äîbut the best engine knows both styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one ‚Äúbest‚Äù approach.\n\nMap Elites\nMap Elites (Mouret & Clune, 2015) maintains an archive organized by behavior:\n\nDefine behavior dimensions ‚Äî characteristics describing how a solution works (not just how well)\nDiscretize into bins ‚Äî each cell represents a ‚Äúniche‚Äù\nKeep the best per bin ‚Äî new solutions compete only within their niche\nMutate from the archive ‚Äî sample from any occupied bin, mutate, place in appropriate bin\n\nThe result: diverse specialists, each optimal of its type. The archive provides stepping stones: a mutation from one niche might discover something useful for another. Diversity doubles as a search strategy.\n\n\nGEPA‚Äôs Adaptation: Validation Instances as Niches\nGEPA recognizes that the validation set itself defines the behavior space:\n\n\n\n\n\n\n\nMap Elites\nGEPA\n\n\n\n\nBehavior = continuous dimensions\nBehavior = which validation instances are solved\n\n\nBins = discretized regions\n‚ÄúBins‚Äù = individual validation instances\n\n\nArchive = best per bin\nPareto frontier = non-dominated prompts across instances\n\n\n\nEach validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the ‚Äúspecialist for that niche‚Äù‚Äîit stays because it demonstrates something works, even with low aggregate score.\n\n‚ÄúBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.‚Äù ‚Äî Lakshya A Agrawal\n\nThis motivated GEPA‚Äôs design: Pareto selection over per-instance scores naturally implements QD‚Äôs ‚Äúbest per niche‚Äù principle, while the merge operation recombines insights across niches‚Äîexactly what stepping-stone search requires.\nWith both mechanisms in place, there‚Äôs one more operation that makes GEPA powerful: merge‚Äîcombining insights from divergent lineages.\n\n\n\nThe Lineage Tree and System-Aware Merge\nPareto selection preserves specialists‚Äîbut it creates a new problem: insights get siloed in separate branches.\nConsider what happens after 10 iterations of GEPA on AIME problems:\n\n\n\nLineage Tree with Merge Operation\n\n\nThe P2‚ÜíP4 lineage accumulated algebra insights. The P3‚ÜíP5 lineage accumulated number theory insights. Both survive on the Pareto frontier because each solves problems the other can‚Äôt.\nBut what about a problem requiring both?\n\n\nThe Recombination Problem\nSuppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:\n\nP4 knows to subtract equations pairwise, but doesn‚Äôt think to reduce mod p\nP5 knows CRT, but misses the algebraic setup\n\nWith mutation alone, P4 would need to independently rediscover number theory (which P5 already knows), or vice versa. The knowledge exists in our population‚Äîjust in different branches.\nMerge addresses this by combining insights from divergent lineages into a single candidate.\n\n\n\nHow Merge Works\nGEPA‚Äôs merge is ‚Äúsystem-aware‚Äù‚Äîthe LLM understands what it‚Äôs combining, so it can resolve contradictions and synthesize coherently rather than blindly concatenating (unlike genetic algorithm crossover which swaps segments randomly). The reflection LLM receives:\n\nBoth parent prompts with their full instruction text\nLineage context ‚Äî what types of problems each lineage solved\nConflict guidance ‚Äî instructions to resolve contradictions, not ignore them\n\nThe prompt asks the LLM to synthesize, not concatenate:\n\n‚ÄúCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don‚Äôt simply concatenate‚Äîsynthesize into coherent guidance.‚Äù\n\nConcrete example ‚Äî merging our AIME specialists:\n\n\n\n\n\n\n\n\nParent\nSpecialty\nKey instruction\n\n\n\n\nP4\nAlgebra\n‚ÄúSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.‚Äù\n\n\nP5\nNumber theory\n‚ÄúApply CRT for modular constraints. Check for contradictions.‚Äù\n\n\n\nMerged offspring P6:\n\n‚ÄúApproach: (1) For equation systems, subtract pairwise to expose factor relationships‚Äîenumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.‚Äù\n\nP6 inherits both toolkits and adds meta-knowledge about when to combine them. This is the ‚Äúsystem-aware‚Äù part‚Äîthe merge understands the semantics of what it‚Äôs combining.\n\n\n\nWhen to Merge vs.¬†Mutate\nGEPA alternates between operations based on frontier state:\n\n\n\n\n\n\n\n\nCondition\nOperation\nRationale\n\n\n\n\nEarly iterations (&lt; 5)\nMutate\nLet lineages diverge first; nothing to merge yet\n\n\nFrontier has one dominant lineage\nMutate\nNo orthogonal insights to combine\n\n\nFrontier has divergent specialists\nMerge\nRecombine discoveries from parallel explorations\n\n\nRecent merge succeeded\nMutate\nRefine the merged candidate\n\n\n\nThe paper describes the decision:\n\n‚ÄúAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nMerge vs.¬†Genetic Algorithm Crossover\nGEPA‚Äôs merge is analogous to crossover in genetic algorithms, but smarter:\n\n\n\n\n\n\n\nGenetic Algorithms\nGEPA Merge\n\n\n\n\nGenes = bit positions\nInsights = natural language instructions\n\n\nCrossover = swap bit segments randomly\nMerge = LLM synthesizes with understanding\n\n\nCan create invalid offspring\nCan resolve contradictions\n\n\nBlind to semantics\nAware of what instructions mean\n\n\n\nRandom crossover might produce: ‚ÄúSubtract equations pairwise. Apply CRT. Subtract equations pairwise.‚Äù (nonsense duplication)\nLLM merge produces: ‚ÄúSet up algebra first, then apply number-theoretic constraints.‚Äù (coherent synthesis)\n\n\n\nHow the Pieces Fit\nPareto selection preserves the diversity that makes merge valuable in the first place. Lineage tracking tells us which candidates come from divergent branches. Merge recombines their discoveries, and then Pareto selection preserves successful merges alongside the remaining specialists.\nIf you only kept one ‚Äúbest‚Äù prompt, there‚Äôd be nothing interesting to merge. And without merge, insights stay siloed even when the frontier is diverse.\n\nImplementation note: The full GEPA implementation includes safeguards to ensure merge candidates actually have different insights worth combining (checking for common ancestors, avoiding redundant merges, verifying that descendants improved on their ancestor). See the DSPy source for details.\n\nMutation refines a single lineage through reflection. Merge recombines what parallel lineages discovered. Pareto selection preserves both.\nWith reflective mutation, Pareto selection, and merge all in place, here‚Äôs how they combine into GEPA‚Äôs full optimization loop.\n\n\n\nThe Complete Algorithm\nHere‚Äôs how the pieces combine into the full optimization loop.\n\nAlgorithm Overview\nimport random\nimport numpy as np\nfrom typing import Callable\n\ndef gepa(\n    base_prompt: str,\n    trainset: list,\n    valset: list,\n    evaluate_fn: Callable,          # (prompt, example) -&gt; score (0 or 1)\n    run_with_feedback_fn: Callable, # (prompt, examples) -&gt; (traces: list[str], feedback: list[str])\n    reflect_fn: Callable,           # (parent_prompt, traces, feedback) -&gt; new_prompt: str\n    merge_fn: Callable,             # (prompt1, prompt2) -&gt; merged_prompt: str\n    max_iterations: int = 20,\n    minibatch_size: int = 3,\n):\n    \"\"\"\n    GEPA: Genetic-Pareto prompt optimization.\n    \n    Returns the best aggregate prompt; access full frontier via returned dict.\n    \"\"\"\n    \n    # --- Helper functions ---\n    \n    def evaluate_all(prompt, dataset):\n        \"\"\"Return per-instance scores as list.\"\"\"\n        return [evaluate_fn(prompt, ex) for ex in dataset]\n    \n    def evaluate_minibatch(prompt, minibatch):\n        \"\"\"Return aggregate score on minibatch.\"\"\"\n        return sum(evaluate_fn(prompt, ex) for ex in minibatch) / len(minibatch)\n    \n    def dominates(scores_a, scores_b):\n        \"\"\"Does A dominate B? (&gt;= everywhere, &gt; somewhere)\"\"\"\n        a, b = np.array(scores_a), np.array(scores_b)\n        return (a &gt;= b).all() and (a &gt; b).any()\n    \n    def is_dominated_by_frontier(new_scores, frontier, scores):\n        \"\"\"Is new_scores dominated by ANY frontier member?\"\"\"\n        return any(dominates(scores[c], new_scores) for c in frontier)\n    \n    def sample_from_frontier(frontier, scores):\n        \"\"\"Sample weighted by unique wins.\"\"\"\n        n_instances = len(next(iter(scores.values())))\n        weights = []\n        for candidate in frontier:\n            if len(frontier) == 1:\n                unique_wins = n_instances\n            else:\n                others_max = np.array([scores[o] for o in frontier if o != candidate]).max(axis=0)\n                unique_wins = (np.array(scores[candidate]) &gt; others_max).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        return random.choices(frontier, weights=weights)[0]\n    \n    def get_root(prompt, lineage):\n        \"\"\"Trace lineage back to root.\"\"\"\n        while lineage.get(prompt) is not None:\n            prompt = lineage[prompt]\n        return prompt\n    \n    def should_merge(frontier, lineage, iteration):\n        \"\"\"Decide whether to merge or mutate.\"\"\"\n        if len(frontier) &lt; 2 or iteration &lt; 5:\n            return False\n        n_lineages = len(set(get_root(c, lineage) for c in frontier))\n        return random.random() &lt; (n_lineages - 1) / len(frontier)\n    \n    def select_divergent(frontier, parent, lineage):\n        \"\"\"Select a candidate from a different lineage.\"\"\"\n        parent_root = get_root(parent, lineage)\n        others = [c for c in frontier if get_root(c, lineage) != parent_root]\n        return random.choice(others) if others else random.choice(frontier)\n    \n    def best_aggregate(frontier, scores):\n        \"\"\"Return prompt with highest aggregate score.\"\"\"\n        return max(frontier, key=lambda c: sum(scores[c]))\n    \n    # --- Main loop ---\n    \n    candidates = [base_prompt]\n    scores = {base_prompt: evaluate_all(base_prompt, valset)}\n    pareto_frontier = [base_prompt]\n    lineage = {base_prompt: None}\n    \n    for iteration in range(1, max_iterations + 1):\n        \n        # 1. SAMPLE: Select parent from Pareto frontier\n        parent = sample_from_frontier(pareto_frontier, scores)\n        \n        # 2. PROPOSE: Either mutate or merge\n        minibatch = random.sample(trainset, min(minibatch_size, len(trainset)))\n        \n        if should_merge(pareto_frontier, lineage, iteration):\n            other_parent = select_divergent(pareto_frontier, parent, lineage)\n            new_prompt = merge_fn(parent, other_parent)\n        else:\n            traces, feedback = run_with_feedback_fn(parent, minibatch)\n            new_prompt = reflect_fn(parent, traces, feedback)\n        \n        # 3. EVALUATE: Mini-batch gate\n        parent_mb_score = evaluate_minibatch(parent, minibatch)\n        new_mb_score = evaluate_minibatch(new_prompt, minibatch)\n        if new_mb_score &lt;= parent_mb_score:\n            continue  # Reject: didn't improve on mini-batch\n        \n        # Full evaluation\n        new_scores = evaluate_all(new_prompt, valset)\n        \n        # 4. UPDATE: Pareto frontier maintenance\n        if is_dominated_by_frontier(new_scores, pareto_frontier, scores):\n            continue  # Reject: dominated by existing candidate\n        \n        # Remove dominated candidates\n        pareto_frontier = [c for c in pareto_frontier \n                          if not dominates(new_scores, scores[c])]\n        \n        # Add new candidate\n        candidates.append(new_prompt)\n        scores[new_prompt] = new_scores\n        pareto_frontier.append(new_prompt)\n        lineage[new_prompt] = parent\n    \n    return {\n        'best': best_aggregate(pareto_frontier, scores),\n        'frontier': pareto_frontier,\n        'scores': scores,\n        'lineage': lineage,\n    }\n\n\n\nThe Key Decision Points\n1. Candidate Sampling ‚Äî Weighted by unique wins, so specialists get attention proportional to their unique value.\n2. Mutation vs Merge ‚Äî Early iterations favor mutation; merge probability increases as frontier diversifies.\n3. Mini-Batch Gating\nBefore expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:\n\n‚ÄúWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.‚Äù ‚Äî Lakshya A Agrawal\n\n4. Pareto Update\nThe frontier update follows the dominance logic we implemented earlier:\n\nReject new candidates dominated by existing ones (they add nothing)\nRemove existing candidates dominated by the new one (they‚Äôre obsolete)\nKeep all non-dominated candidates (each offers unique value)\n\n\n\n\nComplexity Analysis\n\n\n\n\n\n\n\nOperation\nCost\n\n\n\n\nMutation (3-4 rollouts + reflection)\n3-4 LLM calls + 1 reflection call\n\n\nMini-batch evaluation\n3-4 metric evaluations\n\n\nFull validation evaluation\nN metric evaluations (N = valset size)\n\n\nPareto check\nO(F √ó N) comparisons (F = frontier size)\n\n\n\nThe mini-batch gate matters because most mutations fail‚Äîthey either don‚Äôt improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.\n\n\n\nWhy Each Component Matters\n\n\n\n\n\n\n\n\nComponent\nWithout it\nWith it\n\n\n\n\nTextual feedback\nOptimizer sees only score=0.6\nOptimizer reads ‚Äúwrong answer, expected X, got Y because‚Ä¶‚Äù\n\n\nPareto selection\nSpecialists discarded when aggregate drops\nSpecialists preserved if they solve anything unique\n\n\nLineage tracking\nNo memory of evolutionary history\nCan identify divergent branches for merge\n\n\nMerge operation\nInsights stay siloed in separate branches\nOrthogonal discoveries can combine\n\n\nMini-batch gating\nEvaluate every candidate fully\nReject obvious failures cheaply\n\n\n\nThese interact: Pareto selection preserves the diversity that makes merge valuable, textual feedback makes both mutation and merge more effective, and mini-batch gating keeps evaluation costs reasonable.\n\n\n\nWhat Gets Returned\nThe algorithm returns best_aggregate(pareto_frontier)‚Äîthe prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use track_stats=True to access all candidates and their per-instance scores.\n\nThat‚Äôs the full loop. Now let‚Äôs see what the optimized prompts actually look like.\n\n\n\nWhat GEPA Learns: Domain-Specific Knowledge Encoding\nWe‚Äôve built the full algorithm. What does all this machinery actually produce? The output is prompts that encode domain expertise.\nGEPA can encode domain-specific knowledge directly into prompts‚Äîturning what experts know implicitly into explicit instructions that persist across examples.\n\nPrompts as Knowledge Containers\nTraditional optimization treats prompts as opaque strings to be scored. GEPA treats them as knowledge containers that accumulate insights through the reflection loop:\n\n\n\nFailure Accumulation Experience\n\n\nEach iteration doesn‚Äôt just fix one error‚Äîit extracts the lesson behind the error.\nConcrete example from our AIME experiments:\n\n\n\n\n\n\n\nStage\nPrompt excerpt\n\n\n\n\nSeed\n‚ÄúYou are given a problem and you have to give the answer along with reasoning.‚Äù\n\n\nAfter iter 2\n‚Äú‚Ä¶For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.‚Äù\n\n\nAfter iter 3\n‚Äú‚Ä¶When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.‚Äù\n\n\n\nThe prompt evolved from generic instruction to encoding competition math heuristics. The LLM didn‚Äôt invent these‚Äîit extracted them from its training knowledge, triggered by seeing specific failure modes.\n\n\nThree Categories of Captured Knowledge\nWe observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):\n1. Format and Interface Knowledge - Output schemas (‚Äúreturn JSON with keys: answer, reasoning‚Äù) - API conventions (‚Äúuse library.method(), not library_method()‚Äù) - Parsing requirements (‚Äúintegers only, no leading zeros‚Äù)\nThis is easiest to extract since format errors produce explicit feedback.\n2. Strategic Knowledge - Problem-solving heuristics (‚Äútry small cases first‚Äù) - Domain patterns (‚Äúcompetition problems often combine algebra and number theory‚Äù) - Failure mode awareness (‚Äúwatch for off-by-one errors in counting‚Äù)\nThis emerges from reflecting on why approaches failed, not just that they failed.\n3. Factual Domain Knowledge - API names and signatures (‚Äútorch.einsum, not torch.einstein_sum‚Äù) - Domain constants (‚Äústandard gravity = 9.81 m/s¬≤‚Äù) - Constraint relationships (‚Äúin valid Sudoku, each row/column/box contains 1-9 exactly once‚Äù)\nThe LLM already knows this‚Äîreflection surfaces it into the prompt where it‚Äôs consistently applied.\n\n\nWhy Prompts Beat Weights (Sometimes)\nFine-tuning encodes knowledge in model weights‚Äîopaque, distributed, hard to inspect. GEPA encodes knowledge in natural language‚Äîreadable and editable.\n\n\n\nAspect\nFine-tuning\nGEPA prompts\n\n\n\n\nInspectability\nBlack box\nHuman-readable instructions\n\n\nEditability\nRequires retraining\nEdit the text directly\n\n\nComposability\nTrain new model\nMerge prompt sections\n\n\nSample efficiency\nThousands of examples\nTens of examples\n\n\n\nA GEPA-optimized prompt can be read by a human to understand what strategies it learned. You can edit it to add domain knowledge the optimizer missed, or transfer it to different LLMs.\n\n\nThe Preservation Problem\nBut new insights can overwrite old ones. We saw this in our hands-on experiment when iteration 3‚Äôs number theory insights overwrote iteration 2‚Äôs algebra insights, causing catastrophic forgetting.\nPareto selection prevents this. By preserving prompts that are best on any validation instance, it keeps specialized knowledge around even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier, and the merge operation can later combine their insights.\nWithout this, GEPA would periodically erase its own discoveries.\n\n\nLimitation: Knowledge Must Be Triggerable\nGEPA can only surface knowledge the base LLM already has. It‚Äôs extraction, not creation. Stronger base models yield better results because there‚Äôs more latent knowledge to work with. For specialized domains (custom hardware APIs, proprietary protocols), you may need human-written seed instructions or few-shot examples to get the reflection loop started.\n\nSo far we‚Äôve focused on training: optimize prompts on labeled examples, deploy the best one. But GEPA‚Äôs machinery‚Äîreflective mutation, Pareto selection, merge‚Äîcan also run at inference time as a search algorithm."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "href": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "title": "GEPA Deepdive",
    "section": "Beyond Training: GEPA for Inference-Time Search",
    "text": "Beyond Training: GEPA for Inference-Time Search\nEverything we‚Äôve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that flips this on its head.\n\nTwo Paradigms of Operation\nTrain-then-generalize (what we‚Äôve built so far):\n\nOptimize prompts on a training set\nSelect the best-aggregate prompt from the Pareto frontier\nDeploy that prompt on new, unseen tasks\nGoal: learn generalizable lessons that transfer\n\nTest-time search (inference-time optimization):\n\nYou have a batch of hard tasks you need to solve now\nOptimize directly on the tasks themselves\nGEPA searches for solutions, storing the best prompt per task\nGoal: maximize performance on these specific instances\n\nThe mechanics are identical‚Äîreflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you‚Äôre using GEPA as a search algorithm over the solution space. This is an instance of inference-time compute scaling‚Äîinvesting more computation at inference to solve harder problems.\nThe key mechanical change: In normal GEPA, you have separate trainset (to learn from via reflection) and valset (to evaluate generalization). The Pareto frontier tracks per-instance performance on valset, preserving prompts that generalize well.\nFor test-time search, pass the same problems as both trainset and valset:\n# Test-time search: optimize directly on the tasks you want to solve\noptimized = GEPA(metric=metric_with_feedback, auto=\"medium\").compile(\n    program,\n    trainset=hard_problems,  # The actual tasks you need solved\n    valset=hard_problems,    # Same held-out validation\n)\nThis tells GEPA: ‚ÄúI don‚Äôt care about generalization‚Äîoptimize directly on these specific problems.‚Äù The Pareto frontier now tracks ‚Äúbest prompt for each problem‚Äù rather than ‚Äúprompts that transfer to unseen data.‚Äù See the GEPA API documentation for full parameter details.\n\n‚ÄúGiven a batch of tasks that we want to solve and given some budget‚Ä¶ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nWhy GEPA Beats High-Temperature Sampling\nTraditional inference-time strategies sample at high temperature to generate many candidates, then pick the best. But these samples tend to be similar‚Äîvariations on the same approach. GEPA induces genuine diversity through Pareto tracking (maintaining candidates that excel at something different) and reflective mutation (proposing structurally different strategies based on what went wrong, not random perturbations).\nWhen feedback says ‚Äúmemory bandwidth bottleneck,‚Äù the next candidate might switch from a naive loop to shared-memory tiling‚Äîa qualitative change that temperature variation rarely discovers. On the MATH benchmark, this approach achieves 93% accuracy compared to 67% with basic DSPy ChainOfThought.\n\n\n\nSelf-Bootstrapping at inference\nDuring training, you iterate a fixed number of times and deploy the result. At inference time, you can keep iterating on a single hard problem until it‚Äôs solved‚Äîand GEPA‚Äôs reflective loop creates a self-bootstrapping dynamic:\n\nRound 1: Generate rollout ‚Üí compiler error (‚Äúundefined variable x‚Äù) ‚Üí reflect ‚Üí propose fix\nRound 2: Code compiles ‚Üí runtime error (division by zero) ‚Üí reflect ‚Üí propose fix\n\nRound 3: Runtime works ‚Üí wrong output (‚Äúexpected 42, got 41‚Äù) ‚Üí reflect ‚Üí propose fix\nRound 4: Correct output ‚úì\n\nEach iteration surfaces the next failure mode‚Äîyou can‚Äôt discover the runtime error until the compile error is fixed. Traditional sampling generates 100 candidates that all hit the same compiler error. GEPA‚Äôs iterative reflection progresses through the failure cascade.\n\n‚ÄúIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it‚Äôs kind of like self-bootstrapping data to train itself.‚Äù ‚Äî Lakshya A Agrawal\n\nThis is why test-time GEPA can solve problems that stumped training: it has the budget to chase failure modes deeper than any fixed training run.\n\nNote: When optimizing a batch of tasks, Pareto selection ensures that fixing one problem doesn‚Äôt discard prompts that solved others‚Äîsee ‚ÄúCross-Task Transfer Within a Batch‚Äù below.\n\n\n\n\nCross-Task Transfer Within a Batch\nWhen solving related tasks (e.g., a batch of CUDA kernels), insights compound across the batch:\n\n‚ÄúAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.‚Äù ‚Äî Lakshya A Agrawal\n\nThe frontier maintains multiple specialized prompts simultaneously:\n\n\n\nPrompt\nSpecialization\nProblems solved\n\n\n\n\nP_conv\nConvolutional operators\n#1, #4, #7\n\n\nP_reduce\nReduction/summation operators\n#2, #5, #8\n\n\nP_matmul\nMatrix multiplication\n#3, #6\n\n\n\n\n‚ÄúOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.‚Äù ‚Äî Lakshya A Agrawal\n\nWhen a new problem arrives, GEPA tries candidates from across the frontier‚Äîthe convolution specialist might crack it, or the reduction specialist, or insights from both might merge.\n\n\n\nCross-Task Insight Transfer\n\n\n\n\n\nBackground Optimization Loops\nThe self-bootstrapping pattern suggests a natural application: background GEPA loops for personalization in tools like Cursor and other AI-assisted environments.\n\n‚ÄúFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt‚Ä¶ cursor can learn a user-specific prompt that works well specifically for you.‚Äù ‚Äî Lakshya A Agrawal\n\nEvery correction you provide‚Äîrejecting a verbose explanation, fixing a code style‚Äîbecomes a training signal that accumulates into a personalized prompt. The infrastructure isn‚Äôt widespread yet, but the pattern points toward continuous adaptation rather than one-shot optimization.\n\n\n\nWhat GEPA Stores\nFor each task in the batch, GEPA tracks both artifacts:\n\n‚ÄúGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.‚Äù ‚Äî Lakshya A Agrawal\n\n\nBest outputs ‚Äî the actual solutions, ready to use\nBest prompts ‚Äî specialized strategies representing different subdomains of your problem space\n\nYou can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.\n\nWhen the conditions are right‚Äîrich feedback, high-value tasks worth the compute, domains where the LLM has strong priors‚Äîtest-time GEPA can beat sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.\n\n\nCase Study: Code Optimization for Novel Hardware\nThe GEPA paper demonstrates test-time search on code optimization for hardware with limited pre-training data‚Äîa domain well-suited to GEPA‚Äôs reflective approach.\n\nAMD NPU Kernels: Optimization Without Pre-Training Knowledge\nAMD‚Äôs NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The NPUEval benchmark shows how difficult this is: even with compiler feedback and RAG, state-of-the-art LLMs achieve only ~10% mean vectorization score.\nGEPA‚Äôs approach doesn‚Äôt require prior examples‚Äîit iteratively generates kernels, receives compiler errors or performance metrics, reflects on feedback, and proposes targeted improvements. The compiler error messages contain the fix: ‚ÄúSymbol not found: npu_matmul‚Äù triggers reflection that surfaces the correct API.\n\n‚ÄúGEPA can be used to generate optimized kernels for AMD‚Äôs NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\nCUDA Kernels: Outperforming Human Baselines\nKernelBench (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels. The benchmark sets a low baseline: frontier reasoning models match the PyTorch baseline on less than 20% of tasks using the fast‚ÇÅ metric (correct and faster than PyTorch). Efficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven‚Äôt learned to apply reliably.\nThe KernelBench paper shows that feedback-driven refinement improves results substantially‚Äîfast‚ÇÅ scores jumped 3-6x when execution results and profiler feedback were provided in context. GEPA applies this same principle systematically, with Pareto tracking to preserve diverse optimization strategies rather than ad-hoc iteration.\n\n‚ÄúFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.‚Äù ‚Äî Lakshya A Agrawal\n\nThe self-bootstrapping dynamic is especially effective here: each compilation error or profiler bottleneck reveals the next fix, letting GEPA progress through failure cascades that stump one-shot sampling.\n\n\nCross-Kernel Transfer\nWhen optimizing a batch of related kernels, insights compound across tasks:\n\n‚ÄúIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.‚Äù ‚Äî Lakshya A Agrawal\n\nAs GEPA optimizes each kernel, the Pareto frontier accumulates specialized prompts‚Äîone excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax. This cross-task transfer is why batch optimization outperforms solving each kernel independently."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "href": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "title": "GEPA Deepdive",
    "section": "Conclusion: When to Reach for GEPA",
    "text": "Conclusion: When to Reach for GEPA\nWe opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculus‚Äîmatching RL‚Äôs optimization performance at a fraction of the sample cost by doing something RL can‚Äôt: reading the feedback.\nOn AIME math problems: 46.6% ‚Üí 56.6%. On MATH benchmark: 67% ‚Üí 93%. CUDA kernels that outperform human-written PyTorch baselines. All through prompt optimization alone, no fine-tuning required.\nAs LLMs have gotten better at self-reflection, this kind of optimization has become practical. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix.\n\n\nUse GEPA for Train-Then-Generalize When:\n\nYou have rich textual feedback (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)\nYour evaluation budget is limited (50-500 examples, not 5,000)\nYou‚Äôre optimizing compound AI systems where prompts orchestrate multi-step pipelines\nYou need interpretable results‚Äîprompts you can read, edit, and reason about\n\n\n\nUse GEPA for Test-Time Search When:\n\nYou have a batch of high-value tasks worth the compute investment\nEach task produces execution feedback (tests, profilers, validators)\nTasks are related enough for cross-task transfer to help\n\n\n\nStick with Traditional Approaches When:\n\nYou have abundant labeled data and compute budget for fine-tuning\nFeedback is purely scalar with no explanatory signal\nThe task is already solved by few-shot prompting\nYou need sub-second latency\n\n\n\n\nGet Started\nReady to try GEPA on your own pipelines?\n\nGEPA for AIME Tutorial ‚Äî Complete walkthrough from setup to optimized results\nGEPA API Reference ‚Äî Full parameter documentation\nPaper ‚Äî Algorithm details and experimental methodology\n\nThe core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads it‚Äîand learns."
  }
]