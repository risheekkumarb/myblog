[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html",
    "href": "posts/gepa-deepdive/gepa_final_article.html",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "This article was made possible by the research papers referenced throughout, the Weaviate discussion with Lakshya A Agrawal, and guidance from Kerem Turgutlu at answer.ai. Created using the solveit platform by fast.ai.\nYou‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.\nBut what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) achieves exactly this‚Äîby exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "href": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "title": "GEPA Deepdive",
    "section": "How GEPA Works: Building It From Scratch",
    "text": "How GEPA Works: Building It From Scratch\nGEPA combines two key innovations: reflective prompt mutation (learning from textual feedback) and Pareto selection (preserving diverse specialists). Each is powerful alone; together they compound.\nWe‚Äôll build them from scratch in this section:\n\nReflective mutation ‚Äî How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.\nPareto selection ‚Äî Why always improving your ‚Äúbest‚Äù prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.\nMerge ‚Äî How GEPA combines insights from divergent lineages (covered after the complete algorithm).\n\nBy the end, you‚Äôll see how these mechanisms combine into GEPA‚Äôs full evolutionary loop.\n\n\nüöÄ Quick Start: Using GEPA in 30 Seconds\n\nüìñ Full tutorial: GEPA for AIME (Math) ‚Äî optimizing GPT-4.1 Mini from 46.6% ‚Üí 56.6% on AIME 2025.\n\nBefore diving deep, here‚Äôs what using GEPA looks like in practice:\nStep 1: Configure your language model\nimport dspy\n\nlm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, max_tokens=32000)\ndspy.configure(lm=lm)\nStep 2: Define your program\nprogram = dspy.ChainOfThought(\"problem -&gt; answer\")\nStep 3: Define a metric that returns feedback (not just a score)\nThis is the key difference from other optimizers‚Äîyour metric explains why something failed:\ndef metric_with_feedback(example, prediction, trace=None, **kwargs):\n    correct_answer = example.answer\n    pred_answer = prediction.answer\n    \n    score = int(correct_answer == pred_answer)\n    \n    if score == 1:\n        feedback = f\"Correct! The answer is '{correct_answer}'.\"\n    else:\n        feedback = f\"Incorrect. Expected '{correct_answer}', got '{pred_answer}'.\"\n        # Add any additional context that could help improvement:\n        if hasattr(example, 'solution'):\n            feedback += f\" Solution: {example.solution}\"\n    \n    return dspy.Prediction(score=score, feedback=feedback)\nStep 4: Optimize with GEPA\nfrom dspy import GEPA\n\noptimizer = GEPA(\n    metric=metric_with_feedback,\n    auto=\"light\",           # Budget preset: \"light\", \"medium\", or \"heavy\"\n    num_threads=32,         # Parallel evaluation threads\n)\n\noptimized_program = optimizer.compile(\n    program,\n    trainset=train_set,\n    valset=val_set,\n)\nThat‚Äôs it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.\n\nWhat‚Äôs happening under the hood?\n\n\n\n\n\n\n\nComponent\nWhat it does\n\n\n\n\nTextual feedback\nYour metric returns why something failed, not just a score\n\n\nReflective mutation\nAn LLM reads the feedback and proposes improved instructions\n\n\nPareto selection\nDiverse specialists are preserved, not just the ‚Äúbest‚Äù prompt\n\n\nMerge operations\nInsights from divergent lineages get combined\n\n\n\nThe rest of this section explains why each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you need‚Äîsee the DSPy GEPA API Reference for full parameter details.\nNow let‚Äôs make this concrete by building the core mechanism from scratch.\n\n\nHands-On: Building Reflective Mutation from Scratch\nLet‚Äôs make this concrete by implementing GEPA‚Äôs core mechanism on a real task.\nThe Problem: AIME Math Competition\nWe‚Äôll optimize prompts for solving AIME (American Invitational Mathematics Examination) problems ‚Äî challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.\nfrom datasets import load_dataset\ndset = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n# 90 problems with solutions and integer answers\nWhy AIME for testing prompt optimization?\n\nClear ground truth ‚Äî Every answer is an integer (0-999), so evaluation is unambiguous\nRich failure modes ‚Äî Wrong answers come from algebraic errors, missed cases, misread constraints\nDomain knowledge helps ‚Äî Prompts that encode strategies (‚Äúsubtract equations pairwise‚Äù, ‚Äúenumerate all cases‚Äù) measurably improve performance\nSmall dataset ‚Äî Only 90 problems, so sample efficiency matters\n\nThe Setup\n# Split: 10 train, 10 validation (simulating scarce labeled data)\ntdf = df.sample(45).iloc[:10]  # Training mini-batches drawn from here\nvdf = df.drop(tdf.index).iloc[:10]  # Held-out validation\n\n# Base model: Gemini 2.5 Flash via LiteLLM\n# Metric: Exact match (predicted integer == ground truth)\ndef metric(ground_truth, prediction):\n    return int(ground_truth) == prediction['answer']\nSeed Prompt\nWe start with a minimal instruction:\nseed_prompt = \"\"\"You are given a problem and you have to give the answer \nalong with reasoning. Do not return anything apart from json. \nIt should be parsable by json.loads()\"\"\"\nBaseline validation accuracy: 10% (1/10 correct)\nCan reflective mutation improve this? Let‚Äôs find out.\n\n\n\nStep 1: The Feedback Function\nFirst, we need a function that tells the reflection LLM what went wrong. We‚Äôll start with minimal feedback‚Äîjust the correct answer:\ndef feedback(ground_truth, prediction):\n    if int(ground_truth) != prediction['answer']:\n        return f'You got it wrong! The solution is {ground_truth}'\n    return 'You got it right!'\nThis is deliberately simple. Later we‚Äôll discuss how richer feedback (like expert solutions) can improve results.\n\n\n\nStep 2: The Reflection Prompt\nFollowing GEPA‚Äôs structure, we build a prompt that shows the LLM its failures:\nREFLECTION_TEMPLATE = \"\"\"I provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\n{current_prompt}\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n{examples}\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\"\"\"\n\ndef mk_reflection_prompt(df, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\n\n\n\nStep 3: The Complete Optimization Loop\ndef reflect(mb, curr_prompt):\n    \"\"\"Ask LLM to reflect on failures and propose improved instruction.\"\"\"\n    refl_prompt = mk_reflection_prompt(mb, curr_prompt)\n    return _call(refl_prompt, format=ReflectionModel)['new_instruction']\n\ndef optimize_prompt(seed_prompt, traindf, valdf, n_iters=3, mb_size=3):\n    \"\"\"Greedy reflective prompt optimization.\"\"\"\n    prompts, train_scores, val_scores = [seed_prompt], [], []\n    mb = traindf.sample(mb_size)  # Fixed minibatch for this run\n    \n    print(f'Baseline validation: {eval_val(seed_prompt, valdf):.2%}')\n    \n    for i in range(n_iters):\n        # Evaluate current prompt on minibatch\n        mb_eval, mb_score = eval_mb(prompts[-1], mb)\n        print(f\"üìä Minibatch: {mb_score:.2%}\")\n        \n        # Reflect and propose new instruction\n        new_instr = reflect(mb_eval, prompts[-1])\n        new_prompt = new_instr  # The new instruction becomes the new prompt\n        \n        # Evaluate on validation set\n        val_score = eval_val(new_prompt, valdf)\n        print(f\"üìä Validation: {val_score:.2%}\")\n        \n        prompts.append(new_prompt)\n        val_scores.append(val_score)\n    \n    return dict(prompts=prompts, val_scores=val_scores)\n\n\n\nWhat Actually Happened\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\n\n\n\n\n\n\nIteration\nMinibatch\nValidation\nWhat the reflection learned\n\n\n\n\nBaseline\n‚Äî\n10%\n‚Äî\n\n\n1\n0%\n10%\nJSON formatting details, output structure rules\n\n\n2\n0%\n30%\nSystems of equations strategy, remainder/modular arithmetic tips\n\n\n3\n67%\n10%\nOver-specialized on number theory, solved #9 but lost generality\n\n\n\nThe good: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:\n\n‚ÄúWhen dealing with systems of equations like \\(xy + Az = C\\), \\(yz + Ax = C\\), \\(zx + Ay = C\\), consider subtracting equations pairwise to find relationships between variables, such as \\((x-z)(y-A)=0\\), which implies \\(x=z\\) or \\(y=A\\). Systematically explore all such cases.‚Äù\n\nThis is directly from the actual output‚Äîthe reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.\nThe bad: Iteration 3 achieved 67% on its minibatch but dropped to 10% validation. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:\n\n‚ÄúWhen the problem involves number theory and remainders (e.g., \\(n \\pmod x\\), \\(n \\pmod y\\)), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.‚Äù\n\nThis over-specialized advice (‚Äúcan lead to an answer of 0‚Äù) actively hurt performance on non-number-theory problems, dropping validation from 30% back to 10% (1/10)‚Äîthough notably, it did solve problem #9, which earlier prompts couldn‚Äôt.\n\n\n\nThe Greedy Selection Problem\nThis demonstrates exactly why GEPA uses Pareto selection instead of always taking the ‚Äúbest‚Äù prompt:\n\nIteration 2‚Äôs prompt was a specialist‚Äîit learned something valuable about systems of equations\nIteration 3 tried to improve on iteration 2, but the minibatch had different problems\nThe reflection overwrote the systems-of-equations insight while adding number-theory tips that were too specific\nResult: catastrophic forgetting\n\nWith greedy selection, we would have discarded iteration 2‚Äôs valuable insight. Pareto selection would keep it‚Äîbecause it was best on at least one validation instance.\n\n\n\nThe Missing Ingredient: Rich Feedback\nOur minimal feedback (\"You got it wrong! The solution is 349\") only tells the model that it failed, not why or how to fix it.\nThe AIME dataset includes expert solutions. A richer feedback function could use them:\ndef feedback_rich(row):\n    if int(row.answer) != row.pred['answer']:\n        sol = row.solution[:500] + \"...\" if len(row.solution) &gt; 500 else row.solution\n        return f\"\"\"Wrong! Expected {row.answer}, got {row.pred['answer']}.\n        \nModel's reasoning: {row.pred['short_reasoning']}\n\nExpert solution approach:\n{sol}\"\"\"\n    return \"Correct!\"\nWith rich feedback, the reflection LLM can extract specific strategies from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: the feedback contains the fix.\nCompare this to RL, which would only see reward = 0 and have to statistically infer what went wrong across thousands of trajectories.\n\n\n\nKey Takeaways\n\nReflective mutation works ‚Äî Even with minimal feedback, the LLM extracted useful domain knowledge\nGreedy selection fails ‚Äî Iteration 3‚Äôs collapse shows why we need to preserve specialist insights\nFeedback quality matters ‚Äî Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with\nSample efficiency is real ‚Äî We saw meaningful optimization with just 3 iterations on 3 examples each\n\nThis is the core limitation of greedy optimization: catastrophic forgetting. The solution? Pareto selection‚Äîwhich we‚Äôll build from scratch next.\n\n\nHands-On: Building the Pareto Frontier\nIn the reflective mutation section, we saw greedy selection fail‚Äîiteration 3‚Äôs over-specialized prompt dropped validation from 30% to 10%, losing iteration 2‚Äôs valuable systems-of-equations insights. The fundamental problem: always improving your ‚Äúbest‚Äù prompt discards specialist knowledge.\nGEPA‚Äôs solution: Pareto selection. Instead of keeping one best prompt, maintain a frontier of prompts where each excels at something no other prompt beats.\n\n\nWhat is Pareto Dominance?\nA prompt dominates another if it‚Äôs at least as good everywhere, and strictly better somewhere:\n\n‚â• on every validation instance, AND\n\n&gt; on at least one instance\n\nIf prompt A dominates prompt B, we can safely discard B‚ÄîA is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.\nExample: Consider four prompts evaluated on 10 validation instances. We‚Äôll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:\n\n\n\nPrompt\nInstances Solved\nAggregate\nStatus\n\n\n\n\nP0 (seed)\n#0 only\n10%\nDominated by P2\n\n\nP1 (iter 1)\n#0 only\n10%\nDominated by P2\n\n\nP2 (iter 2)\n#0, #1, #2\n30%\nFrontier ‚úì\n\n\nP3 (iter 3)\n#9 only\n10%\nFrontier ‚úì\n\n\n\nP2 dominates both P0 and P1‚Äîit solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which nothing else could.\nThe Pareto frontier is {P2, P3}. Both contain unique value.\n\n\n\nImplementation: Dominance Checking\nWe represent per-instance scores as boolean arrays (1 = solved, 0 = failed):\nimport numpy as np\n\ndef dominates(candidate_scores, other_scores):\n    \"\"\"Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)\"\"\"\n    candidate = np.array(candidate_scores)\n    other = np.array(other_scores)\n    return (candidate &gt;= other).all() and (candidate &gt; other).any()\n\ndef is_dominated_by_any(new_scores, frontier_scores):\n    \"\"\"Is new_scores dominated by ANY prompt in the frontier?\"\"\"\n    new = np.array(new_scores)\n    for existing in frontier_scores:\n        if dominates(np.array(existing), new):\n            return True\n    return False\n\ndef get_dominated_indices(new_scores, frontier_scores):\n    \"\"\"Which frontier prompts does new_scores dominate?\"\"\"\n    new = np.array(new_scores)\n    return [i for i, existing in enumerate(frontier_scores) \n            if dominates(new, np.array(existing))]\n\n\n\nTracing Through: Why P3 Survives\nLet‚Äôs verify the dominance relationships from our example:\nP0 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP1 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP2 = [1,1,1,0,0,0,0,0,0,0]  # Solves: #0, #1, #2\nP3 = [0,0,0,0,0,0,0,0,0,1]  # Solves: #9\n\n# Does P2 dominate P0?\ndominates(P2, P0)  # True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2\n\n# Does P2 dominate P1?\ndominates(P2, P1)  # True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2\n\n# Does P2 dominate P3?\ndominates(P2, P3)  # False! P2 loses on #9 (0 &lt; 1)\n\n# Does P3 dominate P2?\ndominates(P3, P2)  # False! P3 loses on #0, #1, #2\nNeither P2 nor P3 dominates the other‚Äîthey‚Äôre Pareto incomparable. Each solves problems the other can‚Äôt. Both stay on the frontier.\n\n\n\nPareto Frontier Explained\n\n\n\n\n\nThe Complete Frontier Manager\nclass ParetoFrontier:\n    def __init__(self):\n        self.prompts = []\n        self.scores = []  # scores[i][j] = prompt i's score on instance j\n    \n    def add(self, prompt, instance_scores):\n        \"\"\"Try to add a prompt. Returns True if it joins the frontier.\"\"\"\n        # Reject if dominated by existing frontier member\n        if is_dominated_by_any(instance_scores, self.scores):\n            return False\n        \n        # Remove any frontier members this prompt dominates\n        dominated = get_dominated_indices(instance_scores, self.scores)\n        for i in sorted(dominated, reverse=True):  # Remove from end first\n            del self.prompts[i]\n            del self.scores[i]\n        \n        # Add to frontier\n        self.prompts.append(prompt)\n        self.scores.append(instance_scores)\n        return True\n    \n    def sample(self):\n        \"\"\"Sample a prompt, weighted by unique wins.\"\"\"\n        weights = []\n        scores_arr = np.array(self.scores)\n        for i in range(len(self.prompts)):\n            # How many instances is this prompt *uniquely* best on?\n            others_best = np.delete(scores_arr, i, axis=0).max(axis=0) if len(self.prompts) &gt; 1 else np.zeros_like(scores_arr[i])\n            unique_wins = (scores_arr[i] &gt; others_best).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        \n        return np.random.choice(self.prompts, p=np.array(weights)/sum(weights))\n    \n    def best_aggregate(self):\n        \"\"\"Return the prompt with highest aggregate score.\"\"\"\n        aggregates = [sum(s) for s in self.scores]\n        return self.prompts[np.argmax(aggregates)]\n\n\n\nPutting It Together: Pareto-Guided Optimization\ndef optimize_with_pareto(seed_prompt, traindf, valdf, n_iters=5, mb_size=3):\n    \"\"\"Reflective mutation with Pareto frontier selection.\"\"\"\n    frontier = ParetoFrontier()\n    \n    # Initialize with seed\n    seed_scores = evaluate_per_instance(seed_prompt, valdf)\n    frontier.add(seed_prompt, seed_scores)\n    print(f'Baseline: {sum(seed_scores)/len(seed_scores):.1%}')\n    \n    for i in range(n_iters):\n        print(f\"\\n{'='*40}\\nIteration {i+1}\\n{'='*40}\")\n        \n        # Sample parent from frontier (weighted by unique wins)\n        parent = frontier.sample()\n        \n        # Run on minibatch, reflect, propose mutation\n        mb = traindf.sample(mb_size)\n        mb_results = evaluate_with_traces(parent, mb)\n        new_prompt = reflect_and_mutate(parent, mb_results)\n        \n        # Evaluate on full validation set\n        new_scores = evaluate_per_instance(new_prompt, valdf)\n        new_agg = sum(new_scores) / len(new_scores)\n        print(f\"New prompt: {new_agg:.1%} aggregate\")\n        \n        # Try to add to frontier\n        if frontier.add(new_prompt, new_scores):\n            print(f\"‚úì Added to frontier (size: {len(frontier.prompts)})\")\n        else:\n            print(f\"‚úó Dominated, rejected\")\n    \n    return frontier.best_aggregate()\n\n\n\nWhat We Observed on AIME\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\nIteration\nAggregate\nInstances Solved\nFrontier Action\n\n\n\n\nSeed\n10%\n#0\nInitialize\n\n\n1\n10%\n#0\nDominated by seed, rejected\n\n\n2\n30%\n#0, #1, #2\nAdded, dominates seed\n\n\n3\n10%\n#9 only\nAdded ‚úì (unique win on #9)\n\n\n\nThe key moment: Iteration 3 scored only 10%‚Äîworse than iteration 2‚Äôs 30%. Greedy selection would discard it entirely.\nBut it solved instance #9, which nothing else could. Pareto selection preserves it.\nOur final frontier: {P2, P3} - P2: Strong generalist (30%), knows systems-of-equations strategies - P3: Instance-9 specialist (10%), knows whatever cracked that specific problem\nBoth insights survive. The merge operation (covered later) can combine them.\n\n\n\nWhy This Matters: No More Catastrophic Forgetting\n\n\n\nSelection Strategy\nWhat happens to specialists\n\n\n\n\nGreedy\nDiscarded whenever aggregate score drops\n\n\nPareto\nPreserved if they solve anything unique\n\n\n\nGreedy selection caused our iteration 3 collapse‚Äîthe number-theory prompt overwrote the algebra prompt‚Äôs insights. Pareto selection prevents this by construction: you can‚Äôt remove a prompt from the frontier unless something else does everything it does, plus more.\nThis is the core of GEPA‚Äôs sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier never loses them in the first place.\n\nWe just saw Pareto selection preserve iteration 3‚Äôs prompt despite its 10% aggregate score‚Äîbecause it solved instance #9, which nothing else could. But this raises a question: why does keeping ‚Äúlosers‚Äù help optimization? Shouldn‚Äôt we focus resources on the best candidates?\nThe answer comes from quality-diversity algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.\n\n\n\nWhy Pareto Works: Quality-Diversity and Map Elites\nWe‚Äôve now built both core mechanisms from scratch‚Äîreflective mutation and Pareto selection. Before diving into the full algorithm, let‚Äôs briefly step back and understand why this approach works so well.\nThe Pareto frontier isn‚Äôt a novel invention‚Äîit draws from quality-diversity (QD) algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.\n\nThe Core Insight\nTraditional optimization asks: ‚ÄúWhat‚Äôs the single best solution?‚Äù\nQuality-diversity asks: ‚ÄúWhat‚Äôs the best solution of each type?‚Äù\nComplex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensively‚Äîbut the best engine knows both styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one ‚Äúbest‚Äù approach.\n\n\nMap Elites: The Inspiration\nMap Elites (Mouret & Clune, 2015) maintains an archive organized by behavior:\n\nDefine behavior dimensions ‚Äî characteristics describing how a solution works (not just how well)\nDiscretize into bins ‚Äî each cell represents a ‚Äúniche‚Äù\nKeep the best per bin ‚Äî new solutions compete only within their niche\nMutate from the archive ‚Äî sample from any occupied bin, mutate, place in appropriate bin\n\nThe result: diverse specialists, each optimal of its type. Key insight: the archive provides stepping stones‚Äîa mutation from one niche might discover something useful for another. Diversity isn‚Äôt just nice to have; it‚Äôs a search strategy.\n\n\nGEPA‚Äôs Adaptation: Validation Instances as Niches\nGEPA recognizes that the validation set itself defines the behavior space:\n\n\n\n\n\n\n\nMap Elites\nGEPA\n\n\n\n\nBehavior = continuous dimensions\nBehavior = which validation instances are solved\n\n\nBins = discretized regions\n‚ÄúBins‚Äù = individual validation instances\n\n\nArchive = best per bin\nPareto frontier = non-dominated prompts across instances\n\n\n\nEach validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the ‚Äúspecialist for that niche‚Äù‚Äîit stays because it demonstrates something works, even with low aggregate score.\nConcrete example: In our AIME experiment, instance #9 was a number theory problem requiring CRT. Prompt P3 (10% aggregate) solved it; P2 (30% aggregate) couldn‚Äôt. In Map Elites terms, P3 is the ‚Äúelite‚Äù for the number-theory niche‚Äîit stays in the archive despite low overall score.\n\n‚ÄúBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.‚Äù ‚Äî Lakshya A Agrawal\n\nThis directly motivated GEPA‚Äôs design: Pareto selection over per-instance scores naturally implements QD‚Äôs ‚Äúbest per niche‚Äù principle, while the merge operation recombines insights across niches‚Äîexactly what stepping-stone search requires.\n\n\nExploration-Exploitation Balance\nGEPA‚Äôs Pareto selection handles the exploration-exploitation tradeoff automatically: candidates are sampled weighted by unique wins, so specialists get attention proportional to their unique value. The frontier self-prunes‚Äînoise gets dominated away, genuine diversity persists. No manual tuning required.\nWith both reflective mutation and Pareto selection in place, there‚Äôs one more operation that makes GEPA powerful: merge‚Äîcombining insights from divergent lineages. Let‚Äôs look at the complete algorithm.\n\n\n\nThe Complete Algorithm\nNow that we‚Äôve built reflective mutation and Pareto selection from scratch, and understand why quality-diversity works, let‚Äôs see how all the pieces‚Äîincluding lineage tracking and merge‚Äîcombine into GEPA‚Äôs full optimization loop.\n\nAlgorithm Overview\nGEPA(base_prompt, trainset, valset, max_iterations):\n    \n    # Initialize\n    candidates = [base_prompt]\n    scores = {base_prompt: evaluate_all(base_prompt, valset)}\n    pareto_frontier = [base_prompt]\n    lineage = {base_prompt: None}  # parent tracking\n    \n    for iteration in 1..max_iterations:\n        \n        # 1. SAMPLE: Select candidate from Pareto frontier\n        #    Weighted by number of instances where candidate is best\n        parent = sample_from_frontier(pareto_frontier, scores)\n        \n        # 2. PROPOSE: Either mutate or merge\n        if should_merge(pareto_frontier, iteration):\n            # Select second parent from different lineage\n            other_parent = select_divergent_candidate(pareto_frontier, parent, lineage)\n            new_prompt = merge(parent, other_parent)\n        else:\n            # Run rollouts, collect feedback, reflect\n            minibatch = sample(trainset, k=3)\n            traces, feedback = run_with_feedback(parent, minibatch)\n            # See \"Hands-On: Building Reflective Mutation\" for the reflection prompt structure\n            new_prompt = reflect_and_mutate(parent, traces, feedback)\n        \n        # 3. EVALUATE: Mini-batch gate, then full evaluation\n        parent_mb_score = evaluate(parent, minibatch)\n        new_mb_score = evaluate(new_prompt, minibatch)\n        if new_mb_score &lt;= parent_mb_score:\n            continue  # Reject: didn't improve on mini-batch\n        \n        new_scores = evaluate_all(new_prompt, valset)\n        \n        # 4. UPDATE: Pareto frontier maintenance\n        if is_dominated(new_scores, pareto_frontier):\n            continue  # Reject: dominated by existing candidate\n        \n        # Remove any candidates the new one dominates\n        pareto_frontier = [c for c in pareto_frontier \n                          if not dominates(new_scores, scores[c])]\n        \n        # Add new candidate\n        candidates.append(new_prompt)\n        scores[new_prompt] = new_scores\n        pareto_frontier.append(new_prompt)\n        lineage[new_prompt] = parent\n    \n    # Return best aggregate for deployment; full frontier available via track_stats\n    return best_aggregate(pareto_frontier, scores)\n\n\n\nThe Key Decision Points\n1. Candidate Sampling (Exploration vs Exploitation)\nRather than always improving the single best prompt (greedy), GEPA samples from the entire Pareto frontier. The sampling weight is proportional to how many validation instances the candidate is uniquely best on:\ndef sample_from_frontier(frontier, scores):\n    weights = []\n    for candidate in frontier:\n        # Count instances where this candidate beats all others\n        unique_wins = sum(1 for i in range(n_instances) \n                        if scores[candidate][i] &gt; max(scores[other][i] \n                                                      for other in frontier if other != candidate))\n        weights.append(unique_wins + 1)  # +1 smoothing\n    return random.choices(frontier, weights=weights)[0]\nThis focuses effort on candidates that have demonstrated unique value‚Äîthey solve something nothing else can. Specialists get attention proportional to their specialization.\n2. Mutation vs Merge Decision\nEarly iterations favor mutation (exploring from single candidates). As the frontier diversifies, merge becomes more valuable. The following is a simplified illustration of the decision logic:\ndef should_merge(frontier, iteration):\n    if len(frontier) &lt; 2: return False\n    if iteration &lt; 5: return False  # Let lineages diverge first\n    # Merge with probability proportional to frontier diversity\n    n_lineages = len(set(get_root(c) for c in frontier))\n    return random.random() &lt; (n_lineages - 1) / len(frontier)\n(The actual GEPA implementation may use different thresholds‚Äîsee the paper for details.)\n3. Mini-Batch Gating\nBefore expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:\n\n‚ÄúWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.‚Äù ‚Äî Lakshya A Agrawal\n\n4. Pareto Update\nThe frontier update follows the dominance logic we implemented earlier: - Reject new candidates dominated by existing ones (they add nothing) - Remove existing candidates dominated by the new one (they‚Äôre obsolete) - Keep all non-dominated candidates (each offers unique value)\n\n\n\nComplexity Analysis\n\n\n\n\n\n\n\nOperation\nCost\n\n\n\n\nMutation (3-4 rollouts + reflection)\n3-4 LLM calls + 1 reflection call\n\n\nMini-batch evaluation\n3-4 metric evaluations\n\n\nFull validation evaluation\nN metric evaluations (N = valset size)\n\n\nPareto check\nO(F √ó N) comparisons (F = frontier size)\n\n\n\nThe mini-batch gate is crucial for efficiency. Most mutations fail‚Äîthey either don‚Äôt improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.\n\n\n\nWhy Each Component Matters\n\n\n\n\n\n\n\n\nComponent\nWithout it\nWith it\n\n\n\n\nTextual feedback\nOptimizer sees only score=0.6\nOptimizer reads ‚Äúwrong answer, expected X, got Y because‚Ä¶‚Äù\n\n\nPareto selection\nSpecialists discarded when aggregate drops\nSpecialists preserved if they solve anything unique\n\n\nLineage tracking\nNo memory of evolutionary history\nCan identify divergent branches for merge\n\n\nMerge operation\nInsights stay siloed in separate branches\nOrthogonal discoveries can combine\n\n\nMini-batch gating\nEvaluate every candidate fully\nReject obvious failures cheaply\n\n\n\nThe components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.\n\n\n\nWhat Gets Returned\nThe algorithm returns best_aggregate(pareto_frontier)‚Äîthe prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use track_stats=True to access all candidates and their per-instance scores.\n\nNext: The merge operation in detail‚Äîhow GEPA combines insights from divergent lineages.\n\n\n\nThe Lineage Tree and System-Aware Merge\nWe touched on merge briefly in the algorithm overview‚Äînow let‚Äôs see why it‚Äôs essential and how it actually works.\nWhat ‚Äúsystem-aware‚Äù means: Unlike genetic algorithm crossover (which blindly swaps segments), GEPA‚Äôs merge uses an LLM that understands what it‚Äôs combining‚Äîit can resolve contradictions, synthesize complementary strategies, and produce coherent instructions rather than jumbled concatenations.\nPareto selection preserves specialists‚Äîbut it creates a new problem: insights get siloed in separate branches.\nConsider what happens after 10 iterations of GEPA on AIME problems:\n\n\n\nLineage Tree with Merge Operation\n\n\nThe P2‚ÜíP4 lineage accumulated algebra insights. The P3‚ÜíP5 lineage accumulated number theory insights. Both survive on the Pareto frontier because each solves problems the other can‚Äôt.\nBut what about a problem requiring both?\n\n\nThe Recombination Problem\nSuppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:\n\nP4 knows to subtract equations pairwise, but doesn‚Äôt think to reduce mod p\nP5 knows CRT, but misses the algebraic setup\n\nWith mutation alone, P4 would need to independently rediscover number theory (which P5 already knows), or vice versa. The knowledge exists in our population‚Äîjust in different branches.\nMerge solves this by combining insights from divergent lineages into a single candidate.\n\n\n\nHow Merge Works\nGEPA‚Äôs merge is ‚Äúsystem-aware‚Äù‚Äîit understands what it‚Äôs combining, not just concatenating strings. The reflection LLM receives:\n\nBoth parent prompts with their full instruction text\nLineage context ‚Äî what types of problems each lineage solved\nConflict guidance ‚Äî instructions to resolve contradictions, not ignore them\n\nThe prompt asks the LLM to synthesize, not concatenate:\n\n‚ÄúCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don‚Äôt simply concatenate‚Äîsynthesize into coherent guidance.‚Äù\n\nConcrete example ‚Äî merging our AIME specialists:\n\n\n\n\n\n\n\n\nParent\nSpecialty\nKey instruction\n\n\n\n\nP4\nAlgebra\n‚ÄúSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.‚Äù\n\n\nP5\nNumber theory\n‚ÄúApply CRT for modular constraints. Check for contradictions.‚Äù\n\n\n\nMerged offspring P6: &gt; ‚ÄúApproach: (1) For equation systems, subtract pairwise to expose factor relationships‚Äîenumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.‚Äù\nP6 inherits both toolkits AND adds meta-knowledge about when to combine them. This is the ‚Äúsystem-aware‚Äù part‚Äîthe merge understands the semantics of what it‚Äôs combining.\n\n\n\nWhen to Merge vs.¬†Mutate\nGEPA alternates between operations based on frontier state:\n\n\n\n\n\n\n\n\nCondition\nOperation\nRationale\n\n\n\n\nEarly iterations (&lt; 5)\nMutate\nLet lineages diverge first; nothing to merge yet\n\n\nFrontier has one dominant lineage\nMutate\nNo orthogonal insights to combine\n\n\nFrontier has divergent specialists\nMerge\nRecombine discoveries from parallel explorations\n\n\nRecent merge succeeded\nMutate\nRefine the merged candidate\n\n\n\nThe paper describes the decision:\n\n‚ÄúAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nMerge vs.¬†Genetic Algorithm Crossover\nGEPA‚Äôs merge is analogous to crossover in genetic algorithms, but smarter:\n\n\n\n\n\n\n\nGenetic Algorithms\nGEPA Merge\n\n\n\n\nGenes = bit positions\nInsights = natural language instructions\n\n\nCrossover = swap bit segments randomly\nMerge = LLM synthesizes with understanding\n\n\nCan create invalid offspring\nCan resolve contradictions\n\n\nBlind to semantics\nAware of what instructions mean\n\n\n\nRandom crossover might produce: ‚ÄúSubtract equations pairwise. Apply CRT. Subtract equations pairwise.‚Äù (nonsense duplication)\nLLM merge produces: ‚ÄúSet up algebra first, then apply number-theoretic constraints.‚Äù (coherent synthesis)\n\n\n\nThe Compounding Effect\nThe components reinforce each other:\n\nPareto selection preserves the diversity that makes merge valuable\nLineage tracking identifies which candidates come from divergent branches\n\nMerge recombines orthogonal discoveries into unified candidates\nPareto selection then preserves successful merges alongside remaining specialists\n\nWithout Pareto selection, there‚Äôs nothing interesting to merge‚Äîyou‚Äôd just have variants of one ‚Äúbest‚Äù prompt. Without merge, insights stay siloed even when the frontier is diverse.\nMutation explores depth‚Äîrefining one approach through successive reflections.\nMerge explores breadth‚Äîcombining orthogonal discoveries from parallel paths.\nTogether, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.\n\n\n\nWhat GEPA Learns: Domain-Specific Knowledge Encoding\nWe‚Äôve built the full algorithm‚Äîreflective mutation, Pareto selection, merge. But what does all this machinery actually produce? Let‚Äôs examine the output: prompts that encode domain expertise.\nOne of GEPA‚Äôs most striking capabilities is its ability to encode domain-specific knowledge directly into prompts‚Äîtransforming tacit expertise into explicit instructions that persist across examples.\n\nPrompts as Knowledge Containers\nTraditional optimization treats prompts as opaque strings to be scored. GEPA treats them as knowledge containers that accumulate insights through the reflection loop:\n\n\n\nFailure Accumulation Experience\n\n\nEach iteration doesn‚Äôt just fix one error‚Äîit extracts the lesson behind the error.\nConcrete example from our AIME experiments:\n\n\n\n\n\n\n\nStage\nPrompt excerpt\n\n\n\n\nSeed\n‚ÄúYou are given a problem and you have to give the answer along with reasoning.‚Äù\n\n\nAfter iter 2\n‚Äú‚Ä¶For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.‚Äù\n\n\nAfter iter 3\n‚Äú‚Ä¶When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.‚Äù\n\n\n\nThe prompt evolved from generic instruction to encoding competition math heuristics. The LLM didn‚Äôt invent these‚Äîit extracted them from its training knowledge, triggered by seeing specific failure modes.\n\n\nThree Categories of Captured Knowledge\nWe observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):\n1. Format and Interface Knowledge - Output schemas (‚Äúreturn JSON with keys: answer, reasoning‚Äù) - API conventions (‚Äúuse library.method(), not library_method()‚Äù) - Parsing requirements (‚Äúintegers only, no leading zeros‚Äù)\nThis is easiest to extract‚Äîformat errors produce explicit feedback.\n2. Strategic Knowledge - Problem-solving heuristics (‚Äútry small cases first‚Äù) - Domain patterns (‚Äúcompetition problems often combine algebra and number theory‚Äù) - Failure mode awareness (‚Äúwatch for off-by-one errors in counting‚Äù)\nThis emerges from reflecting on why approaches failed, not just that they failed.\n3. Factual Domain Knowledge - API names and signatures (‚Äútorch.einsum, not torch.einstein_sum‚Äù) - Domain constants (‚Äústandard gravity = 9.81 m/s¬≤‚Äù) - Constraint relationships (‚Äúin valid Sudoku, each row/column/box contains 1-9 exactly once‚Äù)\nThe LLM already knows this‚Äîreflection surfaces it into the prompt where it‚Äôs consistently applied.\n\n\nWhy Prompts Beat Weights (Sometimes)\nFine-tuning encodes knowledge in model weights‚Äîopaque, distributed, hard to inspect. GEPA encodes knowledge in natural language‚Äîreadable, editable, composable.\n\n\n\nAspect\nFine-tuning\nGEPA prompts\n\n\n\n\nInspectability\nBlack box\nHuman-readable instructions\n\n\nEditability\nRequires retraining\nEdit the text directly\n\n\nComposability\nTrain new model\nMerge prompt sections\n\n\nSample efficiency\nThousands of examples\nTens of examples\n\n\n\nA GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.\n\n\nThe Preservation Problem\nKnowledge accumulation has a failure mode: new insights can overwrite old ones. We saw this in our hands-on experiment‚Äîiteration 3‚Äôs number theory insights overwrote iteration 2‚Äôs algebra insights, causing catastrophic forgetting.\nThis is precisely why GEPA uses Pareto selection. By preserving prompts that are best on any validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier‚Äîand the merge operation can later combine their insights.\n\nWithout Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.\n\n\n\nLimitation: Knowledge Must Be Triggerable\nGEPA can only surface knowledge the base LLM already has. If the model doesn‚Äôt know that ‚ÄúCRT‚Äù means Chinese Remainder Theorem, no amount of reflection will discover it. The optimization extracts and organizes existing knowledge‚Äîit doesn‚Äôt create new knowledge.\nThis is why GEPA works better with stronger base models: more latent knowledge to extract. And why, for domains requiring knowledge the LLM lacks, you‚Äôll need to inject it through few-shot examples, retrieval augmentation, or explicit instructions.\n\n‚ÄúLanguage models already have built a lot of prior knowledge‚Ä¶ we can use all of this natural language information to make the LLM itself reflect on its mistakes and improve.‚Äù ‚Äî Lakshya A Agrawal\n\n\nNow that we understand the algorithm and what it produces, let‚Äôs look at how to use GEPA in practice."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "href": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "title": "GEPA Deepdive",
    "section": "Beyond Training: GEPA for Inference-Time Search",
    "text": "Beyond Training: GEPA for Inference-Time Search\nEverything we‚Äôve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.\n\nTwo Paradigms of Operation\nTrain-then-generalize (what we‚Äôve built so far): - Optimize prompts on a training set - Select the best-aggregate prompt from the Pareto frontier - Deploy that prompt on new, unseen tasks - Goal: learn generalizable lessons that transfer\nTest-time search (inference-time optimization): - You have a batch of hard tasks you need to solve now - Optimize directly on the tasks themselves - GEPA searches for solutions, storing the best prompt per task - Goal: maximize performance on these specific instances\nThe mechanics are identical‚Äîreflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you‚Äôre using GEPA as a search algorithm over the solution space. This aligns with the broader trend of inference-time compute scaling‚Äîinvesting more computation at inference to solve harder problems.\nThe key mechanical change: pass valset=trainset (or equivalently, omit valset and set trainset to your target tasks). This tells GEPA to optimize directly on the problems you‚Äôre trying to solve rather than holding out a separate validation set. See the GEPA API documentation for full parameter details.\n# Test-time search: optimize directly on the tasks you want to solve\noptimized = GEPA(metric=metric_with_feedback, auto=\"medium\").compile(\n    program,\n    trainset=hard_problems,  # The actual tasks you need solved\n    valset=hard_problems,    # Same set‚Äîno held-out validation\n)\n\n‚ÄúGiven a batch of tasks that we want to solve and given some budget‚Ä¶ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nWhy GEPA Beats High-Temperature Sampling\nTraditional inference-time compute strategies generate many candidates by sampling at high temperature, then use a verifier or LLM-as-judge to pick the best one. But these samples tend to be similar‚Äîvariations on the same approach.\nRecent research on test-time compute scaling (Snell et al., 2024) shows that ‚Äúcompute-optimal‚Äù strategies‚Äîwhich adaptively allocate inference budget based on problem difficulty‚Äîcan improve efficiency by more than 4x compared to traditional best-of-N sampling. In some cases, smaller models with optimized test-time compute outperform models 14x larger that don‚Äôt use additional inference-time computation.\nThe intuition: High-temperature sampling produces variations around the same approach‚Äîdifferent variable names, slightly different loop bounds, minor syntactic choices. GEPA‚Äôs reflective mutation proposes structurally different strategies based on what went wrong. When the feedback says ‚Äúmemory bandwidth bottleneck,‚Äù the next candidate might switch from a naive loop to shared-memory tiling‚Äîa qualitative change that temperature variation rarely discovers.\nThe Pareto frontier then preserves these diverse strategies rather than converging on a single ‚Äúbest‚Äù approach.\nGEPA induces genuine diversity through two mechanisms:\n\nPareto tracking ‚Äî maintains candidates that each excel at something different, not just the single highest scorer\nReflective mutation ‚Äî proposes structurally different approaches based on what went wrong, not random perturbations\n\n\n‚ÄúDue to the way GEPA operates where it‚Äôs doing the Pareto candidate tracking and it‚Äôs doing reflective mutation, we find that GEPA is itself inducing a huge amount of diversity in the kinds of solutions that it generates.‚Äù ‚Äî Lakshya A Agrawal\n\nRather than 100 slight variations of one approach, GEPA maintains a frontier of genuinely different strategies.\nConcrete results: On the MATH benchmark, GEPA achieves 93% accuracy compared to 67% with basic DSPy ChainOfThought‚Äîa 26 percentage point improvement from prompt optimization alone, no fine-tuning required.\n\n\n\nSelf-Bootstrapping: How Iteration Compounds\nGEPA‚Äôs iterative process creates a self-bootstrapping dynamic:\n\nRound 1: Generate rollouts ‚Üí get feedback (e.g., compiler error) ‚Üí reflect ‚Üí propose fix\nRound 2: The code compiles, but now there‚Äôs a runtime error (division by zero) ‚Üí reflect ‚Üí propose fix\nRound 3: Runtime works, but output is wrong ‚Üí reflect ‚Üí propose fix\n\n\n‚ÄúIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it‚Äôs kind of like self-bootstrapping data to train itself.‚Äù ‚Äî Lakshya A Agrawal\n\nEach iteration surfaces a new failure mode and proposes a solution‚Äîgenerating increasingly challenging training signal from the task itself.\n\n\n\nCross-Task Transfer Within a Batch\nWhen solving related tasks (e.g., a batch of CUDA kernels), insights compound across the batch:\n\n‚ÄúAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.‚Äù ‚Äî Lakshya A Agrawal\n\nThe frontier maintains multiple specialized prompts simultaneously:\n\n\n\nPrompt\nSpecialization\nProblems solved\n\n\n\n\nP_conv\nConvolutional operators\n#1, #4, #7\n\n\nP_reduce\nReduction/summation operators\n#2, #5, #8\n\n\nP_matmul\nMatrix multiplication\n#3, #6\n\n\n\n\n‚ÄúOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.‚Äù ‚Äî Lakshya A Agrawal\n\nWhen a new problem arrives, GEPA tries candidates from across the frontier‚Äîthe convolution specialist might crack it, or the reduction specialist, or insights from both might merge.\n\n\n\nWhat GEPA Stores\nFor each task in the batch, GEPA tracks both artifacts:\n\n‚ÄúGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.‚Äù ‚Äî Lakshya A Agrawal\n\n\nBest outputs ‚Äî the actual solutions, ready to use\nBest prompts ‚Äî specialized strategies representing different subdomains of your problem space\n\nYou can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.\n\n\n\nFeedback Design for Each Paradigm\nThe paradigm choice shapes how you design feedback:\nFor train-then-generalize ‚Äî extract transferable lessons:\n\n‚ÄúYou will ensure that there is no task-specific insights that the prompt captures. So you will write in your feedback something along the lines of ‚Äòtry to extract lessons out of this.‚Äô‚Äù ‚Äî Lakshya A Agrawal\n\ndef metric_generalizable(gold, pred, trace=None):\n    score = int(gold.answer == pred.answer)\n    if score == 0:\n        feedback = (\n            f\"Failed: expected {gold.answer}, got {pred.answer}. \"\n            f\"Extract a GENERAL lesson that would help on similar problems.\"\n        )\n    else:\n        feedback = \"Correct. What general strategy led to success?\"\n    return {\"score\": score, \"feedback\": feedback}\nFor test-time search ‚Äî hyper-specialize to the task:\n\n‚ÄúIf you‚Äôre doing simply an inference time search where you just care about the final outputs, then you will try to provide feedback which is as hyper-specialized to the task as possible.‚Äù ‚Äî Lakshya A Agrawal\n\ndef metric_specialized(gold, pred, trace=None):\n    score = int(gold.answer == pred.answer)\n    if score == 0:\n        feedback = (\n            f\"Your code failed with this particular compiler error: {pred.error}. \"\n            f\"Try to improve on this specific thing.\"\n        )\n    else:\n        feedback = \"Correct.\"\n    return {\"score\": score, \"feedback\": feedback}\n\n\n\nThe Tutor Analogy: Background Optimization Loops\nThis iterative pattern mirrors how students already work with AI tutoring systems:\n\nAttempt problem ‚Üí get solution\nSee an error ‚Üí receive feedback\nTutor explains ‚Üí improved understanding\nRepeat until mastered\n\nThis suggests an intriguing application: background GEPA loops for personalization. Similar patterns are emerging in tools like Cursor and other AI-assisted development environments.\n\n‚ÄúFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt‚Ä¶ cursor can learn a user-specific prompt that works well specifically for you.‚Äù ‚Äî Lakshya A Agrawal\n\nEvery time you provide feedback on an AI-generated solution, a background loop could update not just the current response but a user-specific prompt capturing your preferences and patterns.\n\n\n\nWhen NOT to Use Test-Time GEPA\n\nSparse feedback ‚Äî If your metric only returns pass/fail with no explanation, GEPA can‚Äôt extract lessons\nTasks outside LLM knowledge ‚Äî GEPA surfaces knowledge the model already has; genuinely novel information won‚Äôt emerge from reflection. For such domains, consider retrieval-augmented generation to inject external knowledge\nTight latency constraints ‚Äî Test-time GEPA typically runs 5-20 iterations, each involving LLM calls for rollout + reflection. Budget 30 seconds to 5 minutes per task depending on model latency and iteration count. Interactive use cases need a different approach.\nIdentical tasks ‚Äî If your batch contains identical problems (not just related ones), optimize once and reuse. Cross-task transfer helps when problems are related but distinct‚Äîlike different CUDA kernels that share optimization patterns but have different structures.\n\n\nWhen the conditions are right‚Äîrich feedback, high-value tasks worth the compute, domains where the LLM has strong priors‚Äîtest-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.\n\n\nCase Study: Code Optimization for Novel Hardware\nThe GEPA paper demonstrates test-time search on domains where GEPA‚Äôs strengths shine brightest: code optimization for hardware with limited pre-training data.\n\nAMD NPU Kernels: Optimization Without Pre-Training Knowledge\nAMD‚Äôs NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The NPUEval benchmark reveals just how challenging this is: even with compiler feedback and retrieval-augmented generation (RAG), state-of-the-art LLMs achieve only ~10% mean vectorization score across the dataset.\nGEPA succeeds here because it doesn‚Äôt need prior examples. Instead, it:\n\nGenerates an initial kernel based on generic programming knowledge\nCompiles and runs ‚Üí receives compiler errors or performance metrics\nReflects on feedback ‚Üí proposes targeted improvements\nIterates until the kernel compiles, runs correctly, and performs well\n\nWhy GEPA fits: The GEPA paper demonstrates results on NPUEval, showing that reflective mutation can extract optimization patterns even for hardware with minimal pre-training coverage‚Äîwithout requiring retraining or RAG.\n\n‚ÄúGEPA can be used to generate optimized kernels for AMD‚Äôs NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.‚Äù ‚Äî Lakshya A Agrawal\n\nThe compiler error messages contain the fix. ‚ÄúSymbol not found: npu_matmul‚Äù triggers reflection that surfaces the correct API. ‚ÄúMemory alignment error at line 47‚Äù points directly to what needs fixing.\n\n\nCUDA Kernels: Outperforming Human Baselines\nKernelBench (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels‚Äîa task where even frontier models struggle. The benchmark reveals a sobering baseline: frontier reasoning models match the PyTorch baseline on less than 20% of tasks using the fast‚ÇÅ metric (kernels that are both correct and faster than PyTorch). This isn‚Äôt a matter of prompting‚Äîefficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven‚Äôt learned to apply reliably.\nThe impact of iterative feedback: The KernelBench paper demonstrates that feedback-driven refinement dramatically improves results. By providing execution results and profiler feedback in context, fast‚ÇÅ scores improved from 12%, 36%, and 12% to 43%, 72%, and 18% respectively across their test configurations. This is exactly the mechanism GEPA exploits‚Äîbut with systematic Pareto tracking and reflective mutation rather than ad-hoc iteration.\n\n‚ÄúFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.‚Äù ‚Äî Lakshya A Agrawal\n\nRelated evolutionary approaches show further gains: Stanford‚Äôs separate test-time evolutionary search (distinct from GEPA, published in their Fast Kernels blog post) produces kernels achieving 103-133% of PyTorch reference performance on foundational operators like Conv2D‚Äîdemonstrating that structured search with execution feedback can exceed human-optimized baselines.\n\n\n\n\n\n\n\n\nMethod\nPerformance\nSource\n\n\n\n\nBase LLM (one-shot)\n~3-15% fast‚ÇÅ\nKernelBench paper\n\n\nFrontier reasoning (o1, R1)\nMatch baseline on &lt;20% of tasks\nKernelBench paper\n\n\nWith execution + profiler feedback\n43-72% fast‚ÇÅ (3-6x improvement)\nKernelBench paper\n\n\nEvolutionary test-time search\n103-133% of PyTorch on select kernels\nStanford CRFM (separate from GEPA)\n\n\nGEPA\n‚ÄúSignificant speedups over PyTorch-eager‚Äù\nGEPA paper Fig. 11\n\n\n\nThe gap matters commercially: efficient compilers often lag behind new GPU architectures by over two years‚Äîapproximately one year for CUDA experts to develop optimized implementations and another year to generalize into compilers. GEPA-style optimization could bridge that gap by generating optimized kernels for new hardware before traditional toolchains catch up.\nWhy code optimization is ideal for GEPA:\n\n\n\n\n\n\n\nProperty\nWhy it helps\n\n\n\n\nRich textual feedback\nCompiler errors, profiler output, runtime exceptions all explain why something failed\n\n\nVerifiable correctness\nUnit tests and benchmarks provide unambiguous signal\n\n\nIterative refinement\nEach failed compilation reveals the next fix to try\n\n\nCross-task transfer\nInsights about memory coalescing on one kernel help others\n\n\n\n\n\nThe Self-Bootstrapping Dynamic\nA typical GEPA optimization trajectory for CUDA kernels follows this pattern:\nIteration 1: Generate naive kernel ‚Üí compiler error ‚Äúundeclared identifier threadIdx‚Äù - Reflection: ‚ÄúCUDA kernels require explicit thread indexing. Add threadIdx.x and blockIdx.x.‚Äù\nIteration 2: Compiles, but runtime error (out of bounds) - Reflection: ‚ÄúNeed bounds checking. Add if (idx &lt; n) guard.‚Äù\nIteration 3: Runs correctly, but 10x slower than baseline - Profiler feedback: ‚ÄúMemory bandwidth: 12% of peak. Non-coalesced access pattern.‚Äù - Reflection: ‚ÄúReorganize memory access for coalescing. Use shared memory for reductions.‚Äù\nIteration 4: 1.2x faster than PyTorch baseline\nEach iteration surfaced a new challenge and its solution‚Äîgenerating increasingly sophisticated training signal from the task itself.\n\n\nCross-Kernel Transfer\nWhen optimizing a batch of related kernels, insights compound across tasks:\n\n‚ÄúIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.‚Äù ‚Äî Lakshya A Agrawal\n\nGEPA exploits the relatedness of kernels in a batch. As it optimizes each kernel, the Pareto frontier accumulates specialized prompts‚Äîone excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. When a new kernel arrives, candidates from across the frontier are tried. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax.\nThis cross-task transfer is why batch optimization outperforms solving each kernel independently‚Äîthe accumulated optimization knowledge benefits later tasks.\n\n\nBeyond Kernels\nThe same pattern extends to other code optimization domains where execution produces rich textual feedback:\n\n‚ÄúThere are many other tasks where it can be used‚Äîfor example, with unit tests to generate code patches.‚Äù ‚Äî Lakshya A Agrawal\n\nBug fixing (test failures explain what‚Äôs wrong), performance optimization (profiler output identifies bottlenecks), and API migration (deprecation warnings specify changes) all fit GEPA‚Äôs feedback-driven model."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "href": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "title": "GEPA Deepdive",
    "section": "Conclusion: When to Reach for GEPA",
    "text": "Conclusion: When to Reach for GEPA\nWe opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculus‚Äîmatching RL‚Äôs optimization performance at a fraction of the sample cost by doing something RL can‚Äôt: reading the feedback.\nThe results speak for themselves: 46.6% ‚Üí 56.6% on AIME math competition problems. 67% ‚Üí 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.\nThis represents a genuinely new point in the optimization design space‚Äîone that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. That‚Äôs not incremental improvement‚Äîit‚Äôs a 100x reduction in sample requirements.\n\n\nUse GEPA for Train-Then-Generalize When:\n\nYou have rich textual feedback (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)\nYour evaluation budget is limited (50-500 examples, not 5,000)\nYou‚Äôre optimizing compound AI systems where prompts orchestrate multi-step pipelines\nYou need interpretable results‚Äîprompts you can read, edit, and reason about\n\n\n\nUse GEPA for Test-Time Search When:\n\nYou have a batch of high-value tasks worth the compute investment\nEach task produces execution feedback (tests, profilers, validators)\nTasks are related enough for cross-task transfer to help\n\n\n\nStick with Traditional Approaches When:\n\nYou have abundant labeled data and compute budget for fine-tuning\nFeedback is purely scalar with no explanatory signal\nThe task is already solved by few-shot prompting\nYou need sub-second latency\n\n\n\n\nKnown Limitations\nGEPA isn‚Äôt a silver bullet:\n\nReflection quality varies by domain ‚Äî GEPA excels when the LLM has strong prior knowledge. On highly specialized domains where the base model is weak, reflection produces generic advice rather than actionable fixes.\nFeedback bottleneck ‚Äî The optimizer is only as good as its feedback. If your metric returns ‚Äúwrong‚Äù without explaining why, GEPA degrades to expensive random search.\nValidation set size ‚Äî With fewer than 30 validation examples, prompts can overfit to idiosyncrasies of those specific instances.\n\n\n\n\nGet Started\nReady to try GEPA on your own pipelines?\n\nGEPA for AIME Tutorial ‚Äî Complete walkthrough from setup to optimized results\nGEPA API Reference ‚Äî Full parameter documentation\nPaper ‚Äî Algorithm details and experimental methodology\n\nThe core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads it‚Äîand learns."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "",
    "text": "Write and speak about AI-powered product, engineering and evals.\nPrototype apps like VLM Self-healing Agent; GEPA reimplementation"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "Blog",
    "text": "Blog"
  }
]