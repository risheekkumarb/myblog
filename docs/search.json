[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html",
    "href": "posts/gepa-deepdive/gepa_final_article.html",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "This article was made possible by the research papers referenced throughout, the Weaviate discussion with Lakshya A Agrawal, and guidance from Kerem Turgutlu at answer.ai. Created using the solveit platform by fast.ai.\nYou‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.\nBut what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) achieves exactly this‚Äîby exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "href": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "title": "GEPA Deepdive",
    "section": "How GEPA Works: Building It From Scratch",
    "text": "How GEPA Works: Building It From Scratch\nGEPA combines two key innovations: reflective prompt mutation (learning from textual feedback) and Pareto selection (preserving diverse specialists). Each is powerful alone; together they compound.\nWe‚Äôll build them from scratch in this section:\n\nReflective mutation ‚Äî How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.\nPareto selection ‚Äî Why always improving your ‚Äúbest‚Äù prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.\nMerge ‚Äî How GEPA combines insights from divergent lineages (covered after the complete algorithm).\n\nBy the end, you‚Äôll see how these mechanisms combine into GEPA‚Äôs full evolutionary loop.\n\n\nQuick Start: Using GEPA in 30 Seconds\n\nüìñ Full tutorial: GEPA for AIME (Math) ‚Äî optimizing GPT-4.1 Mini from 46.6% ‚Üí 56.6% on AIME 2025.\n\nBefore diving deep, here‚Äôs what using GEPA looks like in practice:\nStep 1: Configure your language model\nimport dspy\n\nlm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, max_tokens=32000)\ndspy.configure(lm=lm)\nStep 2: Define your program\nprogram = dspy.ChainOfThought(\"problem -&gt; answer\")\nStep 3: Define a metric that returns feedback (not just a score)\nThis is the key difference from other optimizers‚Äîyour metric explains why something failed:\ndef metric_with_feedback(example, prediction, trace=None, **kwargs):\n    correct_answer = example.answer\n    pred_answer = prediction.answer\n    \n    score = int(correct_answer == pred_answer)\n    \n    if score == 1:\n        feedback = f\"Correct! The answer is '{correct_answer}'.\"\n    else:\n        feedback = f\"Incorrect. Expected '{correct_answer}', got '{pred_answer}'.\"\n        # Add any additional context that could help improvement:\n        if hasattr(example, 'solution'):\n            feedback += f\" Solution: {example.solution}\"\n    \n    return dspy.Prediction(score=score, feedback=feedback)\nStep 4: Optimize with GEPA\nfrom dspy import GEPA\n\noptimizer = GEPA(\n    metric=metric_with_feedback,\n    auto=\"light\",           # Budget preset: \"light\", \"medium\", or \"heavy\"\n    num_threads=32,         # Parallel evaluation threads\n)\n\noptimized_program = optimizer.compile(\n    program,\n    trainset=train_set,\n    valset=val_set,\n)\nThat‚Äôs it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.\n\nWhat‚Äôs happening under the hood?\n\n\n\n\n\n\n\nComponent\nWhat it does\n\n\n\n\nTextual feedback\nYour metric returns why something failed, not just a score\n\n\nReflective mutation\nAn LLM reads the feedback and proposes improved instructions\n\n\nPareto selection\nDiverse specialists are preserved, not just the ‚Äúbest‚Äù prompt\n\n\nMerge operations\nInsights from divergent lineages get combined\n\n\n\nThe rest of this section explains why each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you need‚Äîsee the DSPy GEPA API Reference for full parameter details.\nNow let‚Äôs make this concrete by building the core mechanism from scratch.\n\n\nGEPA: REFLECTIVE PROMPT EVOLUTION (Flow diagram from paper)\n\n\n\nGEPA Flowchart\n\n\nThe diagram above shows how these pieces fit together. Starting from the candidate pool (left), GEPA evaluates each prompt on every training task to build a per-instance scores matrix. Pareto filtering then preserves prompts that excel at something‚Äînot just the highest aggregate scorer. New candidates emerge either through reflective mutation (learning from textual feedback on a minibatch) or merge (combining insights from two specialists). Only candidates that pass a cheap minibatch screening get full evaluation.\nLet‚Äôs now build the core mechanism‚Äîreflective mutation‚Äîfrom scratch to see exactly how the ‚ÄúReflect and Propose New Prompt‚Äù step works.\n\n\nHands-On: Building Reflective Mutation from Scratch\nLet‚Äôs make this concrete by implementing GEPA‚Äôs core mechanism on a real task.\nThe Problem: AIME Math Competition\nWe‚Äôll optimize prompts for solving AIME (American Invitational Mathematics Examination) problems ‚Äî challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.\nfrom datasets import load_dataset\ndset = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n# 90 problems with solutions and integer answers\n\n\n\n\n\n\n\n\nproblem\nsolution\nanswer\n\n\n\n\nQuadratic polynomials \\(P(x)\\) and \\(Q(x)\\) have l‚Ä¶\nLet \\(R(x)=P(x)+Q(x).\\) Since the \\(x^2\\)-terms of‚Ä¶\n116\n\n\nThree spheres with radii \\(11\\), \\(13\\), and \\(19\\) ‚Ä¶\nThis solution refers to the Diagram section‚Ä¶\n756\n\n\n\nWhy AIME for testing prompt optimization?\n\nClear ground truth ‚Äî Every answer is an integer (0-999), so evaluation is unambiguous\nRich failure modes ‚Äî Wrong answers come from algebraic errors, missed cases, misread constraints\nDomain knowledge helps ‚Äî Prompts that encode strategies (‚Äúsubtract equations pairwise‚Äù, ‚Äúenumerate all cases‚Äù) measurably improve performance\nSmall dataset ‚Äî Only 90 problems, so sample efficiency matters\n\nThe Setup\n# Split: 10 train, 10 validation (simulating scarce labeled data)\ntdf = df.sample(45).iloc[:10]  # Training mini-batches drawn from here\nvdf = df.drop(tdf.index).iloc[:10]  # Held-out validation\n\n# Base model: Gemini 2.5 Flash via LiteLLM\n# Metric: Exact match (predicted integer == ground truth)\ndef metric(ground_truth, prediction):\n    return int(ground_truth) == prediction['answer']\nSeed Prompt\nWe start with a minimal instruction:\nseed_prompt = \"\"\"You are given a problem and you have to give the answer \nalong with reasoning. Do not return anything apart from json. \nIt should be parsable by json.loads()\"\"\"\nBaseline validation accuracy: 10% (1/10 correct)\nCan reflective mutation improve this? Let‚Äôs find out.\n\n\n\nStep 1: The Feedback Function\nFirst, we need a function that tells the reflection LLM what went wrong. We‚Äôll start with minimal feedback‚Äîjust the correct answer:\ndef feedback(ground_truth, prediction):\n    if int(ground_truth) != prediction['answer']:\n        return f'You got it wrong! The solution is {ground_truth}'\n    return 'You got it right!'\nThis is deliberately simple. Later we‚Äôll discuss how richer feedback (like expert solutions) can improve results.\n\n\n\nStep 2: The Reflection Prompt\nFollowing GEPA‚Äôs structure, we build a prompt that shows the LLM its failures:\nREFLECTION_TEMPLATE = \"\"\"I provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\n{current_prompt}\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n{examples}\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\"\"\"\n\ndef mk_reflection_prompt(df, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\nExample filled-in reflection prompt:\nI provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\nYou are given a problem and you have to give the answer along with reasoning. \nDo not return anything apart from json. It should be parsable by json.loads()\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n# Example 1\n## problem\nQuadratic polynomials P(x) and Q(x) have leading coefficient 1. The sum of the roots of P(x) is 7...\n## prediction\n{\"answer\": 42, \"reasoning\": \"I solved the system and got x=7, y=6\"}\n## feedback\nYou got it wrong! The solution is 116\n\n# Example 2\n## problem\nThree spheres with radii 11, 13, and 19 are mutually externally tangent...\n## prediction\n{\"answer\": 756, \"reasoning\": \"Using the tangent sphere formula...\"}\n## feedback\nYou got it right!\n\n# Example 3\n## problem\nFind the remainder when 2^2024 is divided by 1000...\n## prediction\n{\"answer\": 16, \"reasoning\": \"I computed powers of 2 mod 1000...\"}\n## feedback\nYou got it wrong! The solution is 896\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\n\n\nStep 3: The Complete Optimization Loop\ndef mk_reflection_prompt(mb, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\n\ndef reflect(mb, curr_prompt):\n    \"\"\"Ask LLM to reflect on failures and propose improved instruction.\"\"\"\n    refl_prompt = mk_reflection_prompt(mb, curr_prompt)\n    return _call(refl_prompt, format=ReflectionModel)['new_instruction']\n\ndef optimize_prompt(seed_prompt, traindf, valdf, n_iters=3, mb_size=3):\n    \"\"\"Greedy reflective prompt optimization.\"\"\"\n    prompts, train_scores, val_scores = [seed_prompt], [], []\n    mb = traindf.sample(mb_size)  # Fixed minibatch for this run\n    \n    print(f'Baseline validation: {eval_val(seed_prompt, valdf):.2%}')\n    \n    for i in range(n_iters):\n        # Evaluate current prompt on minibatch\n        mb_eval, mb_score = eval_mb(prompts[-1], mb)\n        print(f\"üìä Minibatch: {mb_score:.2%}\")\n        \n        # Reflect and propose new instruction\n        new_instr = reflect(mb_eval, prompts[-1])\n        new_prompt = new_instr  # The new instruction becomes the new prompt\n        \n        # Evaluate on validation set\n        val_score = eval_val(new_prompt, valdf)\n        print(f\"üìä Validation: {val_score:.2%}\")\n        \n        prompts.append(new_prompt)\n        val_scores.append(val_score)\n    \n    return dict(prompts=prompts, val_scores=val_scores)\n\n\n\nWhat Actually Happened\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\n\n\n\n\n\n\nIteration\nMinibatch\nValidation\nWhat the reflection learned\n\n\n\n\nBaseline\n‚Äî\n10%\n‚Äî\n\n\n1\n0%\n10%\nJSON formatting details, output structure rules\n\n\n2\n0%\n30%\nSystems of equations strategy, remainder/modular arithmetic tips\n\n\n3\n67%\n10%\nOver-specialized on number theory, solved #9 but lost generality\n\n\n\nThe good: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:\n\n‚ÄúWhen dealing with systems of equations like \\(xy + Az = C\\), \\(yz + Ax = C\\), \\(zx + Ay = C\\), consider subtracting equations pairwise to find relationships between variables, such as \\((x-z)(y-A)=0\\), which implies \\(x=z\\) or \\(y=A\\). Systematically explore all such cases.‚Äù\n\nThis is directly from the actual output‚Äîthe reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.\nThe bad: Iteration 3 achieved 67% on its minibatch but dropped to 10% validation. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:\n\n‚ÄúWhen the problem involves number theory and remainders (e.g., \\(n \\pmod x\\), \\(n \\pmod y\\)), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.‚Äù\n\nThis over-specialized advice (‚Äúcan lead to an answer of 0‚Äù) actively hurt performance on non-number-theory problems, dropping validation from 30% back to 10% (1/10)‚Äîthough notably, it did solve problem #9, which earlier prompts couldn‚Äôt.\n\n\n\nThe Greedy Selection Problem\nThis demonstrates exactly why GEPA uses Pareto selection instead of always taking the ‚Äúbest‚Äù prompt:\n\nIteration 2‚Äôs prompt was a specialist‚Äîit learned something valuable about systems of equations\nIteration 3 tried to improve on iteration 2, but the minibatch had different problems\nThe reflection overwrote the systems-of-equations insight while adding number-theory tips that were too specific\nResult: catastrophic forgetting\n\nWith greedy selection, we would have discarded iteration 2‚Äôs valuable insight. Pareto selection would keep it‚Äîbecause it was best on at least one validation instance.\n\n\n\nThe Missing Ingredient: Rich Feedback\nOur minimal feedback (\"You got it wrong! The solution is 349\") only tells the model that it failed, not why or how to fix it.\nThe AIME dataset includes expert solutions. A richer feedback function could use them:\ndef feedback_rich(row):\n    if int(row.answer) != row.pred['answer']:\n        sol = row.solution[:500] + \"...\" if len(row.solution) &gt; 500 else row.solution\n        return f\"\"\"Wrong! Expected {row.answer}, got {row.pred['answer']}.\n        \nModel's reasoning: {row.pred['short_reasoning']}\n\nExpert solution approach:\n{sol}\"\"\"\n    return \"Correct!\"\nExample output for a wrong answer:\nWrong! Expected 116, got 42.\n\nModel's reasoning: I set up the system of equations and solved for x=7, y=6, giving 7*6=42.\n\nExpert solution approach:\nLet R(x)=P(x)+Q(x). Since the x¬≤-terms of P and Q have leading coefficient 1, \nR(x) is quadratic with leading coefficient 2. Given the roots condition, we can \nwrite R(x) = 2(x-r‚ÇÅ)(x-r‚ÇÇ). Expanding and comparing coefficients...\nWith rich feedback, the reflection LLM can extract specific strategies from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: the feedback contains the fix.\nCompare this to RL, which would only see reward = 0 and have to statistically infer what went wrong across thousands of trajectories.\n\n\n\nKey Takeaways\n\nReflective mutation works ‚Äî Even with minimal feedback, the LLM extracted useful domain knowledge\nGreedy selection fails ‚Äî Iteration 3‚Äôs collapse shows why we need to preserve specialist insights\nFeedback quality matters ‚Äî Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with\nSample efficiency is real ‚Äî We saw meaningful optimization with just 3 iterations on 3 examples each\n\nThis is the core limitation of greedy optimization: catastrophic forgetting. The solution? Pareto selection‚Äîwhich we‚Äôll build from scratch next.\n\n\nHands-On: Building the Pareto Frontier\nIn the reflective mutation section, we saw greedy selection fail‚Äîiteration 3‚Äôs over-specialized prompt dropped validation from 30% to 10%, losing iteration 2‚Äôs valuable systems-of-equations insights. The fundamental problem: always improving your ‚Äúbest‚Äù prompt discards specialist knowledge.\nGEPA‚Äôs solution: Pareto selection. Instead of keeping one best prompt, maintain a frontier of prompts where each excels at something no other prompt beats.\n\n\nWhat is Pareto Dominance?\nA prompt dominates another if it‚Äôs at least as good everywhere, and strictly better somewhere:\n\n‚â• on every validation instance, AND\n\n&gt; on at least one instance\n\nIf prompt A dominates prompt B, we can safely discard B‚ÄîA is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.\nExample: Consider four prompts evaluated on 10 validation instances. We‚Äôll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:\n\n\n\nPrompt\nInstances Solved\nAggregate\nStatus\n\n\n\n\nP0 (seed)\n#0 only\n10%\nDominated by P2\n\n\nP1 (iter 1)\n#0 only\n10%\nDominated by P2\n\n\nP2 (iter 2)\n#0, #1, #2\n30%\nFrontier ‚úì\n\n\nP3 (iter 3)\n#9 only\n10%\nFrontier ‚úì\n\n\n\nP2 dominates both P0 and P1‚Äîit solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which nothing else could.\nThe Pareto frontier is {P2, P3}. Both contain unique value.\n\n\n\nImplementation: Dominance Checking\nWe represent per-instance scores as boolean arrays (1 = solved, 0 = failed):\nimport numpy as np\n\ndef dominates(candidate_scores, other_scores):\n    \"\"\"Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)\"\"\"\n    candidate = np.array(candidate_scores)\n    other = np.array(other_scores)\n    return (candidate &gt;= other).all() and (candidate &gt; other).any()\n\ndef is_dominated_by_any(new_scores, frontier_scores):\n    \"\"\"Is new_scores dominated by ANY prompt in the frontier?\"\"\"\n    new = np.array(new_scores)\n    for existing in frontier_scores:\n        if dominates(np.array(existing), new):\n            return True\n    return False\n\ndef get_dominated_indices(new_scores, frontier_scores):\n    \"\"\"Which frontier prompts does new_scores dominate?\"\"\"\n    new = np.array(new_scores)\n    return [i for i, existing in enumerate(frontier_scores) \n            if dominates(new, np.array(existing))]\n\n\n\nTracing Through: Why P3 Survives\nLet‚Äôs verify the dominance relationships from our example:\nP0 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP1 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP2 = [1,1,1,0,0,0,0,0,0,0]  # Solves: #0, #1, #2\nP3 = [0,0,0,0,0,0,0,0,0,1]  # Solves: #9\n\n# Does P2 dominate P0?\ndominates(P2, P0)  # True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2\n\n# Does P2 dominate P1?\ndominates(P2, P1)  # True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2\n\n# Does P2 dominate P3?\ndominates(P2, P3)  # False! P2 loses on #9 (0 &lt; 1)\n\n# Does P3 dominate P2?\ndominates(P3, P2)  # False! P3 loses on #0, #1, #2\nNeither P2 nor P3 dominates the other‚Äîthey‚Äôre Pareto incomparable. Each solves problems the other can‚Äôt. Both stay on the frontier.\n\n\n\nPareto Frontier Explained\n\n\n\n\n\nThe Complete Frontier Manager\nclass ParetoFrontier:\n    def __init__(self):\n        self.prompts = []\n        self.scores = []  # scores[i][j] = prompt i's score on instance j\n    \n    def add(self, prompt, instance_scores):\n        \"\"\"Try to add a prompt. Returns True if it joins the frontier.\"\"\"\n        # Reject if dominated by existing frontier member\n        if is_dominated_by_any(instance_scores, self.scores):\n            return False\n        \n        # Remove any frontier members this prompt dominates\n        dominated = get_dominated_indices(instance_scores, self.scores)\n        for i in sorted(dominated, reverse=True):  # Remove from end first\n            del self.prompts[i]\n            del self.scores[i]\n        \n        # Add to frontier\n        self.prompts.append(prompt)\n        self.scores.append(instance_scores)\n        return True\n    \n    def sample(self):\n        \"\"\"Sample a prompt, weighted by unique wins.\"\"\"\n        weights = []\n        scores_arr = np.array(self.scores)\n        for i in range(len(self.prompts)):\n            # How many instances is this prompt *uniquely* best on?\n            others_best = np.delete(scores_arr, i, axis=0).max(axis=0) if len(self.prompts) &gt; 1 else np.zeros_like(scores_arr[i])\n            unique_wins = (scores_arr[i] &gt; others_best).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        \n        return np.random.choice(self.prompts, p=np.array(weights)/sum(weights))\n    \n    def best_aggregate(self):\n        \"\"\"Return the prompt with highest aggregate score.\"\"\"\n        aggregates = [sum(s) for s in self.scores]\n        return self.prompts[np.argmax(aggregates)]\n\n\n\nPutting It Together: Pareto-Guided Optimization\ndef optimize_with_pareto(seed_prompt, traindf, valdf, n_iters=5, mb_size=3):\n    \"\"\"Reflective mutation with Pareto frontier selection.\"\"\"\n    frontier = ParetoFrontier()\n    \n    # Initialize with seed\n    seed_scores = evaluate_per_instance(seed_prompt, valdf)\n    frontier.add(seed_prompt, seed_scores)\n    print(f'Baseline: {sum(seed_scores)/len(seed_scores):.1%}')\n    \n    for i in range(n_iters):\n        print(f\"\\n{'='*40}\\nIteration {i+1}\\n{'='*40}\")\n        \n        # Sample parent from frontier (weighted by unique wins)\n        parent = frontier.sample()\n        \n        # Run on minibatch, reflect, propose mutation\n        mb = traindf.sample(mb_size)\n        mb_results = evaluate_with_traces(parent, mb)\n        new_prompt = reflect_and_mutate(parent, mb_results)\n        \n        # Evaluate on full validation set\n        new_scores = evaluate_per_instance(new_prompt, valdf)\n        new_agg = sum(new_scores) / len(new_scores)\n        print(f\"New prompt: {new_agg:.1%} aggregate\")\n        \n        # Try to add to frontier\n        if frontier.add(new_prompt, new_scores):\n            print(f\"‚úì Added to frontier (size: {len(frontier.prompts)})\")\n        else:\n            print(f\"‚úó Dominated, rejected\")\n    \n    return frontier.best_aggregate()\n\n\n\nWhat We Observed on AIME\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\nIteration\nAggregate\nInstances Solved\nFrontier Action\n\n\n\n\nSeed\n10%\n#0\nInitialize\n\n\n1\n10%\n#0\nDominated by seed, rejected\n\n\n2\n30%\n#0, #1, #2\nAdded, dominates seed\n\n\n3\n10%\n#9 only\nAdded ‚úì (unique win on #9)\n\n\n\nThe key moment: Iteration 3 scored only 10%‚Äîworse than iteration 2‚Äôs 30%. Greedy selection would discard it entirely.\nBut it solved instance #9, which nothing else could. Pareto selection preserves it.\nOur final frontier: {P2, P3}\n\nP2: Strong generalist (30%), knows systems-of-equations strategies\nP3: Instance-9 specialist (10%), knows whatever cracked that specific problem\n\nBoth insights survive. The merge operation (covered later) can combine them.\n\n\n\nWhy This Matters: No More Catastrophic Forgetting\n\n\n\nSelection Strategy\nWhat happens to specialists\n\n\n\n\nGreedy\nDiscarded whenever aggregate score drops\n\n\nPareto\nPreserved if they solve anything unique\n\n\n\nGreedy selection caused our iteration 3 collapse‚Äîthe number-theory prompt overwrote the algebra prompt‚Äôs insights. Pareto selection prevents this by construction: you can‚Äôt remove a prompt from the frontier unless something else does everything it does, plus more.\nThis is the core of GEPA‚Äôs sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier never loses them in the first place.\n\nWe just saw Pareto selection preserve iteration 3‚Äôs prompt despite its 10% aggregate score‚Äîbecause it solved instance #9, which nothing else could. But this raises a question: why does keeping ‚Äúlosers‚Äù help optimization? Shouldn‚Äôt we focus resources on the best candidates?\nThe answer comes from quality-diversity algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.\n\n\n\nWhy Pareto Works: Quality-Diversity and Map Elites\nWe‚Äôve now built both core mechanisms from scratch‚Äîreflective mutation and Pareto selection. Before diving into the full algorithm, let‚Äôs briefly step back and understand why this approach works so well.\nThe Pareto frontier isn‚Äôt a novel invention‚Äîit draws from quality-diversity (QD) algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.\n\nThe Core Insight\nTraditional optimization asks: ‚ÄúWhat‚Äôs the single best solution?‚Äù\nQuality-diversity asks: ‚ÄúWhat‚Äôs the best solution of each type?‚Äù\nComplex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensively‚Äîbut the best engine knows both styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one ‚Äúbest‚Äù approach.\n\n\nMap Elites: The Inspiration\nMap Elites (Mouret & Clune, 2015) maintains an archive organized by behavior:\n\nDefine behavior dimensions ‚Äî characteristics describing how a solution works (not just how well)\nDiscretize into bins ‚Äî each cell represents a ‚Äúniche‚Äù\nKeep the best per bin ‚Äî new solutions compete only within their niche\nMutate from the archive ‚Äî sample from any occupied bin, mutate, place in appropriate bin\n\nThe result: diverse specialists, each optimal of its type. Key insight: the archive provides stepping stones‚Äîa mutation from one niche might discover something useful for another. Diversity isn‚Äôt just nice to have; it‚Äôs a search strategy.\n\n\nGEPA‚Äôs Adaptation: Validation Instances as Niches\nGEPA recognizes that the validation set itself defines the behavior space:\n\n\n\n\n\n\n\nMap Elites\nGEPA\n\n\n\n\nBehavior = continuous dimensions\nBehavior = which validation instances are solved\n\n\nBins = discretized regions\n‚ÄúBins‚Äù = individual validation instances\n\n\nArchive = best per bin\nPareto frontier = non-dominated prompts across instances\n\n\n\nEach validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the ‚Äúspecialist for that niche‚Äù‚Äîit stays because it demonstrates something works, even with low aggregate score.\n\n‚ÄúBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.‚Äù ‚Äî Lakshya A Agrawal\n\nThis directly motivated GEPA‚Äôs design: Pareto selection over per-instance scores naturally implements QD‚Äôs ‚Äúbest per niche‚Äù principle, while the merge operation recombines insights across niches‚Äîexactly what stepping-stone search requires.\nWith both mechanisms in place, there‚Äôs one more operation that makes GEPA powerful: merge‚Äîcombining insights from divergent lineages.\n\n\n\nThe Lineage Tree and System-Aware Merge\nPareto selection preserves specialists‚Äîbut it creates a new problem: insights get siloed in separate branches.\nConsider what happens after 10 iterations of GEPA on AIME problems:\n\n\n\nLineage Tree with Merge Operation\n\n\nThe P2‚ÜíP4 lineage accumulated algebra insights. The P3‚ÜíP5 lineage accumulated number theory insights. Both survive on the Pareto frontier because each solves problems the other can‚Äôt.\nBut what about a problem requiring both?\n\n\nThe Recombination Problem\nSuppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:\n\nP4 knows to subtract equations pairwise, but doesn‚Äôt think to reduce mod p\nP5 knows CRT, but misses the algebraic setup\n\nWith mutation alone, P4 would need to independently rediscover number theory (which P5 already knows), or vice versa. The knowledge exists in our population‚Äîjust in different branches.\nMerge solves this by combining insights from divergent lineages into a single candidate.\n\n\n\nHow Merge Works\nGEPA‚Äôs merge is ‚Äúsystem-aware‚Äù‚Äîthe LLM understands what it‚Äôs combining, so it can resolve contradictions and synthesize coherently rather than blindly concatenating (unlike genetic algorithm crossover which swaps segments randomly). The reflection LLM receives:\n\nBoth parent prompts with their full instruction text\nLineage context ‚Äî what types of problems each lineage solved\nConflict guidance ‚Äî instructions to resolve contradictions, not ignore them\n\nThe prompt asks the LLM to synthesize, not concatenate:\n\n‚ÄúCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don‚Äôt simply concatenate‚Äîsynthesize into coherent guidance.‚Äù\n\nConcrete example ‚Äî merging our AIME specialists:\n\n\n\n\n\n\n\n\nParent\nSpecialty\nKey instruction\n\n\n\n\nP4\nAlgebra\n‚ÄúSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.‚Äù\n\n\nP5\nNumber theory\n‚ÄúApply CRT for modular constraints. Check for contradictions.‚Äù\n\n\n\nMerged offspring P6: &gt; ‚ÄúApproach: (1) For equation systems, subtract pairwise to expose factor relationships‚Äîenumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.‚Äù\nP6 inherits both toolkits AND adds meta-knowledge about when to combine them. This is the ‚Äúsystem-aware‚Äù part‚Äîthe merge understands the semantics of what it‚Äôs combining.\n\n\n\nWhen to Merge vs.¬†Mutate\nGEPA alternates between operations based on frontier state:\n\n\n\n\n\n\n\n\nCondition\nOperation\nRationale\n\n\n\n\nEarly iterations (&lt; 5)\nMutate\nLet lineages diverge first; nothing to merge yet\n\n\nFrontier has one dominant lineage\nMutate\nNo orthogonal insights to combine\n\n\nFrontier has divergent specialists\nMerge\nRecombine discoveries from parallel explorations\n\n\nRecent merge succeeded\nMutate\nRefine the merged candidate\n\n\n\nThe paper describes the decision:\n\n‚ÄúAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nMerge vs.¬†Genetic Algorithm Crossover\nGEPA‚Äôs merge is analogous to crossover in genetic algorithms, but smarter:\n\n\n\n\n\n\n\nGenetic Algorithms\nGEPA Merge\n\n\n\n\nGenes = bit positions\nInsights = natural language instructions\n\n\nCrossover = swap bit segments randomly\nMerge = LLM synthesizes with understanding\n\n\nCan create invalid offspring\nCan resolve contradictions\n\n\nBlind to semantics\nAware of what instructions mean\n\n\n\nRandom crossover might produce: ‚ÄúSubtract equations pairwise. Apply CRT. Subtract equations pairwise.‚Äù (nonsense duplication)\nLLM merge produces: ‚ÄúSet up algebra first, then apply number-theoretic constraints.‚Äù (coherent synthesis)\n\n\n\nThe Compounding Effect\nThe components reinforce each other:\n\nPareto selection preserves the diversity that makes merge valuable\nLineage tracking identifies which candidates come from divergent branches\n\nMerge recombines orthogonal discoveries into unified candidates\nPareto selection then preserves successful merges alongside remaining specialists\n\nWithout Pareto selection, there‚Äôs nothing interesting to merge‚Äîyou‚Äôd just have variants of one ‚Äúbest‚Äù prompt. Without merge, insights stay siloed even when the frontier is diverse.\n\nImplementation note: The full GEPA implementation includes safeguards to ensure merge candidates actually have different insights worth combining‚Äîchecking for common ancestors, avoiding redundant merges, and verifying that descendants improved on their ancestor. See the DSPy source for details.\n\nMutation explores depth‚Äîrefining one approach through successive reflections.\nMerge explores breadth‚Äîcombining orthogonal discoveries from parallel paths.\nTogether, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.\nWith reflective mutation, Pareto selection, and merge all in place, here‚Äôs how they combine into GEPA‚Äôs full optimization loop.\n\n\n\nThe Complete Algorithm\nWith all pieces in place, here‚Äôs how they combine into the full optimization loop.\n\nAlgorithm Overview\nimport random\nimport numpy as np\nfrom typing import Callable\n\ndef gepa(\n    base_prompt: str,\n    trainset: list,\n    valset: list,\n    evaluate_fn: Callable,          # (prompt, example) -&gt; score (0 or 1)\n    run_with_feedback_fn: Callable, # (prompt, examples) -&gt; (traces: list[str], feedback: list[str])\n    reflect_fn: Callable,           # (parent_prompt, traces, feedback) -&gt; new_prompt: str\n    merge_fn: Callable,             # (prompt1, prompt2) -&gt; merged_prompt: str\n    max_iterations: int = 20,\n    minibatch_size: int = 3,\n):\n    \"\"\"\n    GEPA: Genetic-Pareto prompt optimization.\n    \n    Returns the best aggregate prompt; access full frontier via returned dict.\n    \"\"\"\n    \n    # --- Helper functions ---\n    \n    def evaluate_all(prompt, dataset):\n        \"\"\"Return per-instance scores as list.\"\"\"\n        return [evaluate_fn(prompt, ex) for ex in dataset]\n    \n    def evaluate_minibatch(prompt, minibatch):\n        \"\"\"Return aggregate score on minibatch.\"\"\"\n        return sum(evaluate_fn(prompt, ex) for ex in minibatch) / len(minibatch)\n    \n    def dominates(scores_a, scores_b):\n        \"\"\"Does A dominate B? (&gt;= everywhere, &gt; somewhere)\"\"\"\n        a, b = np.array(scores_a), np.array(scores_b)\n        return (a &gt;= b).all() and (a &gt; b).any()\n    \n    def is_dominated_by_frontier(new_scores, frontier, scores):\n        \"\"\"Is new_scores dominated by ANY frontier member?\"\"\"\n        return any(dominates(scores[c], new_scores) for c in frontier)\n    \n    def sample_from_frontier(frontier, scores):\n        \"\"\"Sample weighted by unique wins.\"\"\"\n        n_instances = len(next(iter(scores.values())))\n        weights = []\n        for candidate in frontier:\n            if len(frontier) == 1:\n                unique_wins = n_instances\n            else:\n                others_max = np.array([scores[o] for o in frontier if o != candidate]).max(axis=0)\n                unique_wins = (np.array(scores[candidate]) &gt; others_max).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        return random.choices(frontier, weights=weights)[0]\n    \n    def get_root(prompt, lineage):\n        \"\"\"Trace lineage back to root.\"\"\"\n        while lineage.get(prompt) is not None:\n            prompt = lineage[prompt]\n        return prompt\n    \n    def should_merge(frontier, lineage, iteration):\n        \"\"\"Decide whether to merge or mutate.\"\"\"\n        if len(frontier) &lt; 2 or iteration &lt; 5:\n            return False\n        n_lineages = len(set(get_root(c, lineage) for c in frontier))\n        return random.random() &lt; (n_lineages - 1) / len(frontier)\n    \n    def select_divergent(frontier, parent, lineage):\n        \"\"\"Select a candidate from a different lineage.\"\"\"\n        parent_root = get_root(parent, lineage)\n        others = [c for c in frontier if get_root(c, lineage) != parent_root]\n        return random.choice(others) if others else random.choice(frontier)\n    \n    def best_aggregate(frontier, scores):\n        \"\"\"Return prompt with highest aggregate score.\"\"\"\n        return max(frontier, key=lambda c: sum(scores[c]))\n    \n    # --- Main loop ---\n    \n    candidates = [base_prompt]\n    scores = {base_prompt: evaluate_all(base_prompt, valset)}\n    pareto_frontier = [base_prompt]\n    lineage = {base_prompt: None}\n    \n    for iteration in range(1, max_iterations + 1):\n        \n        # 1. SAMPLE: Select parent from Pareto frontier\n        parent = sample_from_frontier(pareto_frontier, scores)\n        \n        # 2. PROPOSE: Either mutate or merge\n        minibatch = random.sample(trainset, min(minibatch_size, len(trainset)))\n        \n        if should_merge(pareto_frontier, lineage, iteration):\n            other_parent = select_divergent(pareto_frontier, parent, lineage)\n            new_prompt = merge_fn(parent, other_parent)\n        else:\n            traces, feedback = run_with_feedback_fn(parent, minibatch)\n            new_prompt = reflect_fn(parent, traces, feedback)\n        \n        # 3. EVALUATE: Mini-batch gate\n        parent_mb_score = evaluate_minibatch(parent, minibatch)\n        new_mb_score = evaluate_minibatch(new_prompt, minibatch)\n        if new_mb_score &lt;= parent_mb_score:\n            continue  # Reject: didn't improve on mini-batch\n        \n        # Full evaluation\n        new_scores = evaluate_all(new_prompt, valset)\n        \n        # 4. UPDATE: Pareto frontier maintenance\n        if is_dominated_by_frontier(new_scores, pareto_frontier, scores):\n            continue  # Reject: dominated by existing candidate\n        \n        # Remove dominated candidates\n        pareto_frontier = [c for c in pareto_frontier \n                          if not dominates(new_scores, scores[c])]\n        \n        # Add new candidate\n        candidates.append(new_prompt)\n        scores[new_prompt] = new_scores\n        pareto_frontier.append(new_prompt)\n        lineage[new_prompt] = parent\n    \n    return {\n        'best': best_aggregate(pareto_frontier, scores),\n        'frontier': pareto_frontier,\n        'scores': scores,\n        'lineage': lineage,\n    }\n\n\n\nThe Key Decision Points\n1. Candidate Sampling ‚Äî Weighted by unique wins, so specialists get attention proportional to their unique value.\n2. Mutation vs Merge ‚Äî Early iterations favor mutation; merge probability increases as frontier diversifies.\n3. Mini-Batch Gating\nBefore expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:\n\n‚ÄúWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.‚Äù ‚Äî Lakshya A Agrawal\n\n4. Pareto Update\nThe frontier update follows the dominance logic we implemented earlier: - Reject new candidates dominated by existing ones (they add nothing) - Remove existing candidates dominated by the new one (they‚Äôre obsolete) - Keep all non-dominated candidates (each offers unique value)\n\n\n\nComplexity Analysis\n\n\n\n\n\n\n\nOperation\nCost\n\n\n\n\nMutation (3-4 rollouts + reflection)\n3-4 LLM calls + 1 reflection call\n\n\nMini-batch evaluation\n3-4 metric evaluations\n\n\nFull validation evaluation\nN metric evaluations (N = valset size)\n\n\nPareto check\nO(F √ó N) comparisons (F = frontier size)\n\n\n\nThe mini-batch gate is crucial for efficiency. Most mutations fail‚Äîthey either don‚Äôt improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.\n\n\n\nWhy Each Component Matters\n\n\n\n\n\n\n\n\nComponent\nWithout it\nWith it\n\n\n\n\nTextual feedback\nOptimizer sees only score=0.6\nOptimizer reads ‚Äúwrong answer, expected X, got Y because‚Ä¶‚Äù\n\n\nPareto selection\nSpecialists discarded when aggregate drops\nSpecialists preserved if they solve anything unique\n\n\nLineage tracking\nNo memory of evolutionary history\nCan identify divergent branches for merge\n\n\nMerge operation\nInsights stay siloed in separate branches\nOrthogonal discoveries can combine\n\n\nMini-batch gating\nEvaluate every candidate fully\nReject obvious failures cheaply\n\n\n\nThe components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.\n\n\n\nWhat Gets Returned\nThe algorithm returns best_aggregate(pareto_frontier)‚Äîthe prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use track_stats=True to access all candidates and their per-instance scores.\n\n\n\n\nBringing It Together\nThe algorithm above integrates everything we‚Äôve built: reflective mutation extracts lessons from textual feedback, Pareto selection preserves specialist insights that greedy selection would discard, and merge recombines discoveries from divergent lineages.\nBut what does all this machinery actually produce? The answer reveals why GEPA matters beyond just better benchmark scores.\n\n\nWhat GEPA Learns: Domain-Specific Knowledge Encoding\nWe‚Äôve built the full algorithm‚Äîreflective mutation, Pareto selection, merge. But what does all this machinery actually produce? Let‚Äôs examine the output: prompts that encode domain expertise.\nOne of GEPA‚Äôs most striking capabilities is its ability to encode domain-specific knowledge directly into prompts‚Äîtransforming tacit expertise into explicit instructions that persist across examples.\n\nPrompts as Knowledge Containers\nTraditional optimization treats prompts as opaque strings to be scored. GEPA treats them as knowledge containers that accumulate insights through the reflection loop:\n\n\n\nFailure Accumulation Experience\n\n\nEach iteration doesn‚Äôt just fix one error‚Äîit extracts the lesson behind the error.\nConcrete example from our AIME experiments:\n\n\n\n\n\n\n\nStage\nPrompt excerpt\n\n\n\n\nSeed\n‚ÄúYou are given a problem and you have to give the answer along with reasoning.‚Äù\n\n\nAfter iter 2\n‚Äú‚Ä¶For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.‚Äù\n\n\nAfter iter 3\n‚Äú‚Ä¶When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.‚Äù\n\n\n\nThe prompt evolved from generic instruction to encoding competition math heuristics. The LLM didn‚Äôt invent these‚Äîit extracted them from its training knowledge, triggered by seeing specific failure modes.\n\n\nThree Categories of Captured Knowledge\nWe observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):\n1. Format and Interface Knowledge - Output schemas (‚Äúreturn JSON with keys: answer, reasoning‚Äù) - API conventions (‚Äúuse library.method(), not library_method()‚Äù) - Parsing requirements (‚Äúintegers only, no leading zeros‚Äù)\nThis is easiest to extract‚Äîformat errors produce explicit feedback.\n2. Strategic Knowledge - Problem-solving heuristics (‚Äútry small cases first‚Äù) - Domain patterns (‚Äúcompetition problems often combine algebra and number theory‚Äù) - Failure mode awareness (‚Äúwatch for off-by-one errors in counting‚Äù)\nThis emerges from reflecting on why approaches failed, not just that they failed.\n3. Factual Domain Knowledge - API names and signatures (‚Äútorch.einsum, not torch.einstein_sum‚Äù) - Domain constants (‚Äústandard gravity = 9.81 m/s¬≤‚Äù) - Constraint relationships (‚Äúin valid Sudoku, each row/column/box contains 1-9 exactly once‚Äù)\nThe LLM already knows this‚Äîreflection surfaces it into the prompt where it‚Äôs consistently applied.\n\n\nWhy Prompts Beat Weights (Sometimes)\nFine-tuning encodes knowledge in model weights‚Äîopaque, distributed, hard to inspect. GEPA encodes knowledge in natural language‚Äîreadable, editable, composable.\n\n\n\nAspect\nFine-tuning\nGEPA prompts\n\n\n\n\nInspectability\nBlack box\nHuman-readable instructions\n\n\nEditability\nRequires retraining\nEdit the text directly\n\n\nComposability\nTrain new model\nMerge prompt sections\n\n\nSample efficiency\nThousands of examples\nTens of examples\n\n\n\nA GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.\n\n\nThe Preservation Problem\nKnowledge accumulation has a failure mode: new insights can overwrite old ones. We saw this in our hands-on experiment‚Äîiteration 3‚Äôs number theory insights overwrote iteration 2‚Äôs algebra insights, causing catastrophic forgetting.\nThis is precisely why GEPA uses Pareto selection. By preserving prompts that are best on any validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier‚Äîand the merge operation can later combine their insights.\n\nWithout Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.\n\n\n\nLimitation: Knowledge Must Be Triggerable\nGEPA can only surface knowledge the base LLM already has‚Äîit extracts and organizes existing knowledge, not creates new knowledge. This is why it works better with stronger base models: more latent knowledge to extract. For highly specialized domains (custom hardware APIs, proprietary protocols), you may need human-curated seed instructions or few-shot examples to bootstrap the reflection loop.\n\nSo far we‚Äôve focused on training: optimize prompts on labeled examples, deploy the best one. But GEPA‚Äôs machinery‚Äîreflective mutation, Pareto selection, merge‚Äîcan also be deployed at inference time as a search algorithm. This opens a second paradigm worth understanding."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "href": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "title": "GEPA Deepdive",
    "section": "Beyond Training: GEPA for Inference-Time Search",
    "text": "Beyond Training: GEPA for Inference-Time Search\nEverything we‚Äôve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.\n\nTwo Paradigms of Operation\nTrain-then-generalize (what we‚Äôve built so far):\n\nOptimize prompts on a training set\nSelect the best-aggregate prompt from the Pareto frontier\nDeploy that prompt on new, unseen tasks\nGoal: learn generalizable lessons that transfer\n\nTest-time search (inference-time optimization):\n\nYou have a batch of hard tasks you need to solve now\nOptimize directly on the tasks themselves\nGEPA searches for solutions, storing the best prompt per task\nGoal: maximize performance on these specific instances\n\nThe mechanics are identical‚Äîreflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you‚Äôre using GEPA as a search algorithm over the solution space. This aligns with the broader trend of inference-time compute scaling‚Äîinvesting more computation at inference to solve harder problems.\nThe key mechanical change: In normal GEPA, you have separate trainset (to learn from via reflection) and valset (to evaluate generalization). The Pareto frontier tracks per-instance performance on valset, preserving prompts that generalize well.\nFor test-time search, pass the same problems as both trainset and valset:\n# Test-time search: optimize directly on the tasks you want to solve\noptimized = GEPA(metric=metric_with_feedback, auto=\"medium\").compile(\n    program,\n    trainset=hard_problems,  # The actual tasks you need solved\n    valset=hard_problems,    # Same held-out validation\n)\nThis tells GEPA: ‚ÄúI don‚Äôt care about generalization‚Äîoptimize directly on these specific problems.‚Äù The Pareto frontier now tracks ‚Äúbest prompt for each problem‚Äù rather than ‚Äúprompts that transfer to unseen data.‚Äù See the GEPA API documentation for full parameter details.\n\n‚ÄúGiven a batch of tasks that we want to solve and given some budget‚Ä¶ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\n\nWhy GEPA Beats High-Temperature Sampling\nTraditional inference-time strategies sample at high temperature to generate many candidates, then pick the best. But these samples tend to be similar‚Äîvariations on the same approach. GEPA induces genuine diversity through Pareto tracking (maintaining candidates that excel at something different) and reflective mutation (proposing structurally different strategies based on what went wrong, not random perturbations).\nWhen feedback says ‚Äúmemory bandwidth bottleneck,‚Äù the next candidate might switch from a naive loop to shared-memory tiling‚Äîa qualitative change that temperature variation rarely discovers. On the MATH benchmark, this approach achieves 93% accuracy compared to 67% with basic DSPy ChainOfThought.\n\n\n\nSelf-Bootstrapping at inference\nDuring training, you iterate a fixed number of times and deploy the result. At inference time, you can keep iterating on a single hard problem until it‚Äôs solved‚Äîand GEPA‚Äôs reflective loop creates a self-bootstrapping dynamic:\n\nRound 1: Generate rollout ‚Üí compiler error (‚Äúundefined variable x‚Äù) ‚Üí reflect ‚Üí propose fix\nRound 2: Code compiles ‚Üí runtime error (division by zero) ‚Üí reflect ‚Üí propose fix\n\nRound 3: Runtime works ‚Üí wrong output (‚Äúexpected 42, got 41‚Äù) ‚Üí reflect ‚Üí propose fix\nRound 4: Correct output ‚úì\n\nEach iteration surfaces the next failure mode‚Äîyou can‚Äôt discover the runtime error until the compile error is fixed. Traditional sampling generates 100 candidates that all hit the same compiler error. GEPA‚Äôs iterative reflection progresses through the failure cascade.\n\n‚ÄúIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it‚Äôs kind of like self-bootstrapping data to train itself.‚Äù ‚Äî Lakshya A Agrawal\n\nThis is why test-time GEPA can solve problems that stumped training: it has the budget to chase failure modes deeper than any fixed training run.\n\nNote: When optimizing a batch of tasks, Pareto selection ensures that fixing one problem doesn‚Äôt discard prompts that solved others‚Äîsee ‚ÄúCross-Task Transfer Within a Batch‚Äù below.\n\n\n\n\nCross-Task Transfer Within a Batch\nWhen solving related tasks (e.g., a batch of CUDA kernels), insights compound across the batch:\n\n‚ÄúAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.‚Äù ‚Äî Lakshya A Agrawal\n\nThe frontier maintains multiple specialized prompts simultaneously:\n\n\n\nPrompt\nSpecialization\nProblems solved\n\n\n\n\nP_conv\nConvolutional operators\n#1, #4, #7\n\n\nP_reduce\nReduction/summation operators\n#2, #5, #8\n\n\nP_matmul\nMatrix multiplication\n#3, #6\n\n\n\n\n‚ÄúOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.‚Äù ‚Äî Lakshya A Agrawal\n\nWhen a new problem arrives, GEPA tries candidates from across the frontier‚Äîthe convolution specialist might crack it, or the reduction specialist, or insights from both might merge.\n\n\n\nCross-Task Insight Transfer\n\n\n\n\n\nBackground Optimization Loops\nThe self-bootstrapping pattern suggests an intriguing application: background GEPA loops for personalization in tools like Cursor and other AI-assisted environments.\n\n‚ÄúFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt‚Ä¶ cursor can learn a user-specific prompt that works well specifically for you.‚Äù ‚Äî Lakshya A Agrawal\n\nEvery correction you provide‚Äîrejecting a verbose explanation, fixing a code style‚Äîbecomes a training signal that accumulates into a personalized prompt. The infrastructure isn‚Äôt widespread yet, but the pattern is clear: continuous adaptation rather than one-shot optimization.\n\n\n\nWhat GEPA Stores\nFor each task in the batch, GEPA tracks both artifacts:\n\n‚ÄúGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.‚Äù ‚Äî Lakshya A Agrawal\n\n\nBest outputs ‚Äî the actual solutions, ready to use\nBest prompts ‚Äî specialized strategies representing different subdomains of your problem space\n\nYou can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.\n\nWhen the conditions are right‚Äîrich feedback, high-value tasks worth the compute, domains where the LLM has strong priors‚Äîtest-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.\n\n\nCase Study: Code Optimization for Novel Hardware\nThe GEPA paper demonstrates test-time search on domains where GEPA‚Äôs strengths shine brightest: code optimization for hardware with limited pre-training data.\n\nAMD NPU Kernels: Optimization Without Pre-Training Knowledge\nAMD‚Äôs NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The NPUEval benchmark reveals just how challenging this is: even with compiler feedback and RAG, state-of-the-art LLMs achieve only ~10% mean vectorization score.\nGEPA‚Äôs approach doesn‚Äôt require prior examples‚Äîit iteratively generates kernels, receives compiler errors or performance metrics, reflects on feedback, and proposes targeted improvements. The compiler error messages contain the fix: ‚ÄúSymbol not found: npu_matmul‚Äù triggers reflection that surfaces the correct API.\n\n‚ÄúGEPA can be used to generate optimized kernels for AMD‚Äôs NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.‚Äù ‚Äî Lakshya A Agrawal\n\n\n\nCUDA Kernels: Outperforming Human Baselines\nKernelBench (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels. The benchmark reveals a sobering baseline: frontier reasoning models match the PyTorch baseline on less than 20% of tasks using the fast‚ÇÅ metric (correct and faster than PyTorch). Efficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven‚Äôt learned to apply reliably.\nThe KernelBench paper shows that feedback-driven refinement dramatically improves results‚Äîfast‚ÇÅ scores jumped 3-6x when execution results and profiler feedback were provided in context. GEPA applies this same principle systematically, with Pareto tracking to preserve diverse optimization strategies rather than ad-hoc iteration.\n\n‚ÄúFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.‚Äù ‚Äî Lakshya A Agrawal\n\nThe self-bootstrapping dynamic is especially effective here: each compilation error or profiler bottleneck reveals the next fix, letting GEPA progress through failure cascades that stump one-shot sampling.\n\n\nCross-Kernel Transfer\nWhen optimizing a batch of related kernels, insights compound across tasks:\n\n‚ÄúIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.‚Äù ‚Äî Lakshya A Agrawal\n\nAs GEPA optimizes each kernel, the Pareto frontier accumulates specialized prompts‚Äîone excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax. This cross-task transfer is why batch optimization outperforms solving each kernel independently."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "href": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "title": "GEPA Deepdive",
    "section": "Conclusion: When to Reach for GEPA",
    "text": "Conclusion: When to Reach for GEPA\nWe opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculus‚Äîmatching RL‚Äôs optimization performance at a fraction of the sample cost by doing something RL can‚Äôt: reading the feedback.\nThe results speak for themselves: 46.6% ‚Üí 56.6% on AIME math competition problems. 67% ‚Üí 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.\nThis represents a genuinely new point in the optimization design space‚Äîone that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. That‚Äôs not incremental improvement‚Äîit‚Äôs a 100x reduction in sample requirements.\n\n\nUse GEPA for Train-Then-Generalize When:\n\nYou have rich textual feedback (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)\nYour evaluation budget is limited (50-500 examples, not 5,000)\nYou‚Äôre optimizing compound AI systems where prompts orchestrate multi-step pipelines\nYou need interpretable results‚Äîprompts you can read, edit, and reason about\n\n\n\nUse GEPA for Test-Time Search When:\n\nYou have a batch of high-value tasks worth the compute investment\nEach task produces execution feedback (tests, profilers, validators)\nTasks are related enough for cross-task transfer to help\n\n\n\nStick with Traditional Approaches When:\n\nYou have abundant labeled data and compute budget for fine-tuning\nFeedback is purely scalar with no explanatory signal\nThe task is already solved by few-shot prompting\nYou need sub-second latency\n\n\n\n\nGet Started\nReady to try GEPA on your own pipelines?\n\nGEPA for AIME Tutorial ‚Äî Complete walkthrough from setup to optimized results\nGEPA API Reference ‚Äî Full parameter documentation\nPaper ‚Äî Algorithm details and experimental methodology\n\nThe core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads it‚Äîand learns."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "",
    "text": "Write and speak about AI-powered product, engineering and evals.\nPrototype apps like VLM Self-healing Agent; GEPA reimplementation"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "Blog",
    "text": "Blog"
  }
]