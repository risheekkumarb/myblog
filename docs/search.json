[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/itches/itches.html",
    "href": "posts/itches/itches.html",
    "title": "fastai example",
    "section": "",
    "text": "Importing Dataset\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\nimg = path.ls()[0]\nimg\n\nPath('/Users/risheekkumar/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg')\n\n\n\nimg = PILImage.create(img)\nimg\n\n\n\n\n\n\n\n\n\nimg.to_thumb(192)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "",
    "text": "Write and speak about AI-powered product, engineering and evals.\nPrototype apps like VLM Self-healing Agent; GEPA reimplementation"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Hi I am Risheekkumar, Applied Scientist",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html",
    "href": "posts/gepa-deepdive/gepa_final_article.html",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "Youâ€™ve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It worksâ€¦ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesnâ€™t scale. Youâ€™re stuck.\nBut what if you could match RLâ€™s optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) achieves exactly thisâ€”by exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, weâ€™ll unpack how it works, why modern LLMsâ€™ improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.\n\n\nTraditional AI optimization techniquesâ€”reinforcement learning and fine-tuningâ€”have achieved remarkable results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?\nConsider: - Agentic pipelines that invoke simulations, query rate-limited APIs, or run multi-step tool chains - Code generation for novel hardware, where each evaluation requires compiling for custom silicon and executing on the device - Complex reasoning tasks with expensive verification steps\nCollecting thousands of rollouts simply isnâ€™t feasible in these settings.\nThe core issue: RL learns by comparison. A 500-step trajectory collapses to reward = 0.73. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectoriesâ€”this one scored 0.8, that one scored 0.4, what differed?â€”requiring sample counts that expensive domains canâ€™t support.\n\nRL and fine-tuning require generating large amounts of rollouts to gather scalar learning signalsâ€”sample inefficient by design.\n\nWhen each rollout costs minutes (or dollars), this approach breaks down.\n\n\n\nModern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:\n\nReasoning traces â€” Chain-of-Thought and ReAct logs expose the modelâ€™s explicit thought process. When a multi-hop QA system fails, you can see where the reasoning went wrong: â€œThe capital of France is Paris. Paris is in Germanyâ€¦â€ The failure mode is visible in the text.\nEnvironment feedback â€” Compiler errors donâ€™t just say â€œfailed.â€ They say cannot find symbol 'x', did you mean 'y'? API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.\nEvaluation rubrics â€” LLM-as-judge systems donâ€™t just score 3/5. They explain: â€œResponse was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.â€\n\nEach of these is dramatically richer than reward = 0.73.\nThe key realization: these traces arenâ€™t just logs for debuggingâ€”theyâ€™re potential input to the optimizer. A compiler error that says â€œdid you mean â€˜yâ€™?â€ contains the fix. A rubric that says â€œtoo verboseâ€ specifies exactly what to change.\nTraditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.\nBut what if we could justâ€¦ read the feedback?\nThis is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?\n\n\n\nHereâ€™s the key insight enabling a new optimization paradigm: LLMs already have prior knowledge about the domains theyâ€™re working in, and theyâ€™re increasingly capable of self-reflection.\nConsider what happens with different types of feedback:\nCompiler errors â€” When the compiler returns cannot find symbol 'x', did you mean 'y'?, the LLM doesnâ€™t need thousands of examples to learn the fix. It already knows the libraryâ€™s API. One error message is enough.\n\nâ€œThe language model already knows that x is not a valid API name in the library but y is. Next time I should try this.â€ â€” Lakshya A Agrawal\n\nLLM-as-judge feedback â€” When a judge says â€œyour summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,â€ the model can directly incorporate â€œbe concise, match casual toneâ€ into its next attempt. No statistical signal extraction required.\nReasoning trace failures â€” When a multi-hop QA trace shows the model correctly retrieved â€œParis is the capital of Franceâ€ but then hallucinated â€œParis is in Germany,â€ the failure point is visible in the text. You can see exactly where the reasoning derailed.\nPrivacy-aware rewriting (PUPA task) â€” In the paperâ€™s experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains why a rewrite failedâ€”â€œleaked the userâ€™s company nameâ€ or â€œremoved too much context, degrading response qualityâ€â€”giving the optimizer actionable signal from each example.\n\n\nThis is the fundamental difference GEPA exploits:\n\n\n\n\n\n\n\nApproach\nHow it learns\n\n\n\n\nRL\nCompare thousands of trajectories statistically: â€œThese 500 scored 0.8, those 500 scored 0.4â€”what differed?â€\n\n\nReflection\nRead the feedback directly: â€œThe compiler said use y, so use y.â€\n\n\n\nRL would need hundreds of rollouts to statistically isolate that xâ†’y is the fix. The LLM gets it from one error message.\n\n\n\nBetter still, modern LLMs donâ€™t just extract point fixesâ€”they can derive generalizable lessons:\n\nNot just â€œuse y instead of xâ€ â†’ but â€œalways verify symbol names against the libraryâ€™s namespace before generating codeâ€\nNot just â€œresponse was too longâ€ â†’ but â€œfor Slack contexts, limit responses to 150 words and use bullet pointsâ€\nNot just â€œleaked company nameâ€ â†’ but â€œscan for proper nouns and replace with generic placeholdersâ€\n\n\nâ€œLLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.â€ â€” Lakshya A Agrawal\n\nThese rules get folded directly into the prompt as instructionsâ€”compounding improvements across examples rather than treating each failure in isolation.\nThis capability unlockâ€”LLMs that can genuinely reflect and generalizeâ€”is what makes GEPA viable now when it wouldnâ€™t have been two years ago.\n\n\n\n\nThis approach wasnâ€™t viable with earlier LLMs. In March 2023, roboticist Eric Jang observed that self-reflection capability â€œseems to be emergent in GPT-4 but not GPT-3.5 or Claude.â€ When asked to write a non-rhyming poem, GPT-4 produced rhymesâ€”but when prompted â€œdid the poem meet the assignment?â€ it apologized and corrected itself. GPT-3.5 and Claude couldnâ€™t recognize their errors.\nThe Reflexion paper (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4â€™s baseline 80%â€”without any weight updates. Similarly, Self-Refine showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.\nBut thereâ€™s a crucial nuance. A comprehensive 2024 survey found that pure â€œintrinsicâ€ self-correctionâ€”where the LLM reflects with no external signalâ€”rarely helps, and can even degrade performance. What does work is self-correction with reliable external feedback: compiler errors, test results, structured rubrics.\nThis is precisely what GEPA exploits. The CRITIC paper (ICLR 2024) highlights that external feedback is â€œcrucialâ€ for successful self-improvement. GEPA doesnâ€™t ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedbackâ€”the compiler said this, the profiler showed that, the judge flagged this rubricâ€”and asks it to reflect on that.\nThe capability unlock isnâ€™t â€œLLMs can now introspect perfectly.â€ Itâ€™s â€œLLMs can now process feedback and generalize lessons effectively.â€\n\nâ€œEarlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessonsâ€¦ But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.â€ â€” Lakshya A Agrawal\n\n\n\n\nTo understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessorâ€”and GEPA represents the latest capability unlock.\n\n\n\nOptimizer Evolution\n\n\n\n\nThe original insight: you donâ€™t need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as few-shot demonstrations for future runs. The system bootstraps its own examples from successful executions.\nExample: Your QA system correctly answers â€œWhatâ€™s the capital of France?â€ â†’ â€œParisâ€. That (question, answer) pair becomes a demonstration shown to the model on future queries.\nLimitation: Demonstrations are static snapshots. Once selected, they donâ€™t adapt when new failure modes emerge. And thereâ€™s no instruction optimizationâ€”the system prompt stays identical whether youâ€™re handling edge cases or common inputs.\n\n\n\nOPRO (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.\nExample: The optimizer sees: - \"Solve the math problem step by step\" â†’ score 0.65 - \"Show your work and verify the answer\" â†’ score 0.72 - \"Break the problem into cases and check each\" â†’ score 0.78\nIt proposes: \"Systematically enumerate cases and verify each solution\" â†’ score 0.81\nLimitation: Score-only signal. The optimizer sees that prompt_v3 scored 0.72 but not why. Did it make algebraic errors? Miss edge cases? The number alone doesnâ€™t say.\n\n\n\nMiPRO recognized that instructions and demonstrations interactâ€”the same instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.\nThe search space problem: Say you have 10 candidate instructions and 5 possible demo sets. Thatâ€™s 50 combinations. Now add instruction variants (â€œBe conciseâ€ vs â€œBe briefâ€ vs â€œAnswer in one sentenceâ€)â€”suddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.\nMiPROâ€™s solution: a cheap surrogate model. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you have run. The predictor learns patterns like â€œinstructions mentioning â€˜step-by-stepâ€™ tend to score higherâ€ or â€œdemos with longer reasoning traces correlate with better performance.â€\nThe workflow:\n\nBootstrap: Run a small random sample of combinations (say, 30 out of 500)\nTrain surrogate: Fit the predictor on those 30 (instruction, demos) â†’ score pairs\nPredict cheaply: Score all 500 combinations using the surrogate (milliseconds, not dollars)\nEvaluate selectively: Only run full evaluation on the top-predicted candidates\nRepeat: Add new results to training data, retrain surrogate, sample again\n\nExample: After 30 random evaluations, the surrogate learns: - Instructions with â€œstep-by-stepâ€ â†’ +0.08 average - Demo set B (which has chain-of-thought examples) â†’ +0.05 average - Combining both â†’ predicted 0.79\nMiPRO focuses budget on high-predicted combinations rather than exhaustive search.\nLimitation: The surrogate learns correlations, not causation. It knows â€œstep-by-step instructions score higherâ€ but not whyâ€”maybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt thatâ€™s 0.75 on everything beats one thatâ€™s 0.95 on hard cases but 0.60 overallâ€”even though that hard-case specialist might contain crucial insights.\n\n\n\nSIMBA reframes prompt optimization as a multi-armed bandit problemâ€”a classic framework for sequential decision-making under uncertainty.\nThe bandit analogy: Imagine youâ€™re in a casino with 100 slot machines. Each has a different (unknown) payout rate. You have 50 tokens. How do you maximize winnings?\n\nPure exploitation: Find one machine that seems good, play it 50 times. Problem: maybe you got lucky earlyâ€”another machine is actually better.\nPure exploration: Try each machine once, thenâ€¦ youâ€™re out of tokens before you learn anything useful.\nSmart balance: Track your uncertainty about each machine. Play machines where youâ€™re uncertain (might be great!) more than machines youâ€™re confident are mediocre.\n\nSIMBA applies this to prompts. Each candidate prompt is a â€œslot machine.â€ Each evaluation is a â€œpull.â€ The score is the â€œpayout.â€\nHow it works:\n\nInitialize: Start with a pool of candidate prompts (maybe generated by an LLM or hand-written)\nTrack statistics: For each prompt, maintain: average score so far, number of times evaluated, and a confidence interval (range of plausible true scores)\nSample strategically: Use Upper Confidence Bound (UCB) to pick which prompt to evaluate nextâ€”favoring prompts with high uncertainty OR high average\nUpdate beliefs: After evaluation, narrow the confidence interval for that prompt\nRepeat: Eventually, confidence intervals separateâ€”you know which prompts are best\n\nExample in action: You have 20 candidate prompts, budget for 50 evaluations.\n\n\n\nPrompt\nEvaluations\nAvg Score\nConfidence Interval\n\n\n\n\nA\n8\n0.74\n[0.70, 0.78]\n\n\nB\n2\n0.71\n[0.55, 0.87]\n\n\nC\n5\n0.68\n[0.62, 0.74]\n\n\n\nWhich to evaluate next? - Prompt A: probably ~0.74, weâ€™re confident - Prompt B: could be 0.55 (bad) or 0.87 (best!)â€”high uncertainty - Prompt C: probably ~0.68, weâ€™re fairly confident itâ€™s worse than A\nSIMBA picks Prompt Bâ€”the uncertainty is valuable. If B turns out great, we found a winner. If bad, weâ€™ve ruled it out cheaply.\nWhy this beats random search: Random would waste evaluations on prompts we already know are bad. SIMBA focuses budget on decisions that matterâ€”resolving uncertainty between plausibly-good candidates.\nLimitation: SIMBA efficiently finds the best prompt but doesnâ€™t understand why it works. The bandit framework treats prompts as black boxes with hidden payout ratesâ€”it canâ€™t reason about â€œthis prompt works because it specifies output format.â€ And like MiPRO, it optimizes aggregate score: a prompt scoring 0.75 uniformly beats one scoring 0.95 on hard cases but 0.60 elsewhereâ€”even if the hard-case specialist contains insights worth preserving.\n\n\n\nGEPA breaks from this trajectory in two fundamental ways:\n\n\n\n\n\n\n\n\nWhat changed\nBefore GEPA\nWith GEPA\n\n\n\n\nLearning signal\nscore = 0.6\nâ€œExceeded word limit. Missing keyword. Compiler error: use y not x.â€\n\n\nSelection strategy\nBest aggregate score\nPareto frontier of diverse specialists\n\n\n\n1. From scalar scores to textual feedback â€” Instead of just knowing that a prompt scored 0.6, GEPA sees why: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.\nExample: OPRO sees score = 0.6. GEPA sees: &gt; â€œFailed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword â€˜disclaimerâ€™. Passed examples 1-6, 8-11.â€\nThe LLM reflects: â€œI should add an instruction about word limits and required keywords.â€\n2. From greedy to Pareto selection â€” Instead of always promoting the highest-scoring candidate, GEPA maintains a Pareto frontier: candidates that each excel at something no other candidate beats.\nExample: Three candidates evaluated on 10 examples: - prompt_A: 8/10 overall, but fails hard cases #7 and #9 - prompt_B: 6/10 overall, but nails hard cases #7 and #9\n- prompt_C: 7/10 overall, no unique strengths\nGreedy selection keeps only prompt_A. Pareto selection keeps both prompt_A and prompt_Bâ€”because Bâ€™s insights about hard cases might combine with Aâ€™s general strength. prompt_C gets dropped (dominated by A on everything).\nThe contrast with prior optimizers is stark: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains whyâ€”and proposes the fix directly."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#introduction-the-case-for-sample-efficient-prompt-optimization",
    "href": "posts/gepa-deepdive/gepa_final_article.html#introduction-the-case-for-sample-efficient-prompt-optimization",
    "title": "GEPA Deepdive",
    "section": "",
    "text": "Youâ€™ve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It worksâ€¦ 70% of the time. You have 50 labeled examples. Now what?\nFine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesnâ€™t scale. Youâ€™re stuck.\nBut what if you could match RLâ€™s optimization performance using 50 examples instead of 5,000?\nGEPA (Genetic-Pareto) achieves exactly thisâ€”by exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM reflect on its own failures and propose improvements directly.\nIn this post, weâ€™ll unpack how it works, why modern LLMsâ€™ improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.\n\n\nTraditional AI optimization techniquesâ€”reinforcement learning and fine-tuningâ€”have achieved remarkable results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?\nConsider: - Agentic pipelines that invoke simulations, query rate-limited APIs, or run multi-step tool chains - Code generation for novel hardware, where each evaluation requires compiling for custom silicon and executing on the device - Complex reasoning tasks with expensive verification steps\nCollecting thousands of rollouts simply isnâ€™t feasible in these settings.\nThe core issue: RL learns by comparison. A 500-step trajectory collapses to reward = 0.73. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectoriesâ€”this one scored 0.8, that one scored 0.4, what differed?â€”requiring sample counts that expensive domains canâ€™t support.\n\nRL and fine-tuning require generating large amounts of rollouts to gather scalar learning signalsâ€”sample inefficient by design.\n\nWhen each rollout costs minutes (or dollars), this approach breaks down.\n\n\n\nModern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:\n\nReasoning traces â€” Chain-of-Thought and ReAct logs expose the modelâ€™s explicit thought process. When a multi-hop QA system fails, you can see where the reasoning went wrong: â€œThe capital of France is Paris. Paris is in Germanyâ€¦â€ The failure mode is visible in the text.\nEnvironment feedback â€” Compiler errors donâ€™t just say â€œfailed.â€ They say cannot find symbol 'x', did you mean 'y'? API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.\nEvaluation rubrics â€” LLM-as-judge systems donâ€™t just score 3/5. They explain: â€œResponse was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.â€\n\nEach of these is dramatically richer than reward = 0.73.\nThe key realization: these traces arenâ€™t just logs for debuggingâ€”theyâ€™re potential input to the optimizer. A compiler error that says â€œdid you mean â€˜yâ€™?â€ contains the fix. A rubric that says â€œtoo verboseâ€ specifies exactly what to change.\nTraditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.\nBut what if we could justâ€¦ read the feedback?\nThis is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?\n\n\n\nHereâ€™s the key insight enabling a new optimization paradigm: LLMs already have prior knowledge about the domains theyâ€™re working in, and theyâ€™re increasingly capable of self-reflection.\nConsider what happens with different types of feedback:\nCompiler errors â€” When the compiler returns cannot find symbol 'x', did you mean 'y'?, the LLM doesnâ€™t need thousands of examples to learn the fix. It already knows the libraryâ€™s API. One error message is enough.\n\nâ€œThe language model already knows that x is not a valid API name in the library but y is. Next time I should try this.â€ â€” Lakshya A Agrawal\n\nLLM-as-judge feedback â€” When a judge says â€œyour summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,â€ the model can directly incorporate â€œbe concise, match casual toneâ€ into its next attempt. No statistical signal extraction required.\nReasoning trace failures â€” When a multi-hop QA trace shows the model correctly retrieved â€œParis is the capital of Franceâ€ but then hallucinated â€œParis is in Germany,â€ the failure point is visible in the text. You can see exactly where the reasoning derailed.\nPrivacy-aware rewriting (PUPA task) â€” In the paperâ€™s experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains why a rewrite failedâ€”â€œleaked the userâ€™s company nameâ€ or â€œremoved too much context, degrading response qualityâ€â€”giving the optimizer actionable signal from each example.\n\n\nThis is the fundamental difference GEPA exploits:\n\n\n\n\n\n\n\nApproach\nHow it learns\n\n\n\n\nRL\nCompare thousands of trajectories statistically: â€œThese 500 scored 0.8, those 500 scored 0.4â€”what differed?â€\n\n\nReflection\nRead the feedback directly: â€œThe compiler said use y, so use y.â€\n\n\n\nRL would need hundreds of rollouts to statistically isolate that xâ†’y is the fix. The LLM gets it from one error message.\n\n\n\nBetter still, modern LLMs donâ€™t just extract point fixesâ€”they can derive generalizable lessons:\n\nNot just â€œuse y instead of xâ€ â†’ but â€œalways verify symbol names against the libraryâ€™s namespace before generating codeâ€\nNot just â€œresponse was too longâ€ â†’ but â€œfor Slack contexts, limit responses to 150 words and use bullet pointsâ€\nNot just â€œleaked company nameâ€ â†’ but â€œscan for proper nouns and replace with generic placeholdersâ€\n\n\nâ€œLLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.â€ â€” Lakshya A Agrawal\n\nThese rules get folded directly into the prompt as instructionsâ€”compounding improvements across examples rather than treating each failure in isolation.\nThis capability unlockâ€”LLMs that can genuinely reflect and generalizeâ€”is what makes GEPA viable now when it wouldnâ€™t have been two years ago.\n\n\n\n\nThis approach wasnâ€™t viable with earlier LLMs. In March 2023, roboticist Eric Jang observed that self-reflection capability â€œseems to be emergent in GPT-4 but not GPT-3.5 or Claude.â€ When asked to write a non-rhyming poem, GPT-4 produced rhymesâ€”but when prompted â€œdid the poem meet the assignment?â€ it apologized and corrected itself. GPT-3.5 and Claude couldnâ€™t recognize their errors.\nThe Reflexion paper (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4â€™s baseline 80%â€”without any weight updates. Similarly, Self-Refine showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.\nBut thereâ€™s a crucial nuance. A comprehensive 2024 survey found that pure â€œintrinsicâ€ self-correctionâ€”where the LLM reflects with no external signalâ€”rarely helps, and can even degrade performance. What does work is self-correction with reliable external feedback: compiler errors, test results, structured rubrics.\nThis is precisely what GEPA exploits. The CRITIC paper (ICLR 2024) highlights that external feedback is â€œcrucialâ€ for successful self-improvement. GEPA doesnâ€™t ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedbackâ€”the compiler said this, the profiler showed that, the judge flagged this rubricâ€”and asks it to reflect on that.\nThe capability unlock isnâ€™t â€œLLMs can now introspect perfectly.â€ Itâ€™s â€œLLMs can now process feedback and generalize lessons effectively.â€\n\nâ€œEarlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessonsâ€¦ But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.â€ â€” Lakshya A Agrawal\n\n\n\n\nTo understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessorâ€”and GEPA represents the latest capability unlock.\n\n\n\nOptimizer Evolution\n\n\n\n\nThe original insight: you donâ€™t need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as few-shot demonstrations for future runs. The system bootstraps its own examples from successful executions.\nExample: Your QA system correctly answers â€œWhatâ€™s the capital of France?â€ â†’ â€œParisâ€. That (question, answer) pair becomes a demonstration shown to the model on future queries.\nLimitation: Demonstrations are static snapshots. Once selected, they donâ€™t adapt when new failure modes emerge. And thereâ€™s no instruction optimizationâ€”the system prompt stays identical whether youâ€™re handling edge cases or common inputs.\n\n\n\nOPRO (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.\nExample: The optimizer sees: - \"Solve the math problem step by step\" â†’ score 0.65 - \"Show your work and verify the answer\" â†’ score 0.72 - \"Break the problem into cases and check each\" â†’ score 0.78\nIt proposes: \"Systematically enumerate cases and verify each solution\" â†’ score 0.81\nLimitation: Score-only signal. The optimizer sees that prompt_v3 scored 0.72 but not why. Did it make algebraic errors? Miss edge cases? The number alone doesnâ€™t say.\n\n\n\nMiPRO recognized that instructions and demonstrations interactâ€”the same instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.\nThe search space problem: Say you have 10 candidate instructions and 5 possible demo sets. Thatâ€™s 50 combinations. Now add instruction variants (â€œBe conciseâ€ vs â€œBe briefâ€ vs â€œAnswer in one sentenceâ€)â€”suddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.\nMiPROâ€™s solution: a cheap surrogate model. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you have run. The predictor learns patterns like â€œinstructions mentioning â€˜step-by-stepâ€™ tend to score higherâ€ or â€œdemos with longer reasoning traces correlate with better performance.â€\nThe workflow:\n\nBootstrap: Run a small random sample of combinations (say, 30 out of 500)\nTrain surrogate: Fit the predictor on those 30 (instruction, demos) â†’ score pairs\nPredict cheaply: Score all 500 combinations using the surrogate (milliseconds, not dollars)\nEvaluate selectively: Only run full evaluation on the top-predicted candidates\nRepeat: Add new results to training data, retrain surrogate, sample again\n\nExample: After 30 random evaluations, the surrogate learns: - Instructions with â€œstep-by-stepâ€ â†’ +0.08 average - Demo set B (which has chain-of-thought examples) â†’ +0.05 average - Combining both â†’ predicted 0.79\nMiPRO focuses budget on high-predicted combinations rather than exhaustive search.\nLimitation: The surrogate learns correlations, not causation. It knows â€œstep-by-step instructions score higherâ€ but not whyâ€”maybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt thatâ€™s 0.75 on everything beats one thatâ€™s 0.95 on hard cases but 0.60 overallâ€”even though that hard-case specialist might contain crucial insights.\n\n\n\nSIMBA reframes prompt optimization as a multi-armed bandit problemâ€”a classic framework for sequential decision-making under uncertainty.\nThe bandit analogy: Imagine youâ€™re in a casino with 100 slot machines. Each has a different (unknown) payout rate. You have 50 tokens. How do you maximize winnings?\n\nPure exploitation: Find one machine that seems good, play it 50 times. Problem: maybe you got lucky earlyâ€”another machine is actually better.\nPure exploration: Try each machine once, thenâ€¦ youâ€™re out of tokens before you learn anything useful.\nSmart balance: Track your uncertainty about each machine. Play machines where youâ€™re uncertain (might be great!) more than machines youâ€™re confident are mediocre.\n\nSIMBA applies this to prompts. Each candidate prompt is a â€œslot machine.â€ Each evaluation is a â€œpull.â€ The score is the â€œpayout.â€\nHow it works:\n\nInitialize: Start with a pool of candidate prompts (maybe generated by an LLM or hand-written)\nTrack statistics: For each prompt, maintain: average score so far, number of times evaluated, and a confidence interval (range of plausible true scores)\nSample strategically: Use Upper Confidence Bound (UCB) to pick which prompt to evaluate nextâ€”favoring prompts with high uncertainty OR high average\nUpdate beliefs: After evaluation, narrow the confidence interval for that prompt\nRepeat: Eventually, confidence intervals separateâ€”you know which prompts are best\n\nExample in action: You have 20 candidate prompts, budget for 50 evaluations.\n\n\n\nPrompt\nEvaluations\nAvg Score\nConfidence Interval\n\n\n\n\nA\n8\n0.74\n[0.70, 0.78]\n\n\nB\n2\n0.71\n[0.55, 0.87]\n\n\nC\n5\n0.68\n[0.62, 0.74]\n\n\n\nWhich to evaluate next? - Prompt A: probably ~0.74, weâ€™re confident - Prompt B: could be 0.55 (bad) or 0.87 (best!)â€”high uncertainty - Prompt C: probably ~0.68, weâ€™re fairly confident itâ€™s worse than A\nSIMBA picks Prompt Bâ€”the uncertainty is valuable. If B turns out great, we found a winner. If bad, weâ€™ve ruled it out cheaply.\nWhy this beats random search: Random would waste evaluations on prompts we already know are bad. SIMBA focuses budget on decisions that matterâ€”resolving uncertainty between plausibly-good candidates.\nLimitation: SIMBA efficiently finds the best prompt but doesnâ€™t understand why it works. The bandit framework treats prompts as black boxes with hidden payout ratesâ€”it canâ€™t reason about â€œthis prompt works because it specifies output format.â€ And like MiPRO, it optimizes aggregate score: a prompt scoring 0.75 uniformly beats one scoring 0.95 on hard cases but 0.60 elsewhereâ€”even if the hard-case specialist contains insights worth preserving.\n\n\n\nGEPA breaks from this trajectory in two fundamental ways:\n\n\n\n\n\n\n\n\nWhat changed\nBefore GEPA\nWith GEPA\n\n\n\n\nLearning signal\nscore = 0.6\nâ€œExceeded word limit. Missing keyword. Compiler error: use y not x.â€\n\n\nSelection strategy\nBest aggregate score\nPareto frontier of diverse specialists\n\n\n\n1. From scalar scores to textual feedback â€” Instead of just knowing that a prompt scored 0.6, GEPA sees why: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.\nExample: OPRO sees score = 0.6. GEPA sees: &gt; â€œFailed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword â€˜disclaimerâ€™. Passed examples 1-6, 8-11.â€\nThe LLM reflects: â€œI should add an instruction about word limits and required keywords.â€\n2. From greedy to Pareto selection â€” Instead of always promoting the highest-scoring candidate, GEPA maintains a Pareto frontier: candidates that each excel at something no other candidate beats.\nExample: Three candidates evaluated on 10 examples: - prompt_A: 8/10 overall, but fails hard cases #7 and #9 - prompt_B: 6/10 overall, but nails hard cases #7 and #9\n- prompt_C: 7/10 overall, no unique strengths\nGreedy selection keeps only prompt_A. Pareto selection keeps both prompt_A and prompt_Bâ€”because Bâ€™s insights about hard cases might combine with Aâ€™s general strength. prompt_C gets dropped (dominated by A on everything).\nThe contrast with prior optimizers is stark: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains whyâ€”and proposes the fix directly."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "href": "posts/gepa-deepdive/gepa_final_article.html#how-gepa-works-building-it-from-scratch",
    "title": "GEPA Deepdive",
    "section": "How GEPA Works: Building It From Scratch",
    "text": "How GEPA Works: Building It From Scratch\nGEPA combines two key innovations: reflective prompt mutation (learning from textual feedback) and Pareto selection (preserving diverse specialists). Each is powerful alone; together they compound.\nWeâ€™ll build them from scratch in this section:\n\nReflective mutation â€” How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.\nPareto selection â€” Why always improving your â€œbestâ€ prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.\nMerge â€” How GEPA combines insights from divergent lineages (covered after the complete algorithm).\n\nBy the end, youâ€™ll see how these mechanisms combine into GEPAâ€™s full evolutionary loop.\n\n\nğŸš€ Quick Start: Using GEPA in 30 Seconds\n\nğŸ“– Full tutorial: GEPA for AIME (Math) â€” optimizing GPT-4.1 Mini from 46.6% â†’ 56.6% on AIME 2025.\n\nBefore diving deep, hereâ€™s what using GEPA looks like in practice:\nStep 1: Configure your language model\nimport dspy\n\nlm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, max_tokens=32000)\ndspy.configure(lm=lm)\nStep 2: Define your program\nprogram = dspy.ChainOfThought(\"problem -&gt; answer\")\nStep 3: Define a metric that returns feedback (not just a score)\nThis is the key difference from other optimizersâ€”your metric explains why something failed:\ndef metric_with_feedback(example, prediction, trace=None, **kwargs):\n    correct_answer = example.answer\n    pred_answer = prediction.answer\n    \n    score = int(correct_answer == pred_answer)\n    \n    if score == 1:\n        feedback = f\"Correct! The answer is '{correct_answer}'.\"\n    else:\n        feedback = f\"Incorrect. Expected '{correct_answer}', got '{pred_answer}'.\"\n        # Add any additional context that could help improvement:\n        if hasattr(example, 'solution'):\n            feedback += f\" Solution: {example.solution}\"\n    \n    return dspy.Prediction(score=score, feedback=feedback)\nStep 4: Optimize with GEPA\nfrom dspy import GEPA\n\noptimizer = GEPA(\n    metric=metric_with_feedback,\n    auto=\"light\",           # Budget preset: \"light\", \"medium\", or \"heavy\"\n    num_threads=32,         # Parallel evaluation threads\n)\n\noptimized_program = optimizer.compile(\n    program,\n    trainset=train_set,\n    valset=val_set,\n)\nThatâ€™s it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.\n\nWhatâ€™s happening under the hood?\n\n\n\n\n\n\n\nComponent\nWhat it does\n\n\n\n\nTextual feedback\nYour metric returns why something failed, not just a score\n\n\nReflective mutation\nAn LLM reads the feedback and proposes improved instructions\n\n\nPareto selection\nDiverse specialists are preserved, not just the â€œbestâ€ prompt\n\n\nMerge operations\nInsights from divergent lineages get combined\n\n\n\nThe rest of this section explains why each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you needâ€”see the DSPy GEPA API Reference for full parameter details.\nNow letâ€™s make this concrete by building the core mechanism from scratch.\n\n\nHands-On: Building Reflective Mutation from Scratch\nLetâ€™s make this concrete by implementing GEPAâ€™s core mechanism on a real task.\nThe Problem: AIME Math Competition\nWeâ€™ll optimize prompts for solving AIME (American Invitational Mathematics Examination) problems â€” challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.\nfrom datasets import load_dataset\ndset = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n# 90 problems with solutions and integer answers\nWhy AIME for testing prompt optimization?\n\nClear ground truth â€” Every answer is an integer (0-999), so evaluation is unambiguous\nRich failure modes â€” Wrong answers come from algebraic errors, missed cases, misread constraints\nDomain knowledge helps â€” Prompts that encode strategies (â€œsubtract equations pairwiseâ€, â€œenumerate all casesâ€) measurably improve performance\nSmall dataset â€” Only 90 problems, so sample efficiency matters\n\nThe Setup\n# Split: 10 train, 10 validation (simulating scarce labeled data)\ntdf = df.sample(45).iloc[:10]  # Training mini-batches drawn from here\nvdf = df.drop(tdf.index).iloc[:10]  # Held-out validation\n\n# Base model: Gemini 2.5 Flash via LiteLLM\n# Metric: Exact match (predicted integer == ground truth)\ndef metric(ground_truth, prediction):\n    return int(ground_truth) == prediction['answer']\nSeed Prompt\nWe start with a minimal instruction:\nseed_prompt = \"\"\"You are given a problem and you have to give the answer \nalong with reasoning. Do not return anything apart from json. \nIt should be parsable by json.loads()\"\"\"\nBaseline validation accuracy: 10% (1/10 correct)\nCan reflective mutation improve this? Letâ€™s find out.\n\n\n\nStep 1: The Feedback Function\nFirst, we need a function that tells the reflection LLM what went wrong. Weâ€™ll start with minimal feedbackâ€”just the correct answer:\ndef feedback(ground_truth, prediction):\n    if int(ground_truth) != prediction['answer']:\n        return f'You got it wrong! The solution is {ground_truth}'\n    return 'You got it right!'\nThis is deliberately simple. Later weâ€™ll discuss how richer feedback (like expert solutions) can improve results.\n\n\n\nStep 2: The Reflection Prompt\nFollowing GEPAâ€™s structure, we build a prompt that shows the LLM its failures:\nREFLECTION_TEMPLATE = \"\"\"I provided an assistant with the following instructions:\n&lt;curr_instructions&gt;\n{current_prompt}\n\nThe following are examples with assistant's responses and feedback:\n&lt;inputs_outputs_feedback&gt;\n{examples}\n\nYour task: write a new instruction for the assistant.\n\n- Read inputs carefully and identify the input format and task description\n- Read all responses and feedback. Identify niche/domain-specific factual information\n- If the assistant used a generalizable strategy, include that in the instruction\n\nProvide the new instructions.\n\"\"\"\n\ndef mk_reflection_prompt(df, curr_prompt):\n    \"\"\"Build reflection prompt from minibatch results.\"\"\"\n    examples = []\n    for i, row in df.reset_index().iterrows():\n        example = f\"\"\"# Example {i+1}\n## problem\n{row['problem']}\n## prediction\n{row['pred']}\n## feedback\n{feedback(row.answer, row.pred)}\n\"\"\"\n        examples.append(example)\n    \n    return REFLECTION_TEMPLATE.format(\n        current_prompt=curr_prompt,\n        examples=\"\\n\".join(examples)\n    )\n\n\n\nStep 3: The Complete Optimization Loop\ndef reflect(mb, curr_prompt):\n    \"\"\"Ask LLM to reflect on failures and propose improved instruction.\"\"\"\n    refl_prompt = mk_reflection_prompt(mb, curr_prompt)\n    return _call(refl_prompt, format=ReflectionModel)['new_instruction']\n\ndef optimize_prompt(seed_prompt, traindf, valdf, n_iters=3, mb_size=3):\n    \"\"\"Greedy reflective prompt optimization.\"\"\"\n    prompts, train_scores, val_scores = [seed_prompt], [], []\n    mb = traindf.sample(mb_size)  # Fixed minibatch for this run\n    \n    print(f'Baseline validation: {eval_val(seed_prompt, valdf):.2%}')\n    \n    for i in range(n_iters):\n        # Evaluate current prompt on minibatch\n        mb_eval, mb_score = eval_mb(prompts[-1], mb)\n        print(f\"ğŸ“Š Minibatch: {mb_score:.2%}\")\n        \n        # Reflect and propose new instruction\n        new_instr = reflect(mb_eval, prompts[-1])\n        new_prompt = new_instr  # The new instruction becomes the new prompt\n        \n        # Evaluate on validation set\n        val_score = eval_val(new_prompt, valdf)\n        print(f\"ğŸ“Š Validation: {val_score:.2%}\")\n        \n        prompts.append(new_prompt)\n        val_scores.append(val_score)\n    \n    return dict(prompts=prompts, val_scores=val_scores)\n\n\n\nWhat Actually Happened\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\n\n\n\n\n\n\nIteration\nMinibatch\nValidation\nWhat the reflection learned\n\n\n\n\nBaseline\nâ€”\n10%\nâ€”\n\n\n1\n0%\n10%\nJSON formatting details, output structure rules\n\n\n2\n0%\n30%\nSystems of equations strategy, remainder/modular arithmetic tips\n\n\n3\n67%\n10%\nOver-specialized on number theory, solved #9 but lost generality\n\n\n\nThe good: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:\n\nâ€œWhen dealing with systems of equations like \\(xy + Az = C\\), \\(yz + Ax = C\\), \\(zx + Ay = C\\), consider subtracting equations pairwise to find relationships between variables, such as \\((x-z)(y-A)=0\\), which implies \\(x=z\\) or \\(y=A\\). Systematically explore all such cases.â€\n\nThis is directly from the actual outputâ€”the reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.\nThe bad: Iteration 3 achieved 67% on its minibatch but dropped to 10% validation. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:\n\nâ€œWhen the problem involves number theory and remainders (e.g., \\(n \\pmod x\\), \\(n \\pmod y\\)), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.â€\n\nThis over-specialized advice (â€œcan lead to an answer of 0â€) actively hurt performance on non-number-theory problems, dropping validation from 30% back to 10% (1/10)â€”though notably, it did solve problem #9, which earlier prompts couldnâ€™t.\n\n\n\nThe Greedy Selection Problem\nThis demonstrates exactly why GEPA uses Pareto selection instead of always taking the â€œbestâ€ prompt:\n\nIteration 2â€™s prompt was a specialistâ€”it learned something valuable about systems of equations\nIteration 3 tried to improve on iteration 2, but the minibatch had different problems\nThe reflection overwrote the systems-of-equations insight while adding number-theory tips that were too specific\nResult: catastrophic forgetting\n\nWith greedy selection, we would have discarded iteration 2â€™s valuable insight. Pareto selection would keep itâ€”because it was best on at least one validation instance.\n\n\n\nThe Missing Ingredient: Rich Feedback\nOur minimal feedback (\"You got it wrong! The solution is 349\") only tells the model that it failed, not why or how to fix it.\nThe AIME dataset includes expert solutions. A richer feedback function could use them:\ndef feedback_rich(row):\n    if int(row.answer) != row.pred['answer']:\n        sol = row.solution[:500] + \"...\" if len(row.solution) &gt; 500 else row.solution\n        return f\"\"\"Wrong! Expected {row.answer}, got {row.pred['answer']}.\n        \nModel's reasoning: {row.pred['short_reasoning']}\n\nExpert solution approach:\n{sol}\"\"\"\n    return \"Correct!\"\nWith rich feedback, the reflection LLM can extract specific strategies from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: the feedback contains the fix.\nCompare this to RL, which would only see reward = 0 and have to statistically infer what went wrong across thousands of trajectories.\n\n\n\nKey Takeaways\n\nReflective mutation works â€” Even with minimal feedback, the LLM extracted useful domain knowledge\nGreedy selection fails â€” Iteration 3â€™s collapse shows why we need to preserve specialist insights\nFeedback quality matters â€” Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with\nSample efficiency is real â€” We saw meaningful optimization with just 3 iterations on 3 examples each\n\nThis is the core limitation of greedy optimization: catastrophic forgetting. The solution? Pareto selectionâ€”which weâ€™ll build from scratch next.\n\n\nHands-On: Building the Pareto Frontier\nIn the reflective mutation section, we saw greedy selection failâ€”iteration 3â€™s over-specialized prompt dropped validation from 30% to 10%, losing iteration 2â€™s valuable systems-of-equations insights. The fundamental problem: always improving your â€œbestâ€ prompt discards specialist knowledge.\nGEPAâ€™s solution: Pareto selection. Instead of keeping one best prompt, maintain a frontier of prompts where each excels at something no other prompt beats.\n\n\nWhat is Pareto Dominance?\nA prompt dominates another if itâ€™s at least as good everywhere, and strictly better somewhere:\n\nâ‰¥ on every validation instance, AND\n\n&gt; on at least one instance\n\nIf prompt A dominates prompt B, we can safely discard Bâ€”A is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.\nExample: Consider four prompts evaluated on 10 validation instances. Weâ€™ll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:\n\n\n\nPrompt\nInstances Solved\nAggregate\nStatus\n\n\n\n\nP0 (seed)\n#0 only\n10%\nDominated by P2\n\n\nP1 (iter 1)\n#0 only\n10%\nDominated by P2\n\n\nP2 (iter 2)\n#0, #1, #2\n30%\nFrontier âœ“\n\n\nP3 (iter 3)\n#9 only\n10%\nFrontier âœ“\n\n\n\nP2 dominates both P0 and P1â€”it solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which nothing else could.\nThe Pareto frontier is {P2, P3}. Both contain unique value.\n\n\n\nImplementation: Dominance Checking\nWe represent per-instance scores as boolean arrays (1 = solved, 0 = failed):\nimport numpy as np\n\ndef dominates(candidate_scores, other_scores):\n    \"\"\"Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)\"\"\"\n    candidate = np.array(candidate_scores)\n    other = np.array(other_scores)\n    return (candidate &gt;= other).all() and (candidate &gt; other).any()\n\ndef is_dominated_by_any(new_scores, frontier_scores):\n    \"\"\"Is new_scores dominated by ANY prompt in the frontier?\"\"\"\n    new = np.array(new_scores)\n    for existing in frontier_scores:\n        if dominates(np.array(existing), new):\n            return True\n    return False\n\ndef get_dominated_indices(new_scores, frontier_scores):\n    \"\"\"Which frontier prompts does new_scores dominate?\"\"\"\n    new = np.array(new_scores)\n    return [i for i, existing in enumerate(frontier_scores) \n            if dominates(new, np.array(existing))]\n\n\n\nTracing Through: Why P3 Survives\nLetâ€™s verify the dominance relationships from our example:\nP0 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP1 = [1,0,0,0,0,0,0,0,0,0]  # Solves: #0\nP2 = [1,1,1,0,0,0,0,0,0,0]  # Solves: #0, #1, #2\nP3 = [0,0,0,0,0,0,0,0,0,1]  # Solves: #9\n\n# Does P2 dominate P0?\ndominates(P2, P0)  # True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2\n\n# Does P2 dominate P1?\ndominates(P2, P1)  # True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2\n\n# Does P2 dominate P3?\ndominates(P2, P3)  # False! P2 loses on #9 (0 &lt; 1)\n\n# Does P3 dominate P2?\ndominates(P3, P2)  # False! P3 loses on #0, #1, #2\nNeither P2 nor P3 dominates the otherâ€”theyâ€™re Pareto incomparable. Each solves problems the other canâ€™t. Both stay on the frontier.\n\n\n\nPareto Frontier Explained\n\n\n\n\n\nThe Complete Frontier Manager\nclass ParetoFrontier:\n    def __init__(self):\n        self.prompts = []\n        self.scores = []  # scores[i][j] = prompt i's score on instance j\n    \n    def add(self, prompt, instance_scores):\n        \"\"\"Try to add a prompt. Returns True if it joins the frontier.\"\"\"\n        # Reject if dominated by existing frontier member\n        if is_dominated_by_any(instance_scores, self.scores):\n            return False\n        \n        # Remove any frontier members this prompt dominates\n        dominated = get_dominated_indices(instance_scores, self.scores)\n        for i in sorted(dominated, reverse=True):  # Remove from end first\n            del self.prompts[i]\n            del self.scores[i]\n        \n        # Add to frontier\n        self.prompts.append(prompt)\n        self.scores.append(instance_scores)\n        return True\n    \n    def sample(self):\n        \"\"\"Sample a prompt, weighted by unique wins.\"\"\"\n        weights = []\n        scores_arr = np.array(self.scores)\n        for i in range(len(self.prompts)):\n            # How many instances is this prompt *uniquely* best on?\n            others_best = np.delete(scores_arr, i, axis=0).max(axis=0) if len(self.prompts) &gt; 1 else np.zeros_like(scores_arr[i])\n            unique_wins = (scores_arr[i] &gt; others_best).sum()\n            weights.append(unique_wins + 1)  # +1 smoothing\n        \n        return np.random.choice(self.prompts, p=np.array(weights)/sum(weights))\n    \n    def best_aggregate(self):\n        \"\"\"Return the prompt with highest aggregate score.\"\"\"\n        aggregates = [sum(s) for s in self.scores]\n        return self.prompts[np.argmax(aggregates)]\n\n\n\nPutting It Together: Pareto-Guided Optimization\ndef optimize_with_pareto(seed_prompt, traindf, valdf, n_iters=5, mb_size=3):\n    \"\"\"Reflective mutation with Pareto frontier selection.\"\"\"\n    frontier = ParetoFrontier()\n    \n    # Initialize with seed\n    seed_scores = evaluate_per_instance(seed_prompt, valdf)\n    frontier.add(seed_prompt, seed_scores)\n    print(f'Baseline: {sum(seed_scores)/len(seed_scores):.1%}')\n    \n    for i in range(n_iters):\n        print(f\"\\n{'='*40}\\nIteration {i+1}\\n{'='*40}\")\n        \n        # Sample parent from frontier (weighted by unique wins)\n        parent = frontier.sample()\n        \n        # Run on minibatch, reflect, propose mutation\n        mb = traindf.sample(mb_size)\n        mb_results = evaluate_with_traces(parent, mb)\n        new_prompt = reflect_and_mutate(parent, mb_results)\n        \n        # Evaluate on full validation set\n        new_scores = evaluate_per_instance(new_prompt, valdf)\n        new_agg = sum(new_scores) / len(new_scores)\n        print(f\"New prompt: {new_agg:.1%} aggregate\")\n        \n        # Try to add to frontier\n        if frontier.add(new_prompt, new_scores):\n            print(f\"âœ“ Added to frontier (size: {len(frontier.prompts)})\")\n        else:\n            print(f\"âœ— Dominated, rejected\")\n    \n    return frontier.best_aggregate()\n\n\n\nWhat We Observed on AIME\nRunning this on AIME problems with Gemini 2.5 Flash:\n\n\n\nIteration\nAggregate\nInstances Solved\nFrontier Action\n\n\n\n\nSeed\n10%\n#0\nInitialize\n\n\n1\n10%\n#0\nDominated by seed, rejected\n\n\n2\n30%\n#0, #1, #2\nAdded, dominates seed\n\n\n3\n10%\n#9 only\nAdded âœ“ (unique win on #9)\n\n\n\nThe key moment: Iteration 3 scored only 10%â€”worse than iteration 2â€™s 30%. Greedy selection would discard it entirely.\nBut it solved instance #9, which nothing else could. Pareto selection preserves it.\nOur final frontier: {P2, P3} - P2: Strong generalist (30%), knows systems-of-equations strategies - P3: Instance-9 specialist (10%), knows whatever cracked that specific problem\nBoth insights survive. The merge operation (covered later) can combine them.\n\n\n\nWhy This Matters: No More Catastrophic Forgetting\n\n\n\nSelection Strategy\nWhat happens to specialists\n\n\n\n\nGreedy\nDiscarded whenever aggregate score drops\n\n\nPareto\nPreserved if they solve anything unique\n\n\n\nGreedy selection caused our iteration 3 collapseâ€”the number-theory prompt overwrote the algebra promptâ€™s insights. Pareto selection prevents this by construction: you canâ€™t remove a prompt from the frontier unless something else does everything it does, plus more.\nThis is the core of GEPAâ€™s sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier never loses them in the first place.\n\nWe just saw Pareto selection preserve iteration 3â€™s prompt despite its 10% aggregate scoreâ€”because it solved instance #9, which nothing else could. But this raises a question: why does keeping â€œlosersâ€ help optimization? Shouldnâ€™t we focus resources on the best candidates?\nThe answer comes from quality-diversity algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.\n\n\n\nWhy Pareto Works: Quality-Diversity and Map Elites\nWeâ€™ve now built both core mechanisms from scratchâ€”reflective mutation and Pareto selection. Before diving into the full algorithm, letâ€™s briefly step back and understand why this approach works so well.\nThe Pareto frontier isnâ€™t a novel inventionâ€”it draws from quality-diversity (QD) algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.\n\nThe Core Insight\nTraditional optimization asks: â€œWhatâ€™s the single best solution?â€\nQuality-diversity asks: â€œWhatâ€™s the best solution of each type?â€\nComplex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensivelyâ€”but the best engine knows both styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one â€œbestâ€ approach.\n\n\nMap Elites: The Inspiration\nMap Elites (Mouret & Clune, 2015) maintains an archive organized by behavior:\n\nDefine behavior dimensions â€” characteristics describing how a solution works (not just how well)\nDiscretize into bins â€” each cell represents a â€œnicheâ€\nKeep the best per bin â€” new solutions compete only within their niche\nMutate from the archive â€” sample from any occupied bin, mutate, place in appropriate bin\n\nThe result: diverse specialists, each optimal of its type. Key insight: the archive provides stepping stonesâ€”a mutation from one niche might discover something useful for another. Diversity isnâ€™t just nice to have; itâ€™s a search strategy.\n\n\nGEPAâ€™s Adaptation: Validation Instances as Niches\nGEPA recognizes that the validation set itself defines the behavior space:\n\n\n\n\n\n\n\nMap Elites\nGEPA\n\n\n\n\nBehavior = continuous dimensions\nBehavior = which validation instances are solved\n\n\nBins = discretized regions\nâ€œBinsâ€ = individual validation instances\n\n\nArchive = best per bin\nPareto frontier = non-dominated prompts across instances\n\n\n\nEach validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the â€œspecialist for that nicheâ€â€”it stays because it demonstrates something works, even with low aggregate score.\nConcrete example: In our AIME experiment, instance #9 was a number theory problem requiring CRT. Prompt P3 (10% aggregate) solved it; P2 (30% aggregate) couldnâ€™t. In Map Elites terms, P3 is the â€œeliteâ€ for the number-theory nicheâ€”it stays in the archive despite low overall score.\n\nâ€œBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.â€ â€” Lakshya A Agrawal\n\nThis directly motivated GEPAâ€™s design: Pareto selection over per-instance scores naturally implements QDâ€™s â€œbest per nicheâ€ principle, while the merge operation recombines insights across nichesâ€”exactly what stepping-stone search requires.\n\n\nExploration-Exploitation Balance\nGEPAâ€™s Pareto selection handles the exploration-exploitation tradeoff automatically: candidates are sampled weighted by unique wins, so specialists get attention proportional to their unique value. The frontier self-prunesâ€”noise gets dominated away, genuine diversity persists. No manual tuning required.\nWith both reflective mutation and Pareto selection in place, thereâ€™s one more operation that makes GEPA powerful: mergeâ€”combining insights from divergent lineages. Letâ€™s look at the complete algorithm.\n\n\n\nThe Complete Algorithm\nNow that weâ€™ve built reflective mutation and Pareto selection from scratch, and understand why quality-diversity works, letâ€™s see how all the piecesâ€”including lineage tracking and mergeâ€”combine into GEPAâ€™s full optimization loop.\n\nAlgorithm Overview\nGEPA(base_prompt, trainset, valset, max_iterations):\n    \n    # Initialize\n    candidates = [base_prompt]\n    scores = {base_prompt: evaluate_all(base_prompt, valset)}\n    pareto_frontier = [base_prompt]\n    lineage = {base_prompt: None}  # parent tracking\n    \n    for iteration in 1..max_iterations:\n        \n        # 1. SAMPLE: Select candidate from Pareto frontier\n        #    Weighted by number of instances where candidate is best\n        parent = sample_from_frontier(pareto_frontier, scores)\n        \n        # 2. PROPOSE: Either mutate or merge\n        if should_merge(pareto_frontier, iteration):\n            # Select second parent from different lineage\n            other_parent = select_divergent_candidate(pareto_frontier, parent, lineage)\n            new_prompt = merge(parent, other_parent)\n        else:\n            # Run rollouts, collect feedback, reflect\n            minibatch = sample(trainset, k=3)\n            traces, feedback = run_with_feedback(parent, minibatch)\n            # See \"Hands-On: Building Reflective Mutation\" for the reflection prompt structure\n            new_prompt = reflect_and_mutate(parent, traces, feedback)\n        \n        # 3. EVALUATE: Mini-batch gate, then full evaluation\n        parent_mb_score = evaluate(parent, minibatch)\n        new_mb_score = evaluate(new_prompt, minibatch)\n        if new_mb_score &lt;= parent_mb_score:\n            continue  # Reject: didn't improve on mini-batch\n        \n        new_scores = evaluate_all(new_prompt, valset)\n        \n        # 4. UPDATE: Pareto frontier maintenance\n        if is_dominated(new_scores, pareto_frontier):\n            continue  # Reject: dominated by existing candidate\n        \n        # Remove any candidates the new one dominates\n        pareto_frontier = [c for c in pareto_frontier \n                          if not dominates(new_scores, scores[c])]\n        \n        # Add new candidate\n        candidates.append(new_prompt)\n        scores[new_prompt] = new_scores\n        pareto_frontier.append(new_prompt)\n        lineage[new_prompt] = parent\n    \n    # Return best aggregate for deployment; full frontier available via track_stats\n    return best_aggregate(pareto_frontier, scores)\n\n\n\nThe Key Decision Points\n1. Candidate Sampling (Exploration vs Exploitation)\nRather than always improving the single best prompt (greedy), GEPA samples from the entire Pareto frontier. The sampling weight is proportional to how many validation instances the candidate is uniquely best on:\ndef sample_from_frontier(frontier, scores):\n    weights = []\n    for candidate in frontier:\n        # Count instances where this candidate beats all others\n        unique_wins = sum(1 for i in range(n_instances) \n                        if scores[candidate][i] &gt; max(scores[other][i] \n                                                      for other in frontier if other != candidate))\n        weights.append(unique_wins + 1)  # +1 smoothing\n    return random.choices(frontier, weights=weights)[0]\nThis focuses effort on candidates that have demonstrated unique valueâ€”they solve something nothing else can. Specialists get attention proportional to their specialization.\n2. Mutation vs Merge Decision\nEarly iterations favor mutation (exploring from single candidates). As the frontier diversifies, merge becomes more valuable. The following is a simplified illustration of the decision logic:\ndef should_merge(frontier, iteration):\n    if len(frontier) &lt; 2: return False\n    if iteration &lt; 5: return False  # Let lineages diverge first\n    # Merge with probability proportional to frontier diversity\n    n_lineages = len(set(get_root(c) for c in frontier))\n    return random.random() &lt; (n_lineages - 1) / len(frontier)\n(The actual GEPA implementation may use different thresholdsâ€”see the paper for details.)\n3. Mini-Batch Gating\nBefore expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:\n\nâ€œWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.â€ â€” Lakshya A Agrawal\n\n4. Pareto Update\nThe frontier update follows the dominance logic we implemented earlier: - Reject new candidates dominated by existing ones (they add nothing) - Remove existing candidates dominated by the new one (theyâ€™re obsolete) - Keep all non-dominated candidates (each offers unique value)\n\n\n\nComplexity Analysis\n\n\n\n\n\n\n\nOperation\nCost\n\n\n\n\nMutation (3-4 rollouts + reflection)\n3-4 LLM calls + 1 reflection call\n\n\nMini-batch evaluation\n3-4 metric evaluations\n\n\nFull validation evaluation\nN metric evaluations (N = valset size)\n\n\nPareto check\nO(F Ã— N) comparisons (F = frontier size)\n\n\n\nThe mini-batch gate is crucial for efficiency. Most mutations failâ€”they either donâ€™t improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.\n\n\n\nWhy Each Component Matters\n\n\n\n\n\n\n\n\nComponent\nWithout it\nWith it\n\n\n\n\nTextual feedback\nOptimizer sees only score=0.6\nOptimizer reads â€œwrong answer, expected X, got Y becauseâ€¦â€\n\n\nPareto selection\nSpecialists discarded when aggregate drops\nSpecialists preserved if they solve anything unique\n\n\nLineage tracking\nNo memory of evolutionary history\nCan identify divergent branches for merge\n\n\nMerge operation\nInsights stay siloed in separate branches\nOrthogonal discoveries can combine\n\n\nMini-batch gating\nEvaluate every candidate fully\nReject obvious failures cheaply\n\n\n\nThe components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.\n\n\n\nWhat Gets Returned\nThe algorithm returns best_aggregate(pareto_frontier)â€”the prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use track_stats=True to access all candidates and their per-instance scores.\n\nNext: The merge operation in detailâ€”how GEPA combines insights from divergent lineages.\n\n\n\nThe Lineage Tree and System-Aware Merge\nWe touched on merge briefly in the algorithm overviewâ€”now letâ€™s see why itâ€™s essential and how it actually works.\nWhat â€œsystem-awareâ€ means: Unlike genetic algorithm crossover (which blindly swaps segments), GEPAâ€™s merge uses an LLM that understands what itâ€™s combiningâ€”it can resolve contradictions, synthesize complementary strategies, and produce coherent instructions rather than jumbled concatenations.\nPareto selection preserves specialistsâ€”but it creates a new problem: insights get siloed in separate branches.\nConsider what happens after 10 iterations of GEPA on AIME problems:\n\n\n\nLineage Tree with Merge Operation\n\n\nThe P2â†’P4 lineage accumulated algebra insights. The P3â†’P5 lineage accumulated number theory insights. Both survive on the Pareto frontier because each solves problems the other canâ€™t.\nBut what about a problem requiring both?\n\n\nThe Recombination Problem\nSuppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:\n\nP4 knows to subtract equations pairwise, but doesnâ€™t think to reduce mod p\nP5 knows CRT, but misses the algebraic setup\n\nWith mutation alone, P4 would need to independently rediscover number theory (which P5 already knows), or vice versa. The knowledge exists in our populationâ€”just in different branches.\nMerge solves this by combining insights from divergent lineages into a single candidate.\n\n\n\nHow Merge Works\nGEPAâ€™s merge is â€œsystem-awareâ€â€”it understands what itâ€™s combining, not just concatenating strings. The reflection LLM receives:\n\nBoth parent prompts with their full instruction text\nLineage context â€” what types of problems each lineage solved\nConflict guidance â€” instructions to resolve contradictions, not ignore them\n\nThe prompt asks the LLM to synthesize, not concatenate:\n\nâ€œCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Donâ€™t simply concatenateâ€”synthesize into coherent guidance.â€\n\nConcrete example â€” merging our AIME specialists:\n\n\n\n\n\n\n\n\nParent\nSpecialty\nKey instruction\n\n\n\n\nP4\nAlgebra\nâ€œSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.â€\n\n\nP5\nNumber theory\nâ€œApply CRT for modular constraints. Check for contradictions.â€\n\n\n\nMerged offspring P6: &gt; â€œApproach: (1) For equation systems, subtract pairwise to expose factor relationshipsâ€”enumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.â€\nP6 inherits both toolkits AND adds meta-knowledge about when to combine them. This is the â€œsystem-awareâ€ partâ€”the merge understands the semantics of what itâ€™s combining.\n\n\n\nWhen to Merge vs.Â Mutate\nGEPA alternates between operations based on frontier state:\n\n\n\n\n\n\n\n\nCondition\nOperation\nRationale\n\n\n\n\nEarly iterations (&lt; 5)\nMutate\nLet lineages diverge first; nothing to merge yet\n\n\nFrontier has one dominant lineage\nMutate\nNo orthogonal insights to combine\n\n\nFrontier has divergent specialists\nMerge\nRecombine discoveries from parallel explorations\n\n\nRecent merge succeeded\nMutate\nRefine the merged candidate\n\n\n\nThe paper describes the decision:\n\nâ€œAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.â€ â€” Lakshya A Agrawal\n\n\n\n\nMerge vs.Â Genetic Algorithm Crossover\nGEPAâ€™s merge is analogous to crossover in genetic algorithms, but smarter:\n\n\n\n\n\n\n\nGenetic Algorithms\nGEPA Merge\n\n\n\n\nGenes = bit positions\nInsights = natural language instructions\n\n\nCrossover = swap bit segments randomly\nMerge = LLM synthesizes with understanding\n\n\nCan create invalid offspring\nCan resolve contradictions\n\n\nBlind to semantics\nAware of what instructions mean\n\n\n\nRandom crossover might produce: â€œSubtract equations pairwise. Apply CRT. Subtract equations pairwise.â€ (nonsense duplication)\nLLM merge produces: â€œSet up algebra first, then apply number-theoretic constraints.â€ (coherent synthesis)\n\n\n\nThe Compounding Effect\nThe components reinforce each other:\n\nPareto selection preserves the diversity that makes merge valuable\nLineage tracking identifies which candidates come from divergent branches\n\nMerge recombines orthogonal discoveries into unified candidates\nPareto selection then preserves successful merges alongside remaining specialists\n\nWithout Pareto selection, thereâ€™s nothing interesting to mergeâ€”youâ€™d just have variants of one â€œbestâ€ prompt. Without merge, insights stay siloed even when the frontier is diverse.\nMutation explores depthâ€”refining one approach through successive reflections.\nMerge explores breadthâ€”combining orthogonal discoveries from parallel paths.\nTogether, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.\n\n\n\nWhat GEPA Learns: Domain-Specific Knowledge Encoding\nWeâ€™ve built the full algorithmâ€”reflective mutation, Pareto selection, merge. But what does all this machinery actually produce? Letâ€™s examine the output: prompts that encode domain expertise.\nOne of GEPAâ€™s most striking capabilities is its ability to encode domain-specific knowledge directly into promptsâ€”transforming tacit expertise into explicit instructions that persist across examples.\n\nPrompts as Knowledge Containers\nTraditional optimization treats prompts as opaque strings to be scored. GEPA treats them as knowledge containers that accumulate insights through the reflection loop:\n\n\n\nFailure Accumulation Experience\n\n\nEach iteration doesnâ€™t just fix one errorâ€”it extracts the lesson behind the error.\nConcrete example from our AIME experiments:\n\n\n\n\n\n\n\nStage\nPrompt excerpt\n\n\n\n\nSeed\nâ€œYou are given a problem and you have to give the answer along with reasoning.â€\n\n\nAfter iter 2\nâ€œâ€¦For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.â€\n\n\nAfter iter 3\nâ€œâ€¦When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.â€\n\n\n\nThe prompt evolved from generic instruction to encoding competition math heuristics. The LLM didnâ€™t invent theseâ€”it extracted them from its training knowledge, triggered by seeing specific failure modes.\n\n\nThree Categories of Captured Knowledge\nWe observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):\n1. Format and Interface Knowledge - Output schemas (â€œreturn JSON with keys: answer, reasoningâ€) - API conventions (â€œuse library.method(), not library_method()â€) - Parsing requirements (â€œintegers only, no leading zerosâ€)\nThis is easiest to extractâ€”format errors produce explicit feedback.\n2. Strategic Knowledge - Problem-solving heuristics (â€œtry small cases firstâ€) - Domain patterns (â€œcompetition problems often combine algebra and number theoryâ€) - Failure mode awareness (â€œwatch for off-by-one errors in countingâ€)\nThis emerges from reflecting on why approaches failed, not just that they failed.\n3. Factual Domain Knowledge - API names and signatures (â€œtorch.einsum, not torch.einstein_sumâ€) - Domain constants (â€œstandard gravity = 9.81 m/sÂ²â€) - Constraint relationships (â€œin valid Sudoku, each row/column/box contains 1-9 exactly onceâ€)\nThe LLM already knows thisâ€”reflection surfaces it into the prompt where itâ€™s consistently applied.\n\n\nWhy Prompts Beat Weights (Sometimes)\nFine-tuning encodes knowledge in model weightsâ€”opaque, distributed, hard to inspect. GEPA encodes knowledge in natural languageâ€”readable, editable, composable.\n\n\n\nAspect\nFine-tuning\nGEPA prompts\n\n\n\n\nInspectability\nBlack box\nHuman-readable instructions\n\n\nEditability\nRequires retraining\nEdit the text directly\n\n\nComposability\nTrain new model\nMerge prompt sections\n\n\nSample efficiency\nThousands of examples\nTens of examples\n\n\n\nA GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.\n\n\nThe Preservation Problem\nKnowledge accumulation has a failure mode: new insights can overwrite old ones. We saw this in our hands-on experimentâ€”iteration 3â€™s number theory insights overwrote iteration 2â€™s algebra insights, causing catastrophic forgetting.\nThis is precisely why GEPA uses Pareto selection. By preserving prompts that are best on any validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontierâ€”and the merge operation can later combine their insights.\n\nWithout Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.\n\n\n\nLimitation: Knowledge Must Be Triggerable\nGEPA can only surface knowledge the base LLM already has. If the model doesnâ€™t know that â€œCRTâ€ means Chinese Remainder Theorem, no amount of reflection will discover it. The optimization extracts and organizes existing knowledgeâ€”it doesnâ€™t create new knowledge.\nThis is why GEPA works better with stronger base models: more latent knowledge to extract. And why, for domains requiring knowledge the LLM lacks, youâ€™ll need to inject it through few-shot examples, retrieval augmentation, or explicit instructions.\n\nâ€œLanguage models already have built a lot of prior knowledgeâ€¦ we can use all of this natural language information to make the LLM itself reflect on its mistakes and improve.â€ â€” Lakshya A Agrawal\n\n\nNow that we understand the algorithm and what it produces, letâ€™s look at how to use GEPA in practice."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "href": "posts/gepa-deepdive/gepa_final_article.html#beyond-training-gepa-for-inference-time-search",
    "title": "GEPA Deepdive",
    "section": "Beyond Training: GEPA for Inference-Time Search",
    "text": "Beyond Training: GEPA for Inference-Time Search\nEverything weâ€™ve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.\n\nTwo Paradigms of Operation\nTrain-then-generalize (what weâ€™ve built so far): - Optimize prompts on a training set - Select the best-aggregate prompt from the Pareto frontier - Deploy that prompt on new, unseen tasks - Goal: learn generalizable lessons that transfer\nTest-time search (inference-time optimization): - You have a batch of hard tasks you need to solve now - Optimize directly on the tasks themselves - GEPA searches for solutions, storing the best prompt per task - Goal: maximize performance on these specific instances\nThe mechanics are identicalâ€”reflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, youâ€™re using GEPA as a search algorithm over the solution space. This aligns with the broader trend of inference-time compute scalingâ€”investing more computation at inference to solve harder problems.\nThe key mechanical change: pass valset=trainset (or equivalently, omit valset and set trainset to your target tasks). This tells GEPA to optimize directly on the problems youâ€™re trying to solve rather than holding out a separate validation set. See the GEPA API documentation for full parameter details.\n# Test-time search: optimize directly on the tasks you want to solve\noptimized = GEPA(metric=metric_with_feedback, auto=\"medium\").compile(\n    program,\n    trainset=hard_problems,  # The actual tasks you need solved\n    valset=hard_problems,    # Same setâ€”no held-out validation\n)\n\nâ€œGiven a batch of tasks that we want to solve and given some budgetâ€¦ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.â€ â€” Lakshya A Agrawal\n\n\n\n\nWhy GEPA Beats High-Temperature Sampling\nTraditional inference-time compute strategies generate many candidates by sampling at high temperature, then use a verifier or LLM-as-judge to pick the best one. But these samples tend to be similarâ€”variations on the same approach.\nRecent research on test-time compute scaling (Snell et al., 2024) shows that â€œcompute-optimalâ€ strategiesâ€”which adaptively allocate inference budget based on problem difficultyâ€”can improve efficiency by more than 4x compared to traditional best-of-N sampling. In some cases, smaller models with optimized test-time compute outperform models 14x larger that donâ€™t use additional inference-time computation.\nThe intuition: High-temperature sampling produces variations around the same approachâ€”different variable names, slightly different loop bounds, minor syntactic choices. GEPAâ€™s reflective mutation proposes structurally different strategies based on what went wrong. When the feedback says â€œmemory bandwidth bottleneck,â€ the next candidate might switch from a naive loop to shared-memory tilingâ€”a qualitative change that temperature variation rarely discovers.\nThe Pareto frontier then preserves these diverse strategies rather than converging on a single â€œbestâ€ approach.\nGEPA induces genuine diversity through two mechanisms:\n\nPareto tracking â€” maintains candidates that each excel at something different, not just the single highest scorer\nReflective mutation â€” proposes structurally different approaches based on what went wrong, not random perturbations\n\n\nâ€œDue to the way GEPA operates where itâ€™s doing the Pareto candidate tracking and itâ€™s doing reflective mutation, we find that GEPA is itself inducing a huge amount of diversity in the kinds of solutions that it generates.â€ â€” Lakshya A Agrawal\n\nRather than 100 slight variations of one approach, GEPA maintains a frontier of genuinely different strategies.\nConcrete results: On the MATH benchmark, GEPA achieves 93% accuracy compared to 67% with basic DSPy ChainOfThoughtâ€”a 26 percentage point improvement from prompt optimization alone, no fine-tuning required.\n\n\n\nSelf-Bootstrapping: How Iteration Compounds\nGEPAâ€™s iterative process creates a self-bootstrapping dynamic:\n\nRound 1: Generate rollouts â†’ get feedback (e.g., compiler error) â†’ reflect â†’ propose fix\nRound 2: The code compiles, but now thereâ€™s a runtime error (division by zero) â†’ reflect â†’ propose fix\nRound 3: Runtime works, but output is wrong â†’ reflect â†’ propose fix\n\n\nâ€œIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So itâ€™s kind of like self-bootstrapping data to train itself.â€ â€” Lakshya A Agrawal\n\nEach iteration surfaces a new failure mode and proposes a solutionâ€”generating increasingly challenging training signal from the task itself.\n\n\n\nCross-Task Transfer Within a Batch\nWhen solving related tasks (e.g., a batch of CUDA kernels), insights compound across the batch:\n\nâ€œAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.â€ â€” Lakshya A Agrawal\n\nThe frontier maintains multiple specialized prompts simultaneously:\n\n\n\nPrompt\nSpecialization\nProblems solved\n\n\n\n\nP_conv\nConvolutional operators\n#1, #4, #7\n\n\nP_reduce\nReduction/summation operators\n#2, #5, #8\n\n\nP_matmul\nMatrix multiplication\n#3, #6\n\n\n\n\nâ€œOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.â€ â€” Lakshya A Agrawal\n\nWhen a new problem arrives, GEPA tries candidates from across the frontierâ€”the convolution specialist might crack it, or the reduction specialist, or insights from both might merge.\n\n\n\nWhat GEPA Stores\nFor each task in the batch, GEPA tracks both artifacts:\n\nâ€œGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.â€ â€” Lakshya A Agrawal\n\n\nBest outputs â€” the actual solutions, ready to use\nBest prompts â€” specialized strategies representing different subdomains of your problem space\n\nYou can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.\n\n\n\nFeedback Design for Each Paradigm\nThe paradigm choice shapes how you design feedback:\nFor train-then-generalize â€” extract transferable lessons:\n\nâ€œYou will ensure that there is no task-specific insights that the prompt captures. So you will write in your feedback something along the lines of â€˜try to extract lessons out of this.â€™â€ â€” Lakshya A Agrawal\n\ndef metric_generalizable(gold, pred, trace=None):\n    score = int(gold.answer == pred.answer)\n    if score == 0:\n        feedback = (\n            f\"Failed: expected {gold.answer}, got {pred.answer}. \"\n            f\"Extract a GENERAL lesson that would help on similar problems.\"\n        )\n    else:\n        feedback = \"Correct. What general strategy led to success?\"\n    return {\"score\": score, \"feedback\": feedback}\nFor test-time search â€” hyper-specialize to the task:\n\nâ€œIf youâ€™re doing simply an inference time search where you just care about the final outputs, then you will try to provide feedback which is as hyper-specialized to the task as possible.â€ â€” Lakshya A Agrawal\n\ndef metric_specialized(gold, pred, trace=None):\n    score = int(gold.answer == pred.answer)\n    if score == 0:\n        feedback = (\n            f\"Your code failed with this particular compiler error: {pred.error}. \"\n            f\"Try to improve on this specific thing.\"\n        )\n    else:\n        feedback = \"Correct.\"\n    return {\"score\": score, \"feedback\": feedback}\n\n\n\nThe Tutor Analogy: Background Optimization Loops\nThis iterative pattern mirrors how students already work with AI tutoring systems:\n\nAttempt problem â†’ get solution\nSee an error â†’ receive feedback\nTutor explains â†’ improved understanding\nRepeat until mastered\n\nThis suggests an intriguing application: background GEPA loops for personalization. Similar patterns are emerging in tools like Cursor and other AI-assisted development environments.\n\nâ€œFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing promptâ€¦ cursor can learn a user-specific prompt that works well specifically for you.â€ â€” Lakshya A Agrawal\n\nEvery time you provide feedback on an AI-generated solution, a background loop could update not just the current response but a user-specific prompt capturing your preferences and patterns.\n\n\n\nWhen NOT to Use Test-Time GEPA\n\nSparse feedback â€” If your metric only returns pass/fail with no explanation, GEPA canâ€™t extract lessons\nTasks outside LLM knowledge â€” GEPA surfaces knowledge the model already has; genuinely novel information wonâ€™t emerge from reflection. For such domains, consider retrieval-augmented generation to inject external knowledge\nTight latency constraints â€” Test-time GEPA typically runs 5-20 iterations, each involving LLM calls for rollout + reflection. Budget 30 seconds to 5 minutes per task depending on model latency and iteration count. Interactive use cases need a different approach.\nIdentical tasks â€” If your batch contains identical problems (not just related ones), optimize once and reuse. Cross-task transfer helps when problems are related but distinctâ€”like different CUDA kernels that share optimization patterns but have different structures.\n\n\nWhen the conditions are rightâ€”rich feedback, high-value tasks worth the compute, domains where the LLM has strong priorsâ€”test-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.\n\n\nCase Study: Code Optimization for Novel Hardware\nThe GEPA paper demonstrates test-time search on domains where GEPAâ€™s strengths shine brightest: code optimization for hardware with limited pre-training data.\n\nAMD NPU Kernels: Optimization Without Pre-Training Knowledge\nAMDâ€™s NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The NPUEval benchmark reveals just how challenging this is: even with compiler feedback and retrieval-augmented generation (RAG), state-of-the-art LLMs achieve only ~10% mean vectorization score across the dataset.\nGEPA succeeds here because it doesnâ€™t need prior examples. Instead, it:\n\nGenerates an initial kernel based on generic programming knowledge\nCompiles and runs â†’ receives compiler errors or performance metrics\nReflects on feedback â†’ proposes targeted improvements\nIterates until the kernel compiles, runs correctly, and performs well\n\nWhy GEPA fits: The GEPA paper demonstrates results on NPUEval, showing that reflective mutation can extract optimization patterns even for hardware with minimal pre-training coverageâ€”without requiring retraining or RAG.\n\nâ€œGEPA can be used to generate optimized kernels for AMDâ€™s NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.â€ â€” Lakshya A Agrawal\n\nThe compiler error messages contain the fix. â€œSymbol not found: npu_matmulâ€ triggers reflection that surfaces the correct API. â€œMemory alignment error at line 47â€ points directly to what needs fixing.\n\n\nCUDA Kernels: Outperforming Human Baselines\nKernelBench (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernelsâ€”a task where even frontier models struggle. The benchmark reveals a sobering baseline: frontier reasoning models match the PyTorch baseline on less than 20% of tasks using the fastâ‚ metric (kernels that are both correct and faster than PyTorch). This isnâ€™t a matter of promptingâ€”efficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models havenâ€™t learned to apply reliably.\nThe impact of iterative feedback: The KernelBench paper demonstrates that feedback-driven refinement dramatically improves results. By providing execution results and profiler feedback in context, fastâ‚ scores improved from 12%, 36%, and 12% to 43%, 72%, and 18% respectively across their test configurations. This is exactly the mechanism GEPA exploitsâ€”but with systematic Pareto tracking and reflective mutation rather than ad-hoc iteration.\n\nâ€œFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.â€ â€” Lakshya A Agrawal\n\nRelated evolutionary approaches show further gains: Stanfordâ€™s separate test-time evolutionary search (distinct from GEPA, published in their Fast Kernels blog post) produces kernels achieving 103-133% of PyTorch reference performance on foundational operators like Conv2Dâ€”demonstrating that structured search with execution feedback can exceed human-optimized baselines.\n\n\n\n\n\n\n\n\nMethod\nPerformance\nSource\n\n\n\n\nBase LLM (one-shot)\n~3-15% fastâ‚\nKernelBench paper\n\n\nFrontier reasoning (o1, R1)\nMatch baseline on &lt;20% of tasks\nKernelBench paper\n\n\nWith execution + profiler feedback\n43-72% fastâ‚ (3-6x improvement)\nKernelBench paper\n\n\nEvolutionary test-time search\n103-133% of PyTorch on select kernels\nStanford CRFM (separate from GEPA)\n\n\nGEPA\nâ€œSignificant speedups over PyTorch-eagerâ€\nGEPA paper Fig. 11\n\n\n\nThe gap matters commercially: efficient compilers often lag behind new GPU architectures by over two yearsâ€”approximately one year for CUDA experts to develop optimized implementations and another year to generalize into compilers. GEPA-style optimization could bridge that gap by generating optimized kernels for new hardware before traditional toolchains catch up.\nWhy code optimization is ideal for GEPA:\n\n\n\n\n\n\n\nProperty\nWhy it helps\n\n\n\n\nRich textual feedback\nCompiler errors, profiler output, runtime exceptions all explain why something failed\n\n\nVerifiable correctness\nUnit tests and benchmarks provide unambiguous signal\n\n\nIterative refinement\nEach failed compilation reveals the next fix to try\n\n\nCross-task transfer\nInsights about memory coalescing on one kernel help others\n\n\n\n\n\nThe Self-Bootstrapping Dynamic\nA typical GEPA optimization trajectory for CUDA kernels follows this pattern:\nIteration 1: Generate naive kernel â†’ compiler error â€œundeclared identifier threadIdxâ€ - Reflection: â€œCUDA kernels require explicit thread indexing. Add threadIdx.x and blockIdx.x.â€\nIteration 2: Compiles, but runtime error (out of bounds) - Reflection: â€œNeed bounds checking. Add if (idx &lt; n) guard.â€\nIteration 3: Runs correctly, but 10x slower than baseline - Profiler feedback: â€œMemory bandwidth: 12% of peak. Non-coalesced access pattern.â€ - Reflection: â€œReorganize memory access for coalescing. Use shared memory for reductions.â€\nIteration 4: 1.2x faster than PyTorch baseline\nEach iteration surfaced a new challenge and its solutionâ€”generating increasingly sophisticated training signal from the task itself.\n\n\nCross-Kernel Transfer\nWhen optimizing a batch of related kernels, insights compound across tasks:\n\nâ€œIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.â€ â€” Lakshya A Agrawal\n\nGEPA exploits the relatedness of kernels in a batch. As it optimizes each kernel, the Pareto frontier accumulates specialized promptsâ€”one excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. When a new kernel arrives, candidates from across the frontier are tried. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax.\nThis cross-task transfer is why batch optimization outperforms solving each kernel independentlyâ€”the accumulated optimization knowledge benefits later tasks.\n\n\nBeyond Kernels\nThe same pattern extends to other code optimization domains where execution produces rich textual feedback:\n\nâ€œThere are many other tasks where it can be usedâ€”for example, with unit tests to generate code patches.â€ â€” Lakshya A Agrawal\n\nBug fixing (test failures explain whatâ€™s wrong), performance optimization (profiler output identifies bottlenecks), and API migration (deprecation warnings specify changes) all fit GEPAâ€™s feedback-driven model."
  },
  {
    "objectID": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "href": "posts/gepa-deepdive/gepa_final_article.html#conclusion-when-to-reach-for-gepa",
    "title": "GEPA Deepdive",
    "section": "Conclusion: When to Reach for GEPA",
    "text": "Conclusion: When to Reach for GEPA\nWe opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculusâ€”matching RLâ€™s optimization performance at a fraction of the sample cost by doing something RL canâ€™t: reading the feedback.\nThe results speak for themselves: 46.6% â†’ 56.6% on AIME math competition problems. 67% â†’ 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.\nThis represents a genuinely new point in the optimization design spaceâ€”one that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. Thatâ€™s not incremental improvementâ€”itâ€™s a 100x reduction in sample requirements.\n\n\nUse GEPA for Train-Then-Generalize When:\n\nYou have rich textual feedback (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)\nYour evaluation budget is limited (50-500 examples, not 5,000)\nYouâ€™re optimizing compound AI systems where prompts orchestrate multi-step pipelines\nYou need interpretable resultsâ€”prompts you can read, edit, and reason about\n\n\n\nUse GEPA for Test-Time Search When:\n\nYou have a batch of high-value tasks worth the compute investment\nEach task produces execution feedback (tests, profilers, validators)\nTasks are related enough for cross-task transfer to help\n\n\n\nStick with Traditional Approaches When:\n\nYou have abundant labeled data and compute budget for fine-tuning\nFeedback is purely scalar with no explanatory signal\nThe task is already solved by few-shot prompting\nYou need sub-second latency\n\n\n\n\nKnown Limitations\nGEPA isnâ€™t a silver bullet:\n\nReflection quality varies by domain â€” GEPA excels when the LLM has strong prior knowledge. On highly specialized domains where the base model is weak, reflection produces generic advice rather than actionable fixes.\nFeedback bottleneck â€” The optimizer is only as good as its feedback. If your metric returns â€œwrongâ€ without explaining why, GEPA degrades to expensive random search.\nValidation set size â€” With fewer than 30 validation examples, prompts can overfit to idiosyncrasies of those specific instances.\n\n\n\n\nGet Started\nReady to try GEPA on your own pipelines?\n\nGEPA for AIME Tutorial â€” Complete walkthrough from setup to optimized results\nGEPA API Reference â€” Full parameter documentation\nPaper â€” Algorithm details and experimental methodology\n\nThe core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads itâ€”and learns."
  }
]