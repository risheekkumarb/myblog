<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Risheek kumar B">
<meta name="dcterms.date" content="2025-12-21">
<meta name="description" content="An Indepth Handson explaination about GEPA - its core and motivations">

<title>GEPA Deepdive ‚Äì Risheekkumar Baskaran</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Risheekkumar Baskaran</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/risheekkumar-baskaran-748115120/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/risheekkumarb"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/BRisheek"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">GEPA Deepdive</h1>
                  <div>
        <div class="description">
          An Indepth Handson explaination about GEPA - its core and motivations
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">research paper</div>
                <div class="quarto-category">analysis</div>
                <div class="quarto-category">reimplementation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Risheek kumar B </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 21, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<blockquote class="blockquote">
<p>This article was made possible by the research papers referenced throughout, the <a href="https://www.youtube.com/watch?v=fREQrxhBSk0">Weaviate discussion with Lakshya A Agrawal</a>, and guidance from Kerem Turgutlu at answer.ai. Created using the <a href="https://solveit.fast.ai">solveit</a> platform by fast.ai.</p>
</blockquote>
<p>You‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?</p>
<p>Fine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.</p>
<p>But what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?</p>
<p><strong><a href="https://arxiv.org/abs/2507.19457">GEPA</a></strong> (Genetic-Pareto) achieves exactly this‚Äîby exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM <em>reflect on its own failures</em> and propose improvements directly.</p>
<p>In this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.</p>
<section id="the-problem-scalar-rewards-and-expensive-rollouts" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-scalar-rewards-and-expensive-rollouts">The Problem: Scalar Rewards and Expensive Rollouts</h3>
<p>Traditional AI optimization techniques‚Äîreinforcement learning and fine-tuning‚Äîhave achieved remarkable results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?</p>
<p>Consider: - <strong>Agentic pipelines</strong> that invoke simulations, query rate-limited APIs, or run multi-step tool chains - <strong>Code generation for novel hardware</strong>, where each evaluation requires compiling for custom silicon and executing on the device - <strong>Complex reasoning tasks</strong> with expensive verification steps</p>
<p>Collecting thousands of rollouts simply isn‚Äôt feasible in these settings.</p>
<p>The core issue: <strong>RL learns by comparison</strong>. A 500-step trajectory collapses to <code>reward = 0.73</code>. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectories‚Äî<em>this one scored 0.8, that one scored 0.4, what differed?</em>‚Äîrequiring sample counts that expensive domains can‚Äôt support.</p>
<blockquote class="blockquote">
<p><em>RL and fine-tuning require generating large amounts of rollouts to gather scalar learning signals‚Äîsample inefficient by design.</em></p>
</blockquote>
<p>When each rollout costs minutes (or dollars), this approach breaks down.</p>
</section>
<section id="the-insight-llm-pipelines-generate-rich-textual-traces" class="level3">
<h3 class="anchored" data-anchor-id="the-insight-llm-pipelines-generate-rich-textual-traces">The Insight: LLM Pipelines Generate Rich Textual Traces</h3>
<p>Modern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:</p>
<ul>
<li><strong>Reasoning traces</strong> ‚Äî Chain-of-Thought and ReAct logs expose the model‚Äôs explicit thought process. When a multi-hop QA system fails, you can see <em>where</em> the reasoning went wrong: ‚ÄúThe capital of France is Paris. Paris is in Germany‚Ä¶‚Äù The failure mode is visible in the text.</li>
<li><strong>Environment feedback</strong> ‚Äî Compiler errors don‚Äôt just say ‚Äúfailed.‚Äù They say <code>cannot find symbol 'x', did you mean 'y'?</code> API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.</li>
<li><strong>Evaluation rubrics</strong> ‚Äî LLM-as-judge systems don‚Äôt just score 3/5. They explain: ‚ÄúResponse was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.‚Äù</li>
</ul>
<p>Each of these is dramatically richer than <code>reward = 0.73</code>.</p>
<p><strong>The key realization</strong>: these traces aren‚Äôt just logs for debugging‚Äîthey‚Äôre potential <em>input to the optimizer</em>. A compiler error that says ‚Äúdid you mean ‚Äòy‚Äô?‚Äù contains the fix. A rubric that says ‚Äútoo verbose‚Äù specifies exactly what to change.</p>
<p>Traditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.</p>
<p>But what if we could just‚Ä¶ read the feedback?</p>
<p>This is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?</p>
</section>
<section id="the-opportunity-llms-can-reflect-on-their-own-failures" class="level3">
<h3 class="anchored" data-anchor-id="the-opportunity-llms-can-reflect-on-their-own-failures">The Opportunity: LLMs Can Reflect on Their Own Failures</h3>
<p>Here‚Äôs the key insight enabling a new optimization paradigm: <strong>LLMs already have prior knowledge about the domains they‚Äôre working in, and they‚Äôre increasingly capable of self-reflection.</strong></p>
<p>Consider what happens with different types of feedback:</p>
<p><strong>Compiler errors</strong> ‚Äî When the compiler returns <code>cannot find symbol 'x', did you mean 'y'?</code>, the LLM doesn‚Äôt need thousands of examples to learn the fix. It already knows the library‚Äôs API. One error message is enough.</p>
<blockquote class="blockquote">
<p><em>‚ÄúThe language model already knows that x is not a valid API name in the library but y is. Next time I should try this.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p><strong>LLM-as-judge feedback</strong> ‚Äî When a judge says ‚Äúyour summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,‚Äù the model can directly incorporate ‚Äúbe concise, match casual tone‚Äù into its next attempt. No statistical signal extraction required.</p>
<p><strong>Reasoning trace failures</strong> ‚Äî When a multi-hop QA trace shows the model correctly retrieved ‚ÄúParis is the capital of France‚Äù but then hallucinated ‚ÄúParis is in Germany,‚Äù the failure point is <em>visible in the text</em>. You can see exactly where the reasoning derailed.</p>
<p><strong>Privacy-aware rewriting (PUPA task)</strong> ‚Äî In the paper‚Äôs experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains <em>why</em> a rewrite failed‚Äî‚Äúleaked the user‚Äôs company name‚Äù or ‚Äúremoved too much context, degrading response quality‚Äù‚Äîgiving the optimizer actionable signal from each example.</p>
<section id="the-core-asymmetry" class="level4">
<h4 class="anchored" data-anchor-id="the-core-asymmetry">The Core Asymmetry</h4>
<p>This is the fundamental difference GEPA exploits:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>How it learns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>RL</strong></td>
<td>Compare thousands of trajectories statistically: ‚ÄúThese 500 scored 0.8, those 500 scored 0.4‚Äîwhat differed?‚Äù</td>
</tr>
<tr class="even">
<td><strong>Reflection</strong></td>
<td>Read the feedback directly: ‚ÄúThe compiler said use y, so use y.‚Äù</td>
</tr>
</tbody>
</table>
<p>RL would need hundreds of rollouts to statistically isolate that <code>x‚Üíy</code> is the fix. The LLM gets it from one error message.</p>
</section>
<section id="from-fixes-to-generalizable-rules" class="level4">
<h4 class="anchored" data-anchor-id="from-fixes-to-generalizable-rules">From Fixes to Generalizable Rules</h4>
<p>Better still, modern LLMs don‚Äôt just extract point fixes‚Äîthey can derive <em>generalizable lessons</em>:</p>
<ul>
<li>Not just ‚Äúuse <code>y</code> instead of <code>x</code>‚Äù ‚Üí but ‚Äúalways verify symbol names against the library‚Äôs namespace before generating code‚Äù</li>
<li>Not just ‚Äúresponse was too long‚Äù ‚Üí but ‚Äúfor Slack contexts, limit responses to 150 words and use bullet points‚Äù</li>
<li>Not just ‚Äúleaked company name‚Äù ‚Üí but ‚Äúscan for proper nouns and replace with generic placeholders‚Äù</li>
</ul>
<blockquote class="blockquote">
<p><em>‚ÄúLLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>These rules get folded directly into the prompt as instructions‚Äîcompounding improvements across examples rather than treating each failure in isolation.</p>
<p>This capability unlock‚ÄîLLMs that can genuinely reflect and generalize‚Äîis what makes GEPA viable now when it wouldn‚Äôt have been two years ago.</p>
</section>
</section>
<section id="why-now-the-reflection-capability-unlock" class="level3">
<h3 class="anchored" data-anchor-id="why-now-the-reflection-capability-unlock">Why Now? The Reflection Capability Unlock</h3>
<p>This approach wasn‚Äôt viable with earlier LLMs. In March 2023, roboticist <a href="https://evjang.com/2023/03/26/self-reflection.html">Eric Jang observed</a> that self-reflection capability ‚Äúseems to be emergent in GPT-4 but not GPT-3.5 or Claude.‚Äù When asked to write a non-rhyming poem, GPT-4 produced rhymes‚Äîbut when prompted ‚Äúdid the poem meet the assignment?‚Äù it apologized and corrected itself. GPT-3.5 and Claude couldn‚Äôt recognize their errors.</p>
<p>The <a href="https://arxiv.org/abs/2303.11366">Reflexion paper</a> (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4‚Äôs baseline 80%‚Äîwithout any weight updates. Similarly, <a href="https://arxiv.org/abs/2303.17651">Self-Refine</a> showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.</p>
<p>But there‚Äôs a crucial nuance. A <a href="https://aclanthology.org/2024.tacl-1.78/">comprehensive 2024 survey</a> found that pure ‚Äúintrinsic‚Äù self-correction‚Äîwhere the LLM reflects with no external signal‚Äîrarely helps, and can even degrade performance. What <em>does</em> work is self-correction with <strong>reliable external feedback</strong>: compiler errors, test results, structured rubrics.</p>
<p>This is precisely what GEPA exploits. The <a href="https://arxiv.org/abs/2305.11738">CRITIC paper</a> (ICLR 2024) highlights that external feedback is ‚Äúcrucial‚Äù for successful self-improvement. GEPA doesn‚Äôt ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedback‚Äîthe compiler said this, the profiler showed that, the judge flagged this rubric‚Äîand asks it to reflect on <em>that</em>.</p>
<p>The capability unlock isn‚Äôt ‚ÄúLLMs can now introspect perfectly.‚Äù It‚Äôs ‚ÄúLLMs can now <em>process feedback and generalize lessons</em> effectively.‚Äù</p>
<blockquote class="blockquote">
<p><em>‚ÄúEarlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessons‚Ä¶ But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
</section>
<section id="optimizer-evolution-from-few-shot-to-reflection" class="level3">
<h3 class="anchored" data-anchor-id="optimizer-evolution-from-few-shot-to-reflection">Optimizer Evolution: From Few-Shot to Reflection</h3>
<p>To understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessor‚Äîand GEPA represents the latest capability unlock.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/optimizer evolution.webp" class="img-fluid figure-img"></p>
<figcaption>Optimizer Evolution</figcaption>
</figure>
</div>
<section id="bootstrap-few-shot-dspy-2023" class="level4">
<h4 class="anchored" data-anchor-id="bootstrap-few-shot-dspy-2023">Bootstrap Few-Shot (DSPy, 2023)</h4>
<p>The original insight: you don‚Äôt need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as <a href="https://www.promptingguide.ai/techniques/fewshot">few-shot demonstrations</a> for future runs. The system <a href="https://dspy.ai/learn/optimization/optimizers/">bootstraps its own examples</a> from successful executions.</p>
<p><strong>Example</strong>: Your QA system correctly answers ‚ÄúWhat‚Äôs the capital of France?‚Äù ‚Üí ‚ÄúParis‚Äù. That (question, answer) pair becomes a demonstration shown to the model on future queries.</p>
<p><strong>Limitation</strong>: Demonstrations are static snapshots. Once selected, they don‚Äôt adapt when new failure modes emerge. And there‚Äôs no <a href="https://arxiv.org/abs/2309.03409">instruction optimization</a>‚Äîthe system prompt stays identical whether you‚Äôre handling edge cases or common inputs.</p>
</section>
<section id="opro-google-deepmind-2023" class="level4">
<h4 class="anchored" data-anchor-id="opro-google-deepmind-2023">OPRO (Google DeepMind, 2023)</h4>
<p><a href="https://arxiv.org/abs/2309.03409">OPRO</a> (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.</p>
<p><strong>Example</strong>: The optimizer sees: - <code>"Solve the math problem step by step"</code> ‚Üí score 0.65 - <code>"Show your work and verify the answer"</code> ‚Üí score 0.72 - <code>"Break the problem into cases and check each"</code> ‚Üí score 0.78</p>
<p>It proposes: <code>"Systematically enumerate cases and verify each solution"</code> ‚Üí score 0.81</p>
<p><strong>Limitation</strong>: Score-only signal. The optimizer sees <em>that</em> <code>prompt_v3</code> scored 0.72 but not <em>why</em>. Did it make algebraic errors? Miss edge cases? The number alone doesn‚Äôt say.</p>
</section>
<section id="mipro-2024" class="level4">
<h4 class="anchored" data-anchor-id="mipro-2024">MiPRO (2024)</h4>
<p><a href="https://arxiv.org/abs/2406.11695">MiPRO</a> recognized that instructions and demonstrations interact‚Äîthe <em>same</em> instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.</p>
<p><strong>The search space problem</strong>: Say you have 10 candidate instructions and 5 possible demo sets. That‚Äôs 50 combinations. Now add instruction variants (‚ÄúBe concise‚Äù vs ‚ÄúBe brief‚Äù vs ‚ÄúAnswer in one sentence‚Äù)‚Äîsuddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.</p>
<p><strong>MiPRO‚Äôs solution: a cheap surrogate model</strong>. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you <em>have</em> run. The predictor learns patterns like ‚Äúinstructions mentioning ‚Äòstep-by-step‚Äô tend to score higher‚Äù or ‚Äúdemos with longer reasoning traces correlate with better performance.‚Äù</p>
<p>The workflow: 1. <strong>Bootstrap</strong>: Run a small random sample of combinations (say, 30 out of 500) 2. <strong>Train surrogate</strong>: Fit the predictor on those 30 (instruction, demos) ‚Üí score pairs 3. <strong>Predict cheaply</strong>: Score all 500 combinations using the surrogate (milliseconds, not dollars) 4. <strong>Evaluate selectively</strong>: Only run full evaluation on the top-predicted candidates 5. <strong>Repeat</strong>: Add new results to training data, retrain surrogate, sample again</p>
<p><strong>Example</strong>: After 30 random evaluations, the surrogate learns: - Instructions with ‚Äústep-by-step‚Äù ‚Üí +0.08 average - Demo set B (which has chain-of-thought examples) ‚Üí +0.05 average - Combining both ‚Üí predicted 0.79</p>
<p>MiPRO focuses budget on high-predicted combinations rather than exhaustive search.</p>
<p><strong>Limitation</strong>: The surrogate learns <em>correlations</em>, not <em>causation</em>. It knows ‚Äústep-by-step instructions score higher‚Äù but not <em>why</em>‚Äîmaybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt that‚Äôs 0.75 on everything beats one that‚Äôs 0.95 on hard cases but 0.60 overall‚Äîeven though that hard-case specialist might contain crucial insights.</p>
</section>
<section id="simba-2024" class="level4">
<h4 class="anchored" data-anchor-id="simba-2024">SIMBA (2024)</h4>
<p><a href="https://arxiv.org/abs/2410.17116">SIMBA</a> reframes prompt optimization as a <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a> problem‚Äîa classic framework for sequential decision-making under uncertainty.</p>
<p><strong>The bandit analogy</strong>: Imagine you‚Äôre in a casino with 100 slot machines. Each has a different (unknown) payout rate. You have 50 tokens. How do you maximize winnings?</p>
<ul>
<li><strong>Pure exploitation</strong>: Find one machine that seems good, play it 50 times. Problem: maybe you got lucky early‚Äîanother machine is actually better.</li>
<li><strong>Pure exploration</strong>: Try each machine once, then‚Ä¶ you‚Äôre out of tokens before you learn anything useful.</li>
<li><strong>Smart balance</strong>: Track your uncertainty about each machine. Play machines where you‚Äôre <em>uncertain</em> (might be great!) more than machines you‚Äôre <em>confident</em> are mediocre.</li>
</ul>
<p>SIMBA applies this to prompts. Each candidate prompt is a ‚Äúslot machine.‚Äù Each evaluation is a ‚Äúpull.‚Äù The score is the ‚Äúpayout.‚Äù</p>
<p><strong>How it works</strong>:</p>
<ol type="1">
<li><strong>Initialize</strong>: Start with a pool of candidate prompts (maybe generated by an LLM or hand-written)</li>
<li><strong>Track statistics</strong>: For each prompt, maintain: average score so far, number of times evaluated, and a <em>confidence interval</em> (range of plausible true scores)</li>
<li><strong>Sample strategically</strong>: Use <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit#Upper_confidence_bounds">Upper Confidence Bound (UCB)</a> to pick which prompt to evaluate next‚Äîfavoring prompts with high uncertainty OR high average</li>
<li><strong>Update beliefs</strong>: After evaluation, narrow the confidence interval for that prompt</li>
<li><strong>Repeat</strong>: Eventually, confidence intervals separate‚Äîyou know which prompts are best</li>
</ol>
<p><strong>Example in action</strong>: You have 20 candidate prompts, budget for 50 evaluations.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Evaluations</th>
<th>Avg Score</th>
<th>Confidence Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>8</td>
<td>0.74</td>
<td>[0.70, 0.78]</td>
</tr>
<tr class="even">
<td>B</td>
<td>2</td>
<td>0.71</td>
<td>[0.55, 0.87]</td>
</tr>
<tr class="odd">
<td>C</td>
<td>5</td>
<td>0.68</td>
<td>[0.62, 0.74]</td>
</tr>
</tbody>
</table>
<p>Which to evaluate next? - Prompt A: probably ~0.74, we‚Äôre confident - Prompt B: could be 0.55 (bad) or 0.87 (best!)‚Äîhigh uncertainty - Prompt C: probably ~0.68, we‚Äôre fairly confident it‚Äôs worse than A</p>
<p>SIMBA picks <strong>Prompt B</strong>‚Äîthe uncertainty is valuable. If B turns out great, we found a winner. If bad, we‚Äôve ruled it out cheaply.</p>
<p><strong>Why this beats random search</strong>: Random would waste evaluations on prompts we already know are bad. SIMBA focuses budget on <em>decisions that matter</em>‚Äîresolving uncertainty between plausibly-good candidates.</p>
<p><strong>Limitation</strong>: SIMBA efficiently <em>finds</em> the best prompt but doesn‚Äôt understand <em>why</em> it works. The bandit framework treats prompts as black boxes with hidden payout rates‚Äîit can‚Äôt reason about ‚Äúthis prompt works because it specifies output format.‚Äù And like MiPRO, it optimizes aggregate score: a prompt scoring 0.75 uniformly beats one scoring 0.95 on hard cases but 0.60 elsewhere‚Äîeven if the hard-case specialist contains insights worth preserving.</p>
</section>
<section id="gepa-2025-the-reflection-shift" class="level4">
<h4 class="anchored" data-anchor-id="gepa-2025-the-reflection-shift">GEPA (2025): The Reflection Shift</h4>
<p>GEPA breaks from this trajectory in two fundamental ways:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 34%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>What changed</th>
<th>Before GEPA</th>
<th>With GEPA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Learning signal</strong></td>
<td><code>score = 0.6</code></td>
<td>‚ÄúExceeded word limit. Missing keyword. Compiler error: use y not x.‚Äù</td>
</tr>
<tr class="even">
<td><strong>Selection strategy</strong></td>
<td>Best aggregate score</td>
<td>Pareto frontier of diverse specialists</td>
</tr>
</tbody>
</table>
<p><strong>1. From scalar scores to textual feedback</strong> ‚Äî Instead of just knowing <em>that</em> a prompt scored 0.6, GEPA sees <em>why</em>: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.</p>
<p><strong>Example</strong>: OPRO sees <code>score = 0.6</code>. GEPA sees: &gt; ‚ÄúFailed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword ‚Äòdisclaimer‚Äô. Passed examples 1-6, 8-11.‚Äù</p>
<p>The LLM reflects: ‚ÄúI should add an instruction about word limits and required keywords.‚Äù</p>
<p><strong>2. From greedy to Pareto selection</strong> ‚Äî Instead of always promoting the highest-scoring candidate, GEPA maintains a <em><a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto frontier</a></em>: candidates that each excel at <em>something</em> no other candidate beats.</p>
<p><strong>Example</strong>: Three candidates evaluated on 10 examples: - <code>prompt_A</code>: 8/10 overall, but fails hard cases #7 and #9 - <code>prompt_B</code>: 6/10 overall, but nails hard cases #7 and #9<br>
- <code>prompt_C</code>: 7/10 overall, no unique strengths</p>
<p>Greedy selection keeps only <code>prompt_A</code>. Pareto selection keeps both <code>prompt_A</code> <em>and</em> <code>prompt_B</code>‚Äîbecause B‚Äôs insights about hard cases might combine with A‚Äôs general strength. <code>prompt_C</code> gets dropped (dominated by A on everything).</p>
<p>The contrast with prior optimizers is stark: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains why‚Äîand proposes the fix directly.</p>
</section>
</section>
<section id="how-gepa-works-building-it-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-gepa-works-building-it-from-scratch">How GEPA Works: Building It From Scratch</h2>
<p>GEPA combines two key innovations: <strong>reflective prompt mutation</strong> (learning from textual feedback) and <strong>Pareto selection</strong> (preserving diverse specialists). Each is powerful alone; together they compound.</p>
<p>We‚Äôll build them from scratch in this section:</p>
<ol type="1">
<li><p><strong>Reflective mutation</strong> ‚Äî How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.</p></li>
<li><p><strong>Pareto selection</strong> ‚Äî Why always improving your ‚Äúbest‚Äù prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.</p></li>
<li><p><strong>Merge</strong> ‚Äî How GEPA combines insights from divergent lineages (covered after the complete algorithm).</p></li>
</ol>
<p>By the end, you‚Äôll see how these mechanisms combine into GEPA‚Äôs full evolutionary loop.</p>
<hr>
<section id="quick-start-using-gepa-in-30-seconds" class="level3">
<h3 class="anchored" data-anchor-id="quick-start-using-gepa-in-30-seconds">üöÄ Quick Start: Using GEPA in 30 Seconds</h3>
<blockquote class="blockquote">
<p>üìñ <strong>Full tutorial</strong>: <a href="https://dspy.ai/tutorials/gepa_aime/">GEPA for AIME (Math)</a> ‚Äî optimizing GPT-4.1 Mini from 46.6% ‚Üí 56.6% on AIME 2025.</p>
</blockquote>
<p>Before diving deep, here‚Äôs what using GEPA looks like in practice:</p>
<p><strong>Step 1: Configure your language model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dspy</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> dspy.LM(<span class="st">"openai/gpt-4.1-mini"</span>, temperature<span class="op">=</span><span class="dv">1</span>, max_tokens<span class="op">=</span><span class="dv">32000</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>dspy.configure(lm<span class="op">=</span>lm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 2: Define your program</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>program <span class="op">=</span> dspy.ChainOfThought(<span class="st">"problem -&gt; answer"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 3: Define a metric that returns feedback (not just a score)</strong></p>
<p>This is the key difference from other optimizers‚Äîyour metric explains <em>why</em> something failed:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metric_with_feedback(example, prediction, trace<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    correct_answer <span class="op">=</span> example.answer</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    pred_answer <span class="op">=</span> prediction.answer</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> <span class="bu">int</span>(correct_answer <span class="op">==</span> pred_answer)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> score <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="ss">f"Correct! The answer is '</span><span class="sc">{</span>correct_answer<span class="sc">}</span><span class="ss">'."</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="ss">f"Incorrect. Expected '</span><span class="sc">{</span>correct_answer<span class="sc">}</span><span class="ss">', got '</span><span class="sc">{</span>pred_answer<span class="sc">}</span><span class="ss">'."</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add any additional context that could help improvement:</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(example, <span class="st">'solution'</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            feedback <span class="op">+=</span> <span class="ss">f" Solution: </span><span class="sc">{</span>example<span class="sc">.</span>solution<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dspy.Prediction(score<span class="op">=</span>score, feedback<span class="op">=</span>feedback)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 4: Optimize with GEPA</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dspy <span class="im">import</span> GEPA</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> GEPA(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    metric<span class="op">=</span>metric_with_feedback,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    auto<span class="op">=</span><span class="st">"light"</span>,           <span class="co"># Budget preset: "light", "medium", or "heavy"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    num_threads<span class="op">=</span><span class="dv">32</span>,         <span class="co"># Parallel evaluation threads</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>optimized_program <span class="op">=</span> optimizer.<span class="bu">compile</span>(</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    program,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    trainset<span class="op">=</span>train_set,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    valset<span class="op">=</span>val_set,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>That‚Äôs it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.</p>
<hr>
<p><strong>What‚Äôs happening under the hood?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 56%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Textual feedback</strong></td>
<td>Your metric returns <em>why</em> something failed, not just a score</td>
</tr>
<tr class="even">
<td><strong>Reflective mutation</strong></td>
<td>An LLM reads the feedback and proposes improved instructions</td>
</tr>
<tr class="odd">
<td><strong>Pareto selection</strong></td>
<td>Diverse specialists are preserved, not just the ‚Äúbest‚Äù prompt</td>
</tr>
<tr class="even">
<td><strong>Merge operations</strong></td>
<td>Insights from divergent lineages get combined</td>
</tr>
</tbody>
</table>
<p>The rest of this section explains <em>why</em> each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you need‚Äîsee the <a href="https://dspy.ai/api/optimizers/GEPA/overview/">DSPy GEPA API Reference</a> for full parameter details.</p>
<p>Now let‚Äôs make this concrete by building the core mechanism from scratch.</p>
</section>
<section id="hands-on-building-reflective-mutation-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-building-reflective-mutation-from-scratch">Hands-On: Building Reflective Mutation from Scratch</h3>
<p>Let‚Äôs make this concrete by implementing GEPA‚Äôs core mechanism on a real task.</p>
<p><strong>The Problem: AIME Math Competition</strong></p>
<p>We‚Äôll optimize prompts for solving <a href="https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination">AIME</a> (American Invitational Mathematics Examination) problems ‚Äî challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dset <span class="op">=</span> load_dataset(<span class="st">"AI-MO/aimo-validation-aime"</span>)[<span class="st">'train'</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 90 problems with solutions and integer answers</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why AIME for testing prompt optimization?</strong></p>
<ol type="1">
<li><strong>Clear ground truth</strong> ‚Äî Every answer is an integer (0-999), so evaluation is unambiguous</li>
<li><strong>Rich failure modes</strong> ‚Äî Wrong answers come from algebraic errors, missed cases, misread constraints</li>
<li><strong>Domain knowledge helps</strong> ‚Äî Prompts that encode strategies (‚Äúsubtract equations pairwise‚Äù, ‚Äúenumerate all cases‚Äù) measurably improve performance</li>
<li><strong>Small dataset</strong> ‚Äî Only 90 problems, so sample efficiency matters</li>
</ol>
<p><strong>The Setup</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split: 10 train, 10 validation (simulating scarce labeled data)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>tdf <span class="op">=</span> df.sample(<span class="dv">45</span>).iloc[:<span class="dv">10</span>]  <span class="co"># Training mini-batches drawn from here</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>vdf <span class="op">=</span> df.drop(tdf.index).iloc[:<span class="dv">10</span>]  <span class="co"># Held-out validation</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Base model: Gemini 2.5 Flash via LiteLLM</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Metric: Exact match (predicted integer == ground truth)</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metric(ground_truth, prediction):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(ground_truth) <span class="op">==</span> prediction[<span class="st">'answer'</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Seed Prompt</strong></p>
<p>We start with a minimal instruction:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>seed_prompt <span class="op">=</span> <span class="st">"""You are given a problem and you have to give the answer </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="st">along with reasoning. Do not return anything apart from json. </span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="st">It should be parsable by json.loads()"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Baseline validation accuracy: <strong>10%</strong> (1/10 correct)</p>
<p>Can reflective mutation improve this? Let‚Äôs find out.</p>
<hr>
</section>
<section id="step-1-the-feedback-function" class="level3">
<h3 class="anchored" data-anchor-id="step-1-the-feedback-function">Step 1: The Feedback Function</h3>
<p>First, we need a function that tells the reflection LLM what went wrong. We‚Äôll start with <strong>minimal feedback</strong>‚Äîjust the correct answer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback(ground_truth, prediction):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">int</span>(ground_truth) <span class="op">!=</span> prediction[<span class="st">'answer'</span>]:</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f'You got it wrong! The solution is </span><span class="sc">{</span>ground_truth<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'You got it right!'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is deliberately simple. Later we‚Äôll discuss how richer feedback (like expert solutions) can improve results.</p>
<hr>
</section>
<section id="step-2-the-reflection-prompt" class="level3">
<h3 class="anchored" data-anchor-id="step-2-the-reflection-prompt">Step 2: The Reflection Prompt</h3>
<p>Following GEPA‚Äôs structure, we build a prompt that shows the LLM its failures:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>REFLECTION_TEMPLATE <span class="op">=</span> <span class="st">"""I provided an assistant with the following instructions:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;curr_instructions&gt;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="sc">{current_prompt}</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="st">The following are examples with assistant's responses and feedback:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;inputs_outputs_feedback&gt;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="sc">{examples}</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="st">Your task: write a new instruction for the assistant.</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="st">- Read inputs carefully and identify the input format and task description</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="st">- Read all responses and feedback. Identify niche/domain-specific factual information</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="st">- If the assistant used a generalizable strategy, include that in the instruction</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="st">Provide the new instructions.</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mk_reflection_prompt(df, curr_prompt):</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Build reflection prompt from minibatch results."""</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    examples <span class="op">=</span> []</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, row <span class="kw">in</span> df.reset_index().iterrows():</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        example <span class="op">=</span> <span class="ss">f"""# Example </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="ss">## problem</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>row[<span class="st">'problem'</span>]<span class="sc">}</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="ss">## prediction</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>row[<span class="st">'pred'</span>]<span class="sc">}</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="ss">## feedback</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>feedback(row.answer, row.pred)<span class="sc">}</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        examples.append(example)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> REFLECTION_TEMPLATE.<span class="bu">format</span>(</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        current_prompt<span class="op">=</span>curr_prompt,</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        examples<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(examples)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="step-3-the-complete-optimization-loop" class="level3">
<h3 class="anchored" data-anchor-id="step-3-the-complete-optimization-loop">Step 3: The Complete Optimization Loop</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reflect(mb, curr_prompt):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Ask LLM to reflect on failures and propose improved instruction."""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    refl_prompt <span class="op">=</span> mk_reflection_prompt(mb, curr_prompt)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _call(refl_prompt, <span class="bu">format</span><span class="op">=</span>ReflectionModel)[<span class="st">'new_instruction'</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize_prompt(seed_prompt, traindf, valdf, n_iters<span class="op">=</span><span class="dv">3</span>, mb_size<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Greedy reflective prompt optimization."""</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    prompts, train_scores, val_scores <span class="op">=</span> [seed_prompt], [], []</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    mb <span class="op">=</span> traindf.sample(mb_size)  <span class="co"># Fixed minibatch for this run</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Baseline validation: </span><span class="sc">{</span>eval_val(seed_prompt, valdf)<span class="sc">:.2%}</span><span class="ss">'</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate current prompt on minibatch</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        mb_eval, mb_score <span class="op">=</span> eval_mb(prompts[<span class="op">-</span><span class="dv">1</span>], mb)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"üìä Minibatch: </span><span class="sc">{</span>mb_score<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reflect and propose new instruction</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        new_instr <span class="op">=</span> reflect(mb_eval, prompts[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        new_prompt <span class="op">=</span> new_instr  <span class="co"># The new instruction becomes the new prompt</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate on validation set</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        val_score <span class="op">=</span> eval_val(new_prompt, valdf)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"üìä Validation: </span><span class="sc">{</span>val_score<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        prompts.append(new_prompt)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        val_scores.append(val_score)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(prompts<span class="op">=</span>prompts, val_scores<span class="op">=</span>val_scores)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="what-actually-happened" class="level3">
<h3 class="anchored" data-anchor-id="what-actually-happened">What Actually Happened</h3>
<p>Running this on AIME problems with Gemini 2.5 Flash:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 19%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Minibatch</th>
<th>Validation</th>
<th>What the reflection learned</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td>‚Äî</td>
<td>10%</td>
<td>‚Äî</td>
</tr>
<tr class="even">
<td>1</td>
<td>0%</td>
<td>10%</td>
<td>JSON formatting details, output structure rules</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0%</td>
<td><strong>30%</strong></td>
<td>Systems of equations strategy, remainder/modular arithmetic tips</td>
</tr>
<tr class="even">
<td>3</td>
<td>67%</td>
<td><strong>10%</strong></td>
<td>Over-specialized on number theory, solved #9 but lost generality</td>
</tr>
</tbody>
</table>
<p><strong>The good</strong>: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:</p>
<blockquote class="blockquote">
<p><em>‚ÄúWhen dealing with systems of equations like <span class="math inline">\(xy + Az = C\)</span>, <span class="math inline">\(yz + Ax = C\)</span>, <span class="math inline">\(zx + Ay = C\)</span>, consider subtracting equations pairwise to find relationships between variables, such as <span class="math inline">\((x-z)(y-A)=0\)</span>, which implies <span class="math inline">\(x=z\)</span> or <span class="math inline">\(y=A\)</span>. Systematically explore all such cases.‚Äù</em></p>
</blockquote>
<p>This is directly from the actual output‚Äîthe reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.</p>
<p><strong>The bad</strong>: Iteration 3 achieved 67% on its minibatch but dropped to <strong>10% validation</strong>. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:</p>
<blockquote class="blockquote">
<p><em>‚ÄúWhen the problem involves number theory and remainders (e.g., <span class="math inline">\(n \pmod x\)</span>, <span class="math inline">\(n \pmod y\)</span>), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.‚Äù</em></p>
</blockquote>
<p>This over-specialized advice (‚Äúcan lead to an answer of 0‚Äù) actively hurt performance on non-number-theory problems, dropping validation from 30% back to <strong>10%</strong> (1/10)‚Äîthough notably, it did solve problem #9, which earlier prompts couldn‚Äôt.</p>
<hr>
</section>
<section id="the-greedy-selection-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-greedy-selection-problem">The Greedy Selection Problem</h3>
<p>This demonstrates exactly why GEPA uses <strong>Pareto selection</strong> instead of always taking the ‚Äúbest‚Äù prompt:</p>
<ol type="1">
<li><strong>Iteration 2‚Äôs prompt</strong> was a specialist‚Äîit learned something valuable about systems of equations</li>
<li><strong>Iteration 3</strong> tried to improve on iteration 2, but the minibatch had different problems</li>
<li>The reflection <strong>overwrote</strong> the systems-of-equations insight while adding number-theory tips that were <em>too specific</em></li>
<li>Result: <strong>catastrophic forgetting</strong></li>
</ol>
<p>With greedy selection, we would have discarded iteration 2‚Äôs valuable insight. Pareto selection would keep it‚Äîbecause it was <em>best on at least one validation instance</em>.</p>
<hr>
</section>
<section id="the-missing-ingredient-rich-feedback" class="level3">
<h3 class="anchored" data-anchor-id="the-missing-ingredient-rich-feedback">The Missing Ingredient: Rich Feedback</h3>
<p>Our minimal feedback (<code>"You got it wrong! The solution is 349"</code>) only tells the model <em>that</em> it failed, not <em>why</em> or <em>how to fix it</em>.</p>
<p>The AIME dataset includes expert solutions. A richer feedback function could use them:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback_rich(row):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">int</span>(row.answer) <span class="op">!=</span> row.pred[<span class="st">'answer'</span>]:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        sol <span class="op">=</span> row.solution[:<span class="dv">500</span>] <span class="op">+</span> <span class="st">"..."</span> <span class="cf">if</span> <span class="bu">len</span>(row.solution) <span class="op">&gt;</span> <span class="dv">500</span> <span class="cf">else</span> row.solution</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"""Wrong! Expected </span><span class="sc">{</span>row<span class="sc">.</span>answer<span class="sc">}</span><span class="ss">, got </span><span class="sc">{</span>row<span class="sc">.</span>pred[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="ss">        </span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="ss">Model's reasoning: </span><span class="sc">{</span>row<span class="sc">.</span>pred[<span class="st">'short_reasoning'</span>]<span class="sc">}</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="ss">Expert solution approach:</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>sol<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"Correct!"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>With rich feedback, the reflection LLM can extract <em>specific strategies</em> from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: <strong>the feedback contains the fix</strong>.</p>
<p>Compare this to RL, which would only see <code>reward = 0</code> and have to statistically infer what went wrong across thousands of trajectories.</p>
<hr>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li><strong>Reflective mutation works</strong> ‚Äî Even with minimal feedback, the LLM extracted useful domain knowledge</li>
<li><strong>Greedy selection fails</strong> ‚Äî Iteration 3‚Äôs collapse shows why we need to preserve specialist insights</li>
<li><strong>Feedback quality matters</strong> ‚Äî Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with</li>
<li><strong>Sample efficiency is real</strong> ‚Äî We saw meaningful optimization with just 3 iterations on 3 examples each</li>
</ol>
<p>This is the core limitation of greedy optimization: <strong>catastrophic forgetting</strong>. The solution? Pareto selection‚Äîwhich we‚Äôll build from scratch next.</p>
</section>
<section id="hands-on-building-the-pareto-frontier" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-building-the-pareto-frontier">Hands-On: Building the Pareto Frontier</h3>
<p>In the reflective mutation section, we saw greedy selection fail‚Äîiteration 3‚Äôs over-specialized prompt dropped validation from 30% to 10%, losing iteration 2‚Äôs valuable systems-of-equations insights. The fundamental problem: always improving your ‚Äúbest‚Äù prompt discards specialist knowledge.</p>
<p>GEPA‚Äôs solution: <strong>Pareto selection</strong>. Instead of keeping one best prompt, maintain a <em>frontier</em> of prompts where each excels at something no other prompt beats.</p>
<hr>
<section id="what-is-pareto-dominance" class="level4">
<h4 class="anchored" data-anchor-id="what-is-pareto-dominance">What is Pareto Dominance?</h4>
<p>A prompt <strong>dominates</strong> another if it‚Äôs at least as good everywhere, and strictly better somewhere:</p>
<ul>
<li><strong>‚â•</strong> on <em>every</em> validation instance, AND<br>
</li>
<li><strong>&gt;</strong> on <em>at least one</em> instance</li>
</ul>
<p>If prompt A dominates prompt B, we can safely discard B‚ÄîA is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.</p>
<p><strong>Example</strong>: Consider four prompts evaluated on 10 validation instances. We‚Äôll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Instances Solved</th>
<th>Aggregate</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P0 (seed)</td>
<td>#0 only</td>
<td>10%</td>
<td>Dominated by P2</td>
</tr>
<tr class="even">
<td>P1 (iter 1)</td>
<td>#0 only</td>
<td>10%</td>
<td>Dominated by P2</td>
</tr>
<tr class="odd">
<td>P2 (iter 2)</td>
<td>#0, #1, #2</td>
<td>30%</td>
<td><strong>Frontier</strong> ‚úì</td>
</tr>
<tr class="even">
<td>P3 (iter 3)</td>
<td>#9 only</td>
<td>10%</td>
<td><strong>Frontier</strong> ‚úì</td>
</tr>
</tbody>
</table>
<p>P2 dominates both P0 and P1‚Äîit solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which <em>nothing else could</em>.</p>
<p>The Pareto frontier is <strong>{P2, P3}</strong>. Both contain unique value.</p>
<hr>
</section>
<section id="implementation-dominance-checking" class="level4">
<h4 class="anchored" data-anchor-id="implementation-dominance-checking">Implementation: Dominance Checking</h4>
<p>We represent per-instance scores as boolean arrays (1 = solved, 0 = failed):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dominates(candidate_scores, other_scores):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)"""</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    candidate <span class="op">=</span> np.array(candidate_scores)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    other <span class="op">=</span> np.array(other_scores)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (candidate <span class="op">&gt;=</span> other).<span class="bu">all</span>() <span class="kw">and</span> (candidate <span class="op">&gt;</span> other).<span class="bu">any</span>()</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_dominated_by_any(new_scores, frontier_scores):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Is new_scores dominated by ANY prompt in the frontier?"""</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    new <span class="op">=</span> np.array(new_scores)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> existing <span class="kw">in</span> frontier_scores:</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dominates(np.array(existing), new):</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dominated_indices(new_scores, frontier_scores):</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Which frontier prompts does new_scores dominate?"""</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    new <span class="op">=</span> np.array(new_scores)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [i <span class="cf">for</span> i, existing <span class="kw">in</span> <span class="bu">enumerate</span>(frontier_scores) </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> dominates(new, np.array(existing))]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="tracing-through-why-p3-survives" class="level4">
<h4 class="anchored" data-anchor-id="tracing-through-why-p3-survives">Tracing Through: Why P3 Survives</h4>
<p>Let‚Äôs verify the dominance relationships from our example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>P0 <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]  <span class="co"># Solves: #0</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>P1 <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]  <span class="co"># Solves: #0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>P2 <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]  <span class="co"># Solves: #0, #1, #2</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>P3 <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>]  <span class="co"># Solves: #9</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P2 dominate P0?</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>dominates(P2, P0)  <span class="co"># True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P2 dominate P1?</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>dominates(P2, P1)  <span class="co"># True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P2 dominate P3?</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>dominates(P2, P3)  <span class="co"># False! P2 loses on #9 (0 &lt; 1)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P3 dominate P2?</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>dominates(P3, P2)  <span class="co"># False! P3 loses on #0, #1, #2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Neither P2 nor P3 dominates the other‚Äîthey‚Äôre <strong>Pareto incomparable</strong>. Each solves problems the other can‚Äôt. Both stay on the frontier.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/pareto_visual.webp" class="img-fluid figure-img"></p>
<figcaption>Pareto Frontier Explained</figcaption>
</figure>
</div>
<hr>
</section>
<section id="the-complete-frontier-manager" class="level4">
<h4 class="anchored" data-anchor-id="the-complete-frontier-manager">The Complete Frontier Manager</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ParetoFrontier:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prompts <span class="op">=</span> []</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scores <span class="op">=</span> []  <span class="co"># scores[i][j] = prompt i's score on instance j</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add(<span class="va">self</span>, prompt, instance_scores):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Try to add a prompt. Returns True if it joins the frontier."""</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reject if dominated by existing frontier member</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_dominated_by_any(instance_scores, <span class="va">self</span>.scores):</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove any frontier members this prompt dominates</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        dominated <span class="op">=</span> get_dominated_indices(instance_scores, <span class="va">self</span>.scores)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">sorted</span>(dominated, reverse<span class="op">=</span><span class="va">True</span>):  <span class="co"># Remove from end first</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> <span class="va">self</span>.prompts[i]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> <span class="va">self</span>.scores[i]</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add to frontier</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prompts.append(prompt)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scores.append(instance_scores)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>):</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample a prompt, weighted by unique wins."""</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> []</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        scores_arr <span class="op">=</span> np.array(<span class="va">self</span>.scores)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.prompts)):</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># How many instances is this prompt *uniquely* best on?</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            others_best <span class="op">=</span> np.delete(scores_arr, i, axis<span class="op">=</span><span class="dv">0</span>).<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.prompts) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> np.zeros_like(scores_arr[i])</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            unique_wins <span class="op">=</span> (scores_arr[i] <span class="op">&gt;</span> others_best).<span class="bu">sum</span>()</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            weights.append(unique_wins <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># +1 smoothing</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(<span class="va">self</span>.prompts, p<span class="op">=</span>np.array(weights)<span class="op">/</span><span class="bu">sum</span>(weights))</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> best_aggregate(<span class="va">self</span>):</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the prompt with highest aggregate score."""</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        aggregates <span class="op">=</span> [<span class="bu">sum</span>(s) <span class="cf">for</span> s <span class="kw">in</span> <span class="va">self</span>.scores]</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.prompts[np.argmax(aggregates)]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="putting-it-together-pareto-guided-optimization" class="level4">
<h4 class="anchored" data-anchor-id="putting-it-together-pareto-guided-optimization">Putting It Together: Pareto-Guided Optimization</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize_with_pareto(seed_prompt, traindf, valdf, n_iters<span class="op">=</span><span class="dv">5</span>, mb_size<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Reflective mutation with Pareto frontier selection."""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    frontier <span class="op">=</span> ParetoFrontier()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize with seed</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    seed_scores <span class="op">=</span> evaluate_per_instance(seed_prompt, valdf)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    frontier.add(seed_prompt, seed_scores)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Baseline: </span><span class="sc">{</span><span class="bu">sum</span>(seed_scores)<span class="op">/</span><span class="bu">len</span>(seed_scores)<span class="sc">:.1%}</span><span class="ss">'</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">40</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">Iteration </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">40</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample parent from frontier (weighted by unique wins)</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        parent <span class="op">=</span> frontier.sample()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run on minibatch, reflect, propose mutation</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        mb <span class="op">=</span> traindf.sample(mb_size)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        mb_results <span class="op">=</span> evaluate_with_traces(parent, mb)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        new_prompt <span class="op">=</span> reflect_and_mutate(parent, mb_results)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate on full validation set</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        new_scores <span class="op">=</span> evaluate_per_instance(new_prompt, valdf)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        new_agg <span class="op">=</span> <span class="bu">sum</span>(new_scores) <span class="op">/</span> <span class="bu">len</span>(new_scores)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"New prompt: </span><span class="sc">{</span>new_agg<span class="sc">:.1%}</span><span class="ss"> aggregate"</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Try to add to frontier</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> frontier.add(new_prompt, new_scores):</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"‚úì Added to frontier (size: </span><span class="sc">{</span><span class="bu">len</span>(frontier.prompts)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"‚úó Dominated, rejected"</span>)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frontier.best_aggregate()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="what-we-observed-on-aime" class="level4">
<h4 class="anchored" data-anchor-id="what-we-observed-on-aime">What We Observed on AIME</h4>
<p>Running this on AIME problems with Gemini 2.5 Flash:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Iteration</th>
<th>Aggregate</th>
<th>Instances Solved</th>
<th>Frontier Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Seed</td>
<td>10%</td>
<td>#0</td>
<td>Initialize</td>
</tr>
<tr class="even">
<td>1</td>
<td>10%</td>
<td>#0</td>
<td>Dominated by seed, rejected</td>
</tr>
<tr class="odd">
<td>2</td>
<td>30%</td>
<td>#0, #1, #2</td>
<td>Added, dominates seed</td>
</tr>
<tr class="even">
<td>3</td>
<td>10%</td>
<td>#9 only</td>
<td><strong>Added</strong> ‚úì (unique win on #9)</td>
</tr>
</tbody>
</table>
<p><strong>The key moment</strong>: Iteration 3 scored only 10%‚Äîworse than iteration 2‚Äôs 30%. Greedy selection would discard it entirely.</p>
<p>But it solved <strong>instance #9</strong>, which nothing else could. Pareto selection preserves it.</p>
<p>Our final frontier: <strong>{P2, P3}</strong> - P2: Strong generalist (30%), knows systems-of-equations strategies - P3: Instance-9 specialist (10%), knows whatever cracked that specific problem</p>
<p>Both insights survive. The merge operation (covered later) can combine them.</p>
<hr>
</section>
<section id="why-this-matters-no-more-catastrophic-forgetting" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-no-more-catastrophic-forgetting">Why This Matters: No More Catastrophic Forgetting</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Selection Strategy</th>
<th>What happens to specialists</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Greedy</strong></td>
<td>Discarded whenever aggregate score drops</td>
</tr>
<tr class="even">
<td><strong>Pareto</strong></td>
<td>Preserved if they solve <em>anything</em> unique</td>
</tr>
</tbody>
</table>
<p>Greedy selection caused our iteration 3 collapse‚Äîthe number-theory prompt overwrote the algebra prompt‚Äôs insights. Pareto selection prevents this by construction: you can‚Äôt remove a prompt from the frontier unless something else does everything it does, <em>plus more</em>.</p>
<p>This is the core of GEPA‚Äôs sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier <strong>never loses them in the first place</strong>.</p>
<hr>
<p>We just saw Pareto selection preserve iteration 3‚Äôs prompt despite its 10% aggregate score‚Äîbecause it solved instance #9, which nothing else could. But this raises a question: <em>why</em> does keeping ‚Äúlosers‚Äù help optimization? Shouldn‚Äôt we focus resources on the best candidates?</p>
<p>The answer comes from <strong>quality-diversity</strong> algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.</p>
</section>
</section>
<section id="why-pareto-works-quality-diversity-and-map-elites" class="level3">
<h3 class="anchored" data-anchor-id="why-pareto-works-quality-diversity-and-map-elites">Why Pareto Works: Quality-Diversity and Map Elites</h3>
<p>We‚Äôve now built both core mechanisms from scratch‚Äîreflective mutation and Pareto selection. Before diving into the full algorithm, let‚Äôs briefly step back and understand <em>why</em> this approach works so well.</p>
<p>The Pareto frontier isn‚Äôt a novel invention‚Äîit draws from <strong>quality-diversity (QD)</strong> algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.</p>
<section id="the-core-insight" class="level4">
<h4 class="anchored" data-anchor-id="the-core-insight">The Core Insight</h4>
<p>Traditional optimization asks: <em>‚ÄúWhat‚Äôs the single best solution?‚Äù</em></p>
<p>Quality-diversity asks: <em>‚ÄúWhat‚Äôs the best solution of each <em>type</em>?‚Äù</em></p>
<p>Complex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensively‚Äîbut the best engine knows <em>both</em> styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one ‚Äúbest‚Äù approach.</p>
</section>
<section id="map-elites-the-inspiration" class="level4">
<h4 class="anchored" data-anchor-id="map-elites-the-inspiration">Map Elites: The Inspiration</h4>
<p><a href="https://arxiv.org/abs/1504.04909">Map Elites</a> (Mouret &amp; Clune, 2015) maintains an <em>archive</em> organized by behavior:</p>
<ol type="1">
<li><strong>Define behavior dimensions</strong> ‚Äî characteristics describing <em>how</em> a solution works (not just how well)</li>
<li><strong>Discretize into bins</strong> ‚Äî each cell represents a ‚Äúniche‚Äù</li>
<li><strong>Keep the best per bin</strong> ‚Äî new solutions compete only within their niche</li>
<li><strong>Mutate from the archive</strong> ‚Äî sample from any occupied bin, mutate, place in appropriate bin</li>
</ol>
<p>The result: diverse specialists, each optimal <em>of its type</em>. <strong>Key insight</strong>: the archive provides <em>stepping stones</em>‚Äîa mutation from one niche might discover something useful for another. Diversity isn‚Äôt just nice to have; it‚Äôs a search strategy.</p>
</section>
<section id="gepas-adaptation-validation-instances-as-niches" class="level4">
<h4 class="anchored" data-anchor-id="gepas-adaptation-validation-instances-as-niches">GEPA‚Äôs Adaptation: Validation Instances as Niches</h4>
<p>GEPA recognizes that <strong>the validation set itself defines the behavior space</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 66%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Map Elites</th>
<th>GEPA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Behavior = continuous dimensions</td>
<td>Behavior = which validation instances are solved</td>
</tr>
<tr class="even">
<td>Bins = discretized regions</td>
<td>‚ÄúBins‚Äù = individual validation instances</td>
</tr>
<tr class="odd">
<td>Archive = best per bin</td>
<td>Pareto frontier = non-dominated prompts across instances</td>
</tr>
</tbody>
</table>
<p>Each validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the ‚Äúspecialist for that niche‚Äù‚Äîit stays because it demonstrates <em>something works</em>, even with low aggregate score.</p>
<p><strong>Concrete example</strong>: In our AIME experiment, instance #9 was a number theory problem requiring CRT. Prompt P3 (10% aggregate) solved it; P2 (30% aggregate) couldn‚Äôt. In Map Elites terms, P3 is the ‚Äúelite‚Äù for the number-theory niche‚Äîit stays in the archive despite low overall score.</p>
<blockquote class="blockquote">
<p><em>‚ÄúBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>This directly motivated GEPA‚Äôs design: Pareto selection over per-instance scores naturally implements QD‚Äôs ‚Äúbest per niche‚Äù principle, while the merge operation recombines insights across niches‚Äîexactly what stepping-stone search requires.</p>
</section>
<section id="exploration-exploitation-balance" class="level4">
<h4 class="anchored" data-anchor-id="exploration-exploitation-balance">Exploration-Exploitation Balance</h4>
<p>GEPA‚Äôs Pareto selection handles the exploration-exploitation tradeoff automatically: candidates are sampled weighted by unique wins, so specialists get attention proportional to their unique value. The frontier self-prunes‚Äînoise gets dominated away, genuine diversity persists. No manual tuning required.</p>
<p>With both reflective mutation and Pareto selection in place, there‚Äôs one more operation that makes GEPA powerful: <strong>merge</strong>‚Äîcombining insights from divergent lineages. Let‚Äôs look at the complete algorithm.</p>
</section>
</section>
<section id="the-complete-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-complete-algorithm">The Complete Algorithm</h3>
<p>Now that we‚Äôve built reflective mutation and Pareto selection from scratch, and understand <em>why</em> quality-diversity works, let‚Äôs see how all the pieces‚Äîincluding lineage tracking and merge‚Äîcombine into GEPA‚Äôs full optimization loop.</p>
<section id="algorithm-overview" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-overview">Algorithm Overview</h4>
<pre><code>GEPA(base_prompt, trainset, valset, max_iterations):
    
    # Initialize
    candidates = [base_prompt]
    scores = {base_prompt: evaluate_all(base_prompt, valset)}
    pareto_frontier = [base_prompt]
    lineage = {base_prompt: None}  # parent tracking
    
    for iteration in 1..max_iterations:
        
        # 1. SAMPLE: Select candidate from Pareto frontier
        #    Weighted by number of instances where candidate is best
        parent = sample_from_frontier(pareto_frontier, scores)
        
        # 2. PROPOSE: Either mutate or merge
        if should_merge(pareto_frontier, iteration):
            # Select second parent from different lineage
            other_parent = select_divergent_candidate(pareto_frontier, parent, lineage)
            new_prompt = merge(parent, other_parent)
        else:
            # Run rollouts, collect feedback, reflect
            minibatch = sample(trainset, k=3)
            traces, feedback = run_with_feedback(parent, minibatch)
            # See "Hands-On: Building Reflective Mutation" for the reflection prompt structure
            new_prompt = reflect_and_mutate(parent, traces, feedback)
        
        # 3. EVALUATE: Mini-batch gate, then full evaluation
        parent_mb_score = evaluate(parent, minibatch)
        new_mb_score = evaluate(new_prompt, minibatch)
        if new_mb_score &lt;= parent_mb_score:
            continue  # Reject: didn't improve on mini-batch
        
        new_scores = evaluate_all(new_prompt, valset)
        
        # 4. UPDATE: Pareto frontier maintenance
        if is_dominated(new_scores, pareto_frontier):
            continue  # Reject: dominated by existing candidate
        
        # Remove any candidates the new one dominates
        pareto_frontier = [c for c in pareto_frontier 
                          if not dominates(new_scores, scores[c])]
        
        # Add new candidate
        candidates.append(new_prompt)
        scores[new_prompt] = new_scores
        pareto_frontier.append(new_prompt)
        lineage[new_prompt] = parent
    
    # Return best aggregate for deployment; full frontier available via track_stats
    return best_aggregate(pareto_frontier, scores)</code></pre>
<hr>
</section>
<section id="the-key-decision-points" class="level4">
<h4 class="anchored" data-anchor-id="the-key-decision-points">The Key Decision Points</h4>
<p><strong>1. Candidate Sampling (Exploration vs Exploitation)</strong></p>
<p>Rather than always improving the single best prompt (greedy), GEPA samples from the entire Pareto frontier. The sampling weight is proportional to how many validation instances the candidate is <em>uniquely best</em> on:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_from_frontier(frontier, scores):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> []</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> candidate <span class="kw">in</span> frontier:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Count instances where this candidate beats all others</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        unique_wins <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_instances) </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> scores[candidate][i] <span class="op">&gt;</span> <span class="bu">max</span>(scores[other][i] </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>                                                      <span class="cf">for</span> other <span class="kw">in</span> frontier <span class="cf">if</span> other <span class="op">!=</span> candidate))</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        weights.append(unique_wins <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># +1 smoothing</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random.choices(frontier, weights<span class="op">=</span>weights)[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This focuses effort on candidates that have demonstrated <em>unique value</em>‚Äîthey solve something nothing else can. Specialists get attention proportional to their specialization.</p>
<p><strong>2. Mutation vs Merge Decision</strong></p>
<p>Early iterations favor mutation (exploring from single candidates). As the frontier diversifies, merge becomes more valuable. The following is a simplified illustration of the decision logic:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> should_merge(frontier, iteration):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(frontier) <span class="op">&lt;</span> <span class="dv">2</span>: <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">&lt;</span> <span class="dv">5</span>: <span class="cf">return</span> <span class="va">False</span>  <span class="co"># Let lineages diverge first</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Merge with probability proportional to frontier diversity</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    n_lineages <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(get_root(c) <span class="cf">for</span> c <span class="kw">in</span> frontier))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random.random() <span class="op">&lt;</span> (n_lineages <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> <span class="bu">len</span>(frontier)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>(The actual GEPA implementation may use different thresholds‚Äîsee the <a href="https://arxiv.org/abs/2507.19457">paper</a> for details.)</em></p>
<p><strong>3. Mini-Batch Gating</strong></p>
<p>Before expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:</p>
<blockquote class="blockquote">
<p><em>‚ÄúWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p><strong>4. Pareto Update</strong></p>
<p>The frontier update follows the dominance logic we implemented earlier: - <strong>Reject</strong> new candidates dominated by existing ones (they add nothing) - <strong>Remove</strong> existing candidates dominated by the new one (they‚Äôre obsolete) - <strong>Keep</strong> all non-dominated candidates (each offers unique value)</p>
<hr>
</section>
<section id="complexity-analysis" class="level4">
<h4 class="anchored" data-anchor-id="complexity-analysis">Complexity Analysis</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 64%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mutation (3-4 rollouts + reflection)</td>
<td>3-4 LLM calls + 1 reflection call</td>
</tr>
<tr class="even">
<td>Mini-batch evaluation</td>
<td>3-4 metric evaluations</td>
</tr>
<tr class="odd">
<td>Full validation evaluation</td>
<td>N metric evaluations (N = valset size)</td>
</tr>
<tr class="even">
<td>Pareto check</td>
<td>O(F √ó N) comparisons (F = frontier size)</td>
</tr>
</tbody>
</table>
<p>The mini-batch gate is crucial for efficiency. Most mutations fail‚Äîthey either don‚Äôt improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.</p>
<hr>
</section>
<section id="why-each-component-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-each-component-matters">Why Each Component Matters</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 37%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Without it</th>
<th>With it</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Textual feedback</strong></td>
<td>Optimizer sees only <code>score=0.6</code></td>
<td>Optimizer reads ‚Äúwrong answer, expected X, got Y because‚Ä¶‚Äù</td>
</tr>
<tr class="even">
<td><strong>Pareto selection</strong></td>
<td>Specialists discarded when aggregate drops</td>
<td>Specialists preserved if they solve anything unique</td>
</tr>
<tr class="odd">
<td><strong>Lineage tracking</strong></td>
<td>No memory of evolutionary history</td>
<td>Can identify divergent branches for merge</td>
</tr>
<tr class="even">
<td><strong>Merge operation</strong></td>
<td>Insights stay siloed in separate branches</td>
<td>Orthogonal discoveries can combine</td>
</tr>
<tr class="odd">
<td><strong>Mini-batch gating</strong></td>
<td>Evaluate every candidate fully</td>
<td>Reject obvious failures cheaply</td>
</tr>
</tbody>
</table>
<p>The components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.</p>
<hr>
</section>
<section id="what-gets-returned" class="level4">
<h4 class="anchored" data-anchor-id="what-gets-returned">What Gets Returned</h4>
<p>The algorithm returns <code>best_aggregate(pareto_frontier)</code>‚Äîthe prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use <code>track_stats=True</code> to access all candidates and their per-instance scores.</p>
<hr>
<p><em>Next: The merge operation in detail‚Äîhow GEPA combines insights from divergent lineages.</em></p>
</section>
</section>
<section id="the-lineage-tree-and-system-aware-merge" class="level3">
<h3 class="anchored" data-anchor-id="the-lineage-tree-and-system-aware-merge">The Lineage Tree and System-Aware Merge</h3>
<p>We touched on merge briefly in the algorithm overview‚Äînow let‚Äôs see why it‚Äôs essential and how it actually works.</p>
<p><strong>What ‚Äúsystem-aware‚Äù means</strong>: Unlike genetic algorithm crossover (which blindly swaps segments), GEPA‚Äôs merge uses an LLM that <em>understands</em> what it‚Äôs combining‚Äîit can resolve contradictions, synthesize complementary strategies, and produce coherent instructions rather than jumbled concatenations.</p>
<p>Pareto selection preserves specialists‚Äîbut it creates a new problem: <strong>insights get siloed in separate branches</strong>.</p>
<p>Consider what happens after 10 iterations of GEPA on AIME problems:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/Merge_Visual.webp" class="img-fluid figure-img"></p>
<figcaption>Lineage Tree with Merge Operation</figcaption>
</figure>
</div>
<p>The P2‚ÜíP4 lineage accumulated <strong>algebra insights</strong>. The P3‚ÜíP5 lineage accumulated <strong>number theory insights</strong>. Both survive on the Pareto frontier because each solves problems the other can‚Äôt.</p>
<p>But what about a problem requiring <em>both</em>?</p>
<hr>
<section id="the-recombination-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-recombination-problem">The Recombination Problem</h4>
<p>Suppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:</p>
<ul>
<li>P4 knows to subtract equations pairwise, but doesn‚Äôt think to reduce mod p</li>
<li>P5 knows CRT, but misses the algebraic setup</li>
</ul>
<p>With mutation alone, P4 would need to <em>independently rediscover</em> number theory (which P5 already knows), or vice versa. The knowledge exists in our population‚Äîjust in different branches.</p>
<p><strong>Merge</strong> solves this by combining insights from divergent lineages into a single candidate.</p>
<hr>
</section>
<section id="how-merge-works" class="level4">
<h4 class="anchored" data-anchor-id="how-merge-works">How Merge Works</h4>
<p>GEPA‚Äôs merge is ‚Äúsystem-aware‚Äù‚Äîit understands <em>what</em> it‚Äôs combining, not just concatenating strings. The reflection LLM receives:</p>
<ol type="1">
<li><strong>Both parent prompts</strong> with their full instruction text</li>
<li><strong>Lineage context</strong> ‚Äî what types of problems each lineage solved</li>
<li><strong>Conflict guidance</strong> ‚Äî instructions to resolve contradictions, not ignore them</li>
</ol>
<p>The prompt asks the LLM to <em>synthesize</em>, not concatenate:</p>
<blockquote class="blockquote">
<p>‚ÄúCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don‚Äôt simply concatenate‚Äîsynthesize into coherent guidance.‚Äù</p>
</blockquote>
<p><strong>Concrete example</strong> ‚Äî merging our AIME specialists:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 30%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Parent</th>
<th>Specialty</th>
<th>Key instruction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P4</td>
<td>Algebra</td>
<td>‚ÄúSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.‚Äù</td>
</tr>
<tr class="even">
<td>P5</td>
<td>Number theory</td>
<td>‚ÄúApply CRT for modular constraints. Check for contradictions.‚Äù</td>
</tr>
</tbody>
</table>
<p><strong>Merged offspring P6:</strong> &gt; ‚ÄúApproach: (1) For equation systems, subtract pairwise to expose factor relationships‚Äîenumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.‚Äù</p>
<p>P6 inherits both toolkits AND adds <strong>meta-knowledge</strong> about when to combine them. This is the ‚Äúsystem-aware‚Äù part‚Äîthe merge understands the semantics of what it‚Äôs combining.</p>
<hr>
</section>
<section id="when-to-merge-vs.-mutate" class="level4">
<h4 class="anchored" data-anchor-id="when-to-merge-vs.-mutate">When to Merge vs.&nbsp;Mutate</h4>
<p>GEPA alternates between operations based on frontier state:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Condition</th>
<th>Operation</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Early iterations (&lt; 5)</td>
<td>Mutate</td>
<td>Let lineages diverge first; nothing to merge yet</td>
</tr>
<tr class="even">
<td>Frontier has one dominant lineage</td>
<td>Mutate</td>
<td>No orthogonal insights to combine</td>
</tr>
<tr class="odd">
<td>Frontier has divergent specialists</td>
<td>Merge</td>
<td>Recombine discoveries from parallel explorations</td>
</tr>
<tr class="even">
<td>Recent merge succeeded</td>
<td>Mutate</td>
<td>Refine the merged candidate</td>
</tr>
</tbody>
</table>
<p>The paper describes the decision:</p>
<blockquote class="blockquote">
<p><em>‚ÄúAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<hr>
</section>
<section id="merge-vs.-genetic-algorithm-crossover" class="level4">
<h4 class="anchored" data-anchor-id="merge-vs.-genetic-algorithm-crossover">Merge vs.&nbsp;Genetic Algorithm Crossover</h4>
<p>GEPA‚Äôs merge is analogous to crossover in genetic algorithms, but smarter:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 62%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Genetic Algorithms</th>
<th>GEPA Merge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Genes = bit positions</td>
<td>Insights = natural language instructions</td>
</tr>
<tr class="even">
<td>Crossover = swap bit segments randomly</td>
<td>Merge = LLM synthesizes with understanding</td>
</tr>
<tr class="odd">
<td>Can create invalid offspring</td>
<td>Can resolve contradictions</td>
</tr>
<tr class="even">
<td>Blind to semantics</td>
<td>Aware of what instructions <em>mean</em></td>
</tr>
</tbody>
</table>
<p>Random crossover might produce: ‚ÄúSubtract equations pairwise. Apply CRT. Subtract equations pairwise.‚Äù (nonsense duplication)</p>
<p>LLM merge produces: ‚ÄúSet up algebra first, then apply number-theoretic constraints.‚Äù (coherent synthesis)</p>
<hr>
</section>
<section id="the-compounding-effect" class="level4">
<h4 class="anchored" data-anchor-id="the-compounding-effect">The Compounding Effect</h4>
<p>The components reinforce each other:</p>
<ol type="1">
<li><strong>Pareto selection</strong> preserves the diversity that makes merge valuable</li>
<li><strong>Lineage tracking</strong> identifies which candidates come from divergent branches<br>
</li>
<li><strong>Merge</strong> recombines orthogonal discoveries into unified candidates</li>
<li><strong>Pareto selection</strong> then preserves successful merges alongside remaining specialists</li>
</ol>
<p>Without Pareto selection, there‚Äôs nothing interesting to merge‚Äîyou‚Äôd just have variants of one ‚Äúbest‚Äù prompt. Without merge, insights stay siloed even when the frontier is diverse.</p>
<p>Mutation explores <em>depth</em>‚Äîrefining one approach through successive reflections.<br>
Merge explores <em>breadth</em>‚Äîcombining orthogonal discoveries from parallel paths.</p>
<p>Together, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.</p>
</section>
</section>
<section id="what-gepa-learns-domain-specific-knowledge-encoding" class="level3">
<h3 class="anchored" data-anchor-id="what-gepa-learns-domain-specific-knowledge-encoding">What GEPA Learns: Domain-Specific Knowledge Encoding</h3>
<p>We‚Äôve built the full algorithm‚Äîreflective mutation, Pareto selection, merge. But what does all this machinery actually <em>produce</em>? Let‚Äôs examine the output: prompts that encode domain expertise.</p>
<p>One of GEPA‚Äôs most striking capabilities is its ability to <strong>encode domain-specific knowledge directly into prompts</strong>‚Äîtransforming tacit expertise into explicit instructions that persist across examples.</p>
<section id="prompts-as-knowledge-containers" class="level4">
<h4 class="anchored" data-anchor-id="prompts-as-knowledge-containers">Prompts as Knowledge Containers</h4>
<p>Traditional optimization treats prompts as opaque strings to be scored. GEPA treats them as <strong>knowledge containers</strong> that accumulate insights through the reflection loop:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/failure_acc_experience.webp" class="img-fluid figure-img"></p>
<figcaption>Failure Accumulation Experience</figcaption>
</figure>
</div>
<p>Each iteration doesn‚Äôt just fix one error‚Äîit extracts the <em>lesson</em> behind the error.</p>
<p><strong>Concrete example from our AIME experiments:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Prompt excerpt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Seed</strong></td>
<td>‚ÄúYou are given a problem and you have to give the answer along with reasoning.‚Äù</td>
</tr>
<tr class="even">
<td><strong>After iter 2</strong></td>
<td>‚Äú‚Ä¶For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.‚Äù</td>
</tr>
<tr class="odd">
<td><strong>After iter 3</strong></td>
<td>‚Äú‚Ä¶When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.‚Äù</td>
</tr>
</tbody>
</table>
<p>The prompt evolved from generic instruction to encoding <strong>competition math heuristics</strong>. The LLM didn‚Äôt invent these‚Äîit extracted them from its training knowledge, triggered by seeing specific failure modes.</p>
</section>
<section id="three-categories-of-captured-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="three-categories-of-captured-knowledge">Three Categories of Captured Knowledge</h4>
<p>We observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):</p>
<p><strong>1. Format and Interface Knowledge</strong> - Output schemas (‚Äúreturn JSON with keys: answer, reasoning‚Äù) - API conventions (‚Äúuse library.method(), not library_method()‚Äù) - Parsing requirements (‚Äúintegers only, no leading zeros‚Äù)</p>
<p>This is easiest to extract‚Äîformat errors produce explicit feedback.</p>
<p><strong>2. Strategic Knowledge</strong> - Problem-solving heuristics (‚Äútry small cases first‚Äù) - Domain patterns (‚Äúcompetition problems often combine algebra and number theory‚Äù) - Failure mode awareness (‚Äúwatch for off-by-one errors in counting‚Äù)</p>
<p>This emerges from reflecting on <em>why</em> approaches failed, not just <em>that</em> they failed.</p>
<p><strong>3. Factual Domain Knowledge</strong> - API names and signatures (‚Äútorch.einsum, not torch.einstein_sum‚Äù) - Domain constants (‚Äústandard gravity = 9.81 m/s¬≤‚Äù) - Constraint relationships (‚Äúin valid Sudoku, each row/column/box contains 1-9 exactly once‚Äù)</p>
<p>The LLM already knows this‚Äîreflection surfaces it into the prompt where it‚Äôs consistently applied.</p>
</section>
<section id="why-prompts-beat-weights-sometimes" class="level4">
<h4 class="anchored" data-anchor-id="why-prompts-beat-weights-sometimes">Why Prompts Beat Weights (Sometimes)</h4>
<p>Fine-tuning encodes knowledge in model weights‚Äîopaque, distributed, hard to inspect. GEPA encodes knowledge in natural language‚Äîreadable, editable, composable.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>Fine-tuning</th>
<th>GEPA prompts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Inspectability</strong></td>
<td>Black box</td>
<td>Human-readable instructions</td>
</tr>
<tr class="even">
<td><strong>Editability</strong></td>
<td>Requires retraining</td>
<td>Edit the text directly</td>
</tr>
<tr class="odd">
<td><strong>Composability</strong></td>
<td>Train new model</td>
<td>Merge prompt sections</td>
</tr>
<tr class="even">
<td><strong>Sample efficiency</strong></td>
<td>Thousands of examples</td>
<td>Tens of examples</td>
</tr>
</tbody>
</table>
<p>A GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.</p>
</section>
<section id="the-preservation-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-preservation-problem">The Preservation Problem</h4>
<p>Knowledge accumulation has a failure mode: <strong>new insights can overwrite old ones</strong>. We saw this in our hands-on experiment‚Äîiteration 3‚Äôs number theory insights overwrote iteration 2‚Äôs algebra insights, causing catastrophic forgetting.</p>
<p>This is precisely why GEPA uses <strong>Pareto selection</strong>. By preserving prompts that are best on <em>any</em> validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier‚Äîand the merge operation can later combine their insights.</p>
<blockquote class="blockquote">
<p><em>Without Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.</em></p>
</blockquote>
</section>
<section id="limitation-knowledge-must-be-triggerable" class="level4">
<h4 class="anchored" data-anchor-id="limitation-knowledge-must-be-triggerable">Limitation: Knowledge Must Be Triggerable</h4>
<p>GEPA can only surface knowledge the base LLM already has. If the model doesn‚Äôt know that ‚ÄúCRT‚Äù means Chinese Remainder Theorem, no amount of reflection will discover it. The optimization extracts and organizes existing knowledge‚Äîit doesn‚Äôt create new knowledge.</p>
<p>This is why GEPA works better with stronger base models: more latent knowledge to extract. And why, for domains requiring knowledge the LLM lacks, you‚Äôll need to inject it through few-shot examples, retrieval augmentation, or explicit instructions.</p>
<blockquote class="blockquote">
<p><em>‚ÄúLanguage models already have built a lot of prior knowledge‚Ä¶ we can use all of this natural language information to make the LLM itself reflect on its mistakes and improve.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<hr>
<p><em>Now that we understand the algorithm and what it produces, let‚Äôs look at how to use GEPA in practice.</em></p>
</section>
</section>
</section>
<section id="beyond-training-gepa-for-inference-time-search" class="level2">
<h2 class="anchored" data-anchor-id="beyond-training-gepa-for-inference-time-search">Beyond Training: GEPA for Inference-Time Search</h2>
<p>Everything we‚Äôve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.</p>
<section id="two-paradigms-of-operation" class="level3">
<h3 class="anchored" data-anchor-id="two-paradigms-of-operation">Two Paradigms of Operation</h3>
<p><strong>Train-then-generalize</strong> (what we‚Äôve built so far): - Optimize prompts on a training set - Select the best-aggregate prompt from the Pareto frontier - Deploy that prompt on new, unseen tasks - Goal: learn <em>generalizable</em> lessons that transfer</p>
<p><strong>Test-time search</strong> (inference-time optimization): - You have a batch of hard tasks you need to solve <em>now</em> - Optimize directly on the tasks themselves - GEPA searches for solutions, storing the best prompt <em>per task</em> - Goal: maximize performance on <em>these specific instances</em></p>
<p>The mechanics are identical‚Äîreflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you‚Äôre using GEPA as a <strong>search algorithm</strong> over the solution space. This aligns with the broader trend of <a href="https://openai.com/index/learning-to-reason-with-llms/">inference-time compute scaling</a>‚Äîinvesting more computation at inference to solve harder problems.</p>
<p><strong>The key mechanical change</strong>: pass <code>valset=trainset</code> (or equivalently, omit <code>valset</code> and set <code>trainset</code> to your target tasks). This tells GEPA to optimize directly on the problems you‚Äôre trying to solve rather than holding out a separate validation set. See the <a href="https://dspy.ai/api/optimizers/GEPA/overview/">GEPA API documentation</a> for full parameter details.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test-time search: optimize directly on the tasks you want to solve</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>optimized <span class="op">=</span> GEPA(metric<span class="op">=</span>metric_with_feedback, auto<span class="op">=</span><span class="st">"medium"</span>).<span class="bu">compile</span>(</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    program,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    trainset<span class="op">=</span>hard_problems,  <span class="co"># The actual tasks you need solved</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    valset<span class="op">=</span>hard_problems,    <span class="co"># Same set‚Äîno held-out validation</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p><em>‚ÄúGiven a batch of tasks that we want to solve and given some budget‚Ä¶ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<hr>
</section>
<section id="why-gepa-beats-high-temperature-sampling" class="level3">
<h3 class="anchored" data-anchor-id="why-gepa-beats-high-temperature-sampling">Why GEPA Beats High-Temperature Sampling</h3>
<p>Traditional inference-time compute strategies generate many candidates by sampling at high temperature, then use a verifier or LLM-as-judge to pick the best one. But these samples tend to be <em>similar</em>‚Äîvariations on the same approach.</p>
<p>Recent research on <a href="https://arxiv.org/abs/2408.03314">test-time compute scaling</a> (Snell et al., 2024) shows that ‚Äúcompute-optimal‚Äù strategies‚Äîwhich adaptively allocate inference budget based on problem difficulty‚Äîcan improve efficiency by <strong>more than 4x</strong> compared to traditional best-of-N sampling. In some cases, smaller models with optimized test-time compute outperform models <strong>14x larger</strong> that don‚Äôt use additional inference-time computation.</p>
<p><strong>The intuition</strong>: High-temperature sampling produces variations <em>around</em> the same approach‚Äîdifferent variable names, slightly different loop bounds, minor syntactic choices. GEPA‚Äôs reflective mutation proposes <em>structurally different</em> strategies based on what went wrong. When the feedback says ‚Äúmemory bandwidth bottleneck,‚Äù the next candidate might switch from a naive loop to shared-memory tiling‚Äîa qualitative change that temperature variation rarely discovers.</p>
<p>The Pareto frontier then preserves these diverse strategies rather than converging on a single ‚Äúbest‚Äù approach.</p>
<p>GEPA induces <strong>genuine diversity</strong> through two mechanisms:</p>
<ol type="1">
<li><strong>Pareto tracking</strong> ‚Äî maintains candidates that each excel at <em>something different</em>, not just the single highest scorer</li>
<li><strong>Reflective mutation</strong> ‚Äî proposes structurally different approaches based on <em>what went wrong</em>, not random perturbations</li>
</ol>
<blockquote class="blockquote">
<p><em>‚ÄúDue to the way GEPA operates where it‚Äôs doing the Pareto candidate tracking and it‚Äôs doing reflective mutation, we find that GEPA is itself inducing a huge amount of diversity in the kinds of solutions that it generates.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>Rather than 100 slight variations of one approach, GEPA maintains a frontier of genuinely different strategies.</p>
<p><strong>Concrete results</strong>: On the <a href="https://arxiv.org/abs/2103.03874">MATH benchmark</a>, GEPA achieves <strong>93% accuracy</strong> compared to 67% with basic DSPy ChainOfThought‚Äîa 26 percentage point improvement from prompt optimization alone, no fine-tuning required.</p>
<hr>
</section>
<section id="self-bootstrapping-how-iteration-compounds" class="level3">
<h3 class="anchored" data-anchor-id="self-bootstrapping-how-iteration-compounds">Self-Bootstrapping: How Iteration Compounds</h3>
<p>GEPA‚Äôs iterative process creates a self-bootstrapping dynamic:</p>
<ol type="1">
<li><strong>Round 1</strong>: Generate rollouts ‚Üí get feedback (e.g., compiler error) ‚Üí reflect ‚Üí propose fix</li>
<li><strong>Round 2</strong>: The code compiles, but now there‚Äôs a runtime error (division by zero) ‚Üí reflect ‚Üí propose fix</li>
<li><strong>Round 3</strong>: Runtime works, but output is wrong ‚Üí reflect ‚Üí propose fix</li>
</ol>
<blockquote class="blockquote">
<p><em>‚ÄúIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it‚Äôs kind of like self-bootstrapping data to train itself.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>Each iteration surfaces a new failure mode and proposes a solution‚Äîgenerating increasingly challenging training signal from the task itself.</p>
<hr>
</section>
<section id="cross-task-transfer-within-a-batch" class="level3">
<h3 class="anchored" data-anchor-id="cross-task-transfer-within-a-batch">Cross-Task Transfer Within a Batch</h3>
<p>When solving related tasks (e.g., a batch of <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">CUDA kernels</a>), insights compound across the batch:</p>
<blockquote class="blockquote">
<p><em>‚ÄúAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>The frontier maintains multiple specialized prompts simultaneously:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Specialization</th>
<th>Problems solved</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P_conv</td>
<td>Convolutional operators</td>
<td>#1, #4, #7</td>
</tr>
<tr class="even">
<td>P_reduce</td>
<td>Reduction/summation operators</td>
<td>#2, #5, #8</td>
</tr>
<tr class="odd">
<td>P_matmul</td>
<td>Matrix multiplication</td>
<td>#3, #6</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><em>‚ÄúOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>When a new problem arrives, GEPA tries candidates from across the frontier‚Äîthe convolution specialist might crack it, or the reduction specialist, or insights from both might merge.</p>
<hr>
</section>
<section id="what-gepa-stores" class="level3">
<h3 class="anchored" data-anchor-id="what-gepa-stores">What GEPA Stores</h3>
<p>For each task in the batch, GEPA tracks both artifacts:</p>
<blockquote class="blockquote">
<p><em>‚ÄúGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<ul>
<li><strong>Best outputs</strong> ‚Äî the actual solutions, ready to use</li>
<li><strong>Best prompts</strong> ‚Äî specialized strategies representing different subdomains of your problem space</li>
</ul>
<p>You can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.</p>
<hr>
</section>
<section id="feedback-design-for-each-paradigm" class="level3">
<h3 class="anchored" data-anchor-id="feedback-design-for-each-paradigm">Feedback Design for Each Paradigm</h3>
<p>The paradigm choice shapes how you design feedback:</p>
<p><strong>For train-then-generalize</strong> ‚Äî extract transferable lessons:</p>
<blockquote class="blockquote">
<p><em>‚ÄúYou will ensure that there is no task-specific insights that the prompt captures. So you will write in your feedback something along the lines of ‚Äòtry to extract lessons out of this.‚Äô‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metric_generalizable(gold, pred, trace<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> <span class="bu">int</span>(gold.answer <span class="op">==</span> pred.answer)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> score <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> (</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"Failed: expected </span><span class="sc">{</span>gold<span class="sc">.</span>answer<span class="sc">}</span><span class="ss">, got </span><span class="sc">{</span>pred<span class="sc">.</span>answer<span class="sc">}</span><span class="ss">. "</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"Extract a GENERAL lesson that would help on similar problems."</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="st">"Correct. What general strategy led to success?"</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"score"</span>: score, <span class="st">"feedback"</span>: feedback}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>For test-time search</strong> ‚Äî hyper-specialize to the task:</p>
<blockquote class="blockquote">
<p><em>‚ÄúIf you‚Äôre doing simply an inference time search where you just care about the final outputs, then you will try to provide feedback which is as hyper-specialized to the task as possible.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metric_specialized(gold, pred, trace<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> <span class="bu">int</span>(gold.answer <span class="op">==</span> pred.answer)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> score <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> (</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"Your code failed with this particular compiler error: </span><span class="sc">{</span>pred<span class="sc">.</span>error<span class="sc">}</span><span class="ss">. "</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"Try to improve on this specific thing."</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="st">"Correct."</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"score"</span>: score, <span class="st">"feedback"</span>: feedback}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="the-tutor-analogy-background-optimization-loops" class="level3">
<h3 class="anchored" data-anchor-id="the-tutor-analogy-background-optimization-loops">The Tutor Analogy: Background Optimization Loops</h3>
<p>This iterative pattern mirrors how students already work with AI tutoring systems:</p>
<ol type="1">
<li>Attempt problem ‚Üí get solution</li>
<li>See an error ‚Üí receive feedback</li>
<li>Tutor explains ‚Üí improved understanding</li>
<li>Repeat until mastered</li>
</ol>
<p>This suggests an intriguing application: <strong>background GEPA loops for personalization</strong>. Similar patterns are emerging in tools like <a href="https://www.cursor.com/">Cursor</a> and other AI-assisted development environments.</p>
<blockquote class="blockquote">
<p><em>‚ÄúFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt‚Ä¶ cursor can learn a user-specific prompt that works well specifically for you.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>Every time you provide feedback on an AI-generated solution, a background loop could update not just the current response but a <em>user-specific prompt</em> capturing your preferences and patterns.</p>
<hr>
</section>
<section id="when-not-to-use-test-time-gepa" class="level3">
<h3 class="anchored" data-anchor-id="when-not-to-use-test-time-gepa">When NOT to Use Test-Time GEPA</h3>
<ul>
<li><strong>Sparse feedback</strong> ‚Äî If your metric only returns pass/fail with no explanation, GEPA can‚Äôt extract lessons</li>
<li><strong>Tasks outside LLM knowledge</strong> ‚Äî GEPA surfaces knowledge the model already has; genuinely novel information won‚Äôt emerge from reflection. For such domains, consider <a href="https://arxiv.org/abs/2005.11401">retrieval-augmented generation</a> to inject external knowledge</li>
<li><strong>Tight latency constraints</strong> ‚Äî Test-time GEPA typically runs 5-20 iterations, each involving LLM calls for rollout + reflection. Budget 30 seconds to 5 minutes per task depending on model latency and iteration count. Interactive use cases need a different approach.</li>
<li><strong>Identical tasks</strong> ‚Äî If your batch contains <em>identical</em> problems (not just related ones), optimize once and reuse. Cross-task transfer helps when problems are <em>related but distinct</em>‚Äîlike different CUDA kernels that share optimization patterns but have different structures.</li>
</ul>
<hr>
<p>When the conditions <em>are</em> right‚Äîrich feedback, high-value tasks worth the compute, domains where the LLM has strong priors‚Äîtest-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.</p>
</section>
<section id="case-study-code-optimization-for-novel-hardware" class="level3">
<h3 class="anchored" data-anchor-id="case-study-code-optimization-for-novel-hardware">Case Study: Code Optimization for Novel Hardware</h3>
<p>The GEPA paper demonstrates test-time search on domains where GEPA‚Äôs strengths shine brightest: <strong>code optimization for hardware with limited pre-training data</strong>.</p>
<section id="amd-npu-kernels-optimization-without-pre-training-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="amd-npu-kernels-optimization-without-pre-training-knowledge">AMD NPU Kernels: Optimization Without Pre-Training Knowledge</h4>
<p>AMD‚Äôs NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The <a href="https://arxiv.org/abs/2507.14403">NPUEval benchmark</a> reveals just how challenging this is: even with compiler feedback and retrieval-augmented generation (RAG), state-of-the-art LLMs achieve only ~10% mean vectorization score across the dataset.</p>
<p>GEPA succeeds here because it doesn‚Äôt need prior examples. Instead, it:</p>
<ol type="1">
<li><strong>Generates an initial kernel</strong> based on generic programming knowledge</li>
<li><strong>Compiles and runs</strong> ‚Üí receives compiler errors or performance metrics</li>
<li><strong>Reflects on feedback</strong> ‚Üí proposes targeted improvements</li>
<li><strong>Iterates</strong> until the kernel compiles, runs correctly, and performs well</li>
</ol>
<p><strong>Why GEPA fits</strong>: The GEPA paper demonstrates results on NPUEval, showing that reflective mutation can extract optimization patterns even for hardware with minimal pre-training coverage‚Äîwithout requiring retraining or RAG.</p>
<blockquote class="blockquote">
<p><em>‚ÄúGEPA can be used to generate optimized kernels for AMD‚Äôs NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>The compiler error messages contain the fix. ‚ÄúSymbol not found: <code>npu_matmul</code>‚Äù triggers reflection that surfaces the correct API. ‚ÄúMemory alignment error at line 47‚Äù points directly to what needs fixing.</p>
</section>
<section id="cuda-kernels-outperforming-human-baselines" class="level4">
<h4 class="anchored" data-anchor-id="cuda-kernels-outperforming-human-baselines">CUDA Kernels: Outperforming Human Baselines</h4>
<p><a href="https://scalingintelligence.stanford.edu/blogs/kernelbench/">KernelBench</a> (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels‚Äîa task where even frontier models struggle. The benchmark reveals a sobering baseline: <strong>frontier reasoning models match the PyTorch baseline on less than 20% of tasks</strong> using the fast‚ÇÅ metric (kernels that are both correct <em>and</em> faster than PyTorch). This isn‚Äôt a matter of prompting‚Äîefficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven‚Äôt learned to apply reliably.</p>
<p><strong>The impact of iterative feedback</strong>: The KernelBench paper demonstrates that feedback-driven refinement dramatically improves results. By providing execution results and profiler feedback in context, fast‚ÇÅ scores improved from 12%, 36%, and 12% to <strong>43%, 72%, and 18%</strong> respectively across their test configurations. This is exactly the mechanism GEPA exploits‚Äîbut with systematic Pareto tracking and reflective mutation rather than ad-hoc iteration.</p>
<blockquote class="blockquote">
<p><em>‚ÄúFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p><strong>Related evolutionary approaches show further gains</strong>: Stanford‚Äôs separate test-time evolutionary search (distinct from GEPA, published in their <a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html">Fast Kernels blog post</a>) produces kernels achieving <strong>103-133% of PyTorch reference performance</strong> on foundational operators like Conv2D‚Äîdemonstrating that structured search with execution feedback can exceed human-optimized baselines.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 44%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Performance</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base LLM (one-shot)</td>
<td>~3-15% fast‚ÇÅ</td>
<td>KernelBench paper</td>
</tr>
<tr class="even">
<td>Frontier reasoning (o1, R1)</td>
<td>Match baseline on &lt;20% of tasks</td>
<td>KernelBench paper</td>
</tr>
<tr class="odd">
<td>With execution + profiler feedback</td>
<td>43-72% fast‚ÇÅ (3-6x improvement)</td>
<td>KernelBench paper</td>
</tr>
<tr class="even">
<td>Evolutionary test-time search</td>
<td>103-133% of PyTorch on select kernels</td>
<td>Stanford CRFM (separate from GEPA)</td>
</tr>
<tr class="odd">
<td>GEPA</td>
<td>‚ÄúSignificant speedups over PyTorch-eager‚Äù</td>
<td>GEPA paper Fig. 11</td>
</tr>
</tbody>
</table>
<p>The gap matters commercially: <a href="https://scalingintelligence.stanford.edu/blogs/kernelbench/">efficient compilers often lag behind new GPU architectures by over two years</a>‚Äîapproximately one year for CUDA experts to develop optimized implementations and another year to generalize into compilers. GEPA-style optimization could bridge that gap by generating optimized kernels for new hardware before traditional toolchains catch up.</p>
<p><strong>Why code optimization is ideal for GEPA:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 41%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Why it helps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Rich textual feedback</strong></td>
<td>Compiler errors, profiler output, runtime exceptions all explain <em>why</em> something failed</td>
</tr>
<tr class="even">
<td><strong>Verifiable correctness</strong></td>
<td>Unit tests and benchmarks provide unambiguous signal</td>
</tr>
<tr class="odd">
<td><strong>Iterative refinement</strong></td>
<td>Each failed compilation reveals the next fix to try</td>
</tr>
<tr class="even">
<td><strong>Cross-task transfer</strong></td>
<td>Insights about memory coalescing on one kernel help others</td>
</tr>
</tbody>
</table>
</section>
<section id="the-self-bootstrapping-dynamic" class="level4">
<h4 class="anchored" data-anchor-id="the-self-bootstrapping-dynamic">The Self-Bootstrapping Dynamic</h4>
<p>A typical GEPA optimization trajectory for CUDA kernels follows this pattern:</p>
<p><strong>Iteration 1</strong>: Generate naive kernel ‚Üí compiler error ‚Äúundeclared identifier <code>threadIdx</code>‚Äù - Reflection: ‚ÄúCUDA kernels require explicit thread indexing. Add <code>threadIdx.x</code> and <code>blockIdx.x</code>.‚Äù</p>
<p><strong>Iteration 2</strong>: Compiles, but runtime error (out of bounds) - Reflection: ‚ÄúNeed bounds checking. Add <code>if (idx &lt; n)</code> guard.‚Äù</p>
<p><strong>Iteration 3</strong>: Runs correctly, but 10x slower than baseline - Profiler feedback: ‚ÄúMemory bandwidth: 12% of peak. Non-coalesced access pattern.‚Äù - Reflection: ‚ÄúReorganize memory access for coalescing. Use shared memory for reductions.‚Äù</p>
<p><strong>Iteration 4</strong>: 1.2x faster than PyTorch baseline</p>
<p>Each iteration surfaced a new challenge <em>and</em> its solution‚Äîgenerating increasingly sophisticated training signal from the task itself.</p>
</section>
<section id="cross-kernel-transfer" class="level4">
<h4 class="anchored" data-anchor-id="cross-kernel-transfer">Cross-Kernel Transfer</h4>
<p>When optimizing a batch of related kernels, insights compound across tasks:</p>
<blockquote class="blockquote">
<p><em>‚ÄúIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>GEPA exploits the relatedness of kernels in a batch. As it optimizes each kernel, the Pareto frontier accumulates specialized prompts‚Äîone excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. When a new kernel arrives, candidates from across the frontier are tried. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax.</p>
<p>This cross-task transfer is why batch optimization outperforms solving each kernel independently‚Äîthe accumulated optimization knowledge benefits later tasks.</p>
</section>
<section id="beyond-kernels" class="level4">
<h4 class="anchored" data-anchor-id="beyond-kernels">Beyond Kernels</h4>
<p>The same pattern extends to other code optimization domains where execution produces rich textual feedback:</p>
<blockquote class="blockquote">
<p><em>‚ÄúThere are many other tasks where it can be used‚Äîfor example, with unit tests to generate code patches.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p><strong>Bug fixing</strong> (test failures explain what‚Äôs wrong), <strong>performance optimization</strong> (profiler output identifies bottlenecks), and <strong>API migration</strong> (deprecation warnings specify changes) all fit GEPA‚Äôs feedback-driven model.</p>
</section>
</section>
</section>
<section id="conclusion-when-to-reach-for-gepa" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-when-to-reach-for-gepa">Conclusion: When to Reach for GEPA</h2>
<p>We opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculus‚Äîmatching RL‚Äôs optimization performance at a fraction of the sample cost by doing something RL can‚Äôt: <em>reading the feedback</em>.</p>
<p>The results speak for themselves: 46.6% ‚Üí 56.6% on AIME math competition problems. 67% ‚Üí 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.</p>
<p>This represents a genuinely new point in the optimization design space‚Äîone that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. That‚Äôs not incremental improvement‚Äîit‚Äôs a 100x reduction in sample requirements.</p>
<hr>
<section id="use-gepa-for-train-then-generalize-when" class="level3">
<h3 class="anchored" data-anchor-id="use-gepa-for-train-then-generalize-when">Use GEPA for Train-Then-Generalize When:</h3>
<ul>
<li>You have <strong>rich textual feedback</strong> (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)</li>
<li>Your evaluation budget is <strong>limited</strong> (50-500 examples, not 5,000)</li>
<li>You‚Äôre optimizing <strong>compound AI systems</strong> where prompts orchestrate multi-step pipelines</li>
<li>You need <strong>interpretable results</strong>‚Äîprompts you can read, edit, and reason about</li>
</ul>
</section>
<section id="use-gepa-for-test-time-search-when" class="level3">
<h3 class="anchored" data-anchor-id="use-gepa-for-test-time-search-when">Use GEPA for Test-Time Search When:</h3>
<ul>
<li>You have a <strong>batch of high-value tasks</strong> worth the compute investment</li>
<li>Each task produces <strong>execution feedback</strong> (tests, profilers, validators)</li>
<li>Tasks are <strong>related enough</strong> for cross-task transfer to help</li>
</ul>
</section>
<section id="stick-with-traditional-approaches-when" class="level3">
<h3 class="anchored" data-anchor-id="stick-with-traditional-approaches-when">Stick with Traditional Approaches When:</h3>
<ul>
<li>You have <strong>abundant labeled data</strong> and compute budget for fine-tuning</li>
<li>Feedback is <strong>purely scalar</strong> with no explanatory signal</li>
<li>The task is <strong>already solved</strong> by few-shot prompting</li>
<li>You need <strong>sub-second latency</strong></li>
</ul>
<hr>
</section>
<section id="known-limitations" class="level3">
<h3 class="anchored" data-anchor-id="known-limitations">Known Limitations</h3>
<p>GEPA isn‚Äôt a silver bullet:</p>
<ul>
<li><p><strong>Reflection quality varies by domain</strong> ‚Äî GEPA excels when the LLM has strong prior knowledge. On highly specialized domains where the base model is weak, reflection produces generic advice rather than actionable fixes.</p></li>
<li><p><strong>Feedback bottleneck</strong> ‚Äî The optimizer is only as good as its feedback. If your metric returns ‚Äúwrong‚Äù without explaining <em>why</em>, GEPA degrades to expensive random search.</p></li>
<li><p><strong>Validation set size</strong> ‚Äî With fewer than 30 validation examples, prompts can overfit to idiosyncrasies of those specific instances.</p></li>
</ul>
<hr>
</section>
<section id="get-started" class="level3">
<h3 class="anchored" data-anchor-id="get-started">Get Started</h3>
<p>Ready to try GEPA on your own pipelines?</p>
<ul>
<li><strong><a href="https://dspy.ai/tutorials/gepa_aime/">GEPA for AIME Tutorial</a></strong> ‚Äî Complete walkthrough from setup to optimized results</li>
<li><strong><a href="https://dspy.ai/api/optimizers/GEPA/overview/">GEPA API Reference</a></strong> ‚Äî Full parameter documentation</li>
<li><strong><a href="https://arxiv.org/abs/2507.19457">Paper</a></strong> ‚Äî Algorithm details and experimental methodology</li>
</ul>
<p>The core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads it‚Äîand learns.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/blog\.risheekkumar\.in");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>