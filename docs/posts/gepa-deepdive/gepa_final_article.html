<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Risheek kumar B">
<meta name="dcterms.date" content="2025-12-21">
<meta name="description" content="An indepth hands-on explanation with code about GEPA - its core and motivations">

<title>GEPA Deepdive ‚Äì Risheekkumar Baskaran</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-EMRH6XWVFQ"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-EMRH6XWVFQ', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Risheekkumar Baskaran</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/risheekkumar-baskaran-748115120/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/risheekkumarb"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/BRisheek"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">GEPA Deepdive</h1>
                  <div>
        <div class="description">
          An indepth hands-on explanation with code about GEPA - its core and motivations
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">research paper</div>
                <div class="quarto-category">analysis</div>
                <div class="quarto-category">reimplementation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Risheek kumar B </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 21, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<blockquote class="blockquote">
<p>This article was made possible by the research papers referenced throughout, the <a href="https://www.youtube.com/watch?v=fREQrxhBSk0">Weaviate discussion with Lakshya A Agrawal</a>, and <a href="https://www.answer.ai/posts/2025-10-13-video-to-doc.html">guidance</a> from Kerem Turgutlu at answer.ai. Created using the <a href="https://solveit.fast.ai">solveit</a> platform by fast.ai.</p>
</blockquote>
<p>You‚Äôve spent hours tuning your agentic pipeline. The system prompt is 500 words of carefully crafted instructions. It works‚Ä¶ 70% of the time. You have 50 labeled examples. Now what?</p>
<p>Fine-tuning requires thousands of samples. Reinforcement learning needs expensive rollout collection. Manual prompt iteration doesn‚Äôt scale. You‚Äôre stuck.</p>
<p>But what if you could match RL‚Äôs optimization performance using 50 examples instead of 5,000?</p>
<p><strong><a href="https://arxiv.org/abs/2507.19457">GEPA</a></strong> (Genetic-Pareto) achieves exactly this‚Äîby exploiting something traditional optimization ignores: the rich textual traces that LLM systems already produce. Instead of reducing a complex trajectory to a single scalar reward, GEPA lets the LLM <em>reflect on its own failures</em> and propose improvements directly.</p>
<p>In this post, we‚Äôll unpack how it works, why modern LLMs‚Äô improving self-reflection capabilities make this approach newly viable, and what it means for practitioners building compound AI systems.</p>
<section id="the-problem-scalar-rewards-and-expensive-rollouts" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-scalar-rewards-and-expensive-rollouts">The Problem: Scalar Rewards and Expensive Rollouts</h3>
<p>Traditional AI optimization techniques‚Äîreinforcement learning and fine-tuning‚Äîhave achieved remarkable results in domains with abundant data or cheap rollouts (complete execution traces from input to output). But what happens when evaluation is expensive?</p>
<p>Consider:</p>
<ul>
<li><strong>Agentic pipelines</strong> that invoke simulations, query rate-limited APIs, or run multi-step tool chains</li>
<li><strong>Code generation for novel hardware</strong>, where each evaluation requires compiling for custom silicon and executing on the device</li>
<li><strong>Complex reasoning tasks</strong> with expensive verification steps</li>
</ul>
<p>Collecting thousands of rollouts simply isn‚Äôt feasible in these settings.</p>
<p>The core issue: <strong>RL learns by comparison</strong>. A 500-step trajectory collapses to <code>reward = 0.73</code>. Did step 12 fail? Was the reasoning sound but the final answer malformed? The scalar tells you nothing. To extract signal, RL must compare many trajectories‚Äî<em>this one scored 0.8, that one scored 0.4, what differed?</em>‚Äîrequiring sample counts that expensive domains can‚Äôt support.</p>
<blockquote class="blockquote">
<p><em>RL and fine-tuning require generating large amounts of rollouts to gather scalar learning signals‚Äîsample inefficient by design.</em></p>
</blockquote>
<p>When each rollout costs minutes (or dollars), this approach breaks down.</p>
</section>
<section id="the-insight-llm-pipelines-generate-rich-textual-traces" class="level3">
<h3 class="anchored" data-anchor-id="the-insight-llm-pipelines-generate-rich-textual-traces">The Insight: LLM Pipelines Generate Rich Textual Traces</h3>
<p>Modern AI systems built around LLMs are fundamentally different from traditional ML pipelines. At every step of execution, they produce natural language artifacts that traditional optimization simply discards:</p>
<ul>
<li><strong>Reasoning traces</strong> ‚Äî Chain-of-Thought and ReAct logs expose the model‚Äôs explicit thought process. When a multi-hop QA system fails, you can see <em>where</em> the reasoning went wrong: ‚ÄúThe capital of France is Paris. Paris is in Germany‚Ä¶‚Äù The failure mode is visible in the text.</li>
<li><strong>Environment feedback</strong> ‚Äî Compiler errors don‚Äôt just say ‚Äúfailed.‚Äù They say <code>cannot find symbol 'x', did you mean 'y'?</code> API responses return structured explanations. Profilers report exactly which function consumed 80% of runtime.</li>
<li><strong>Evaluation rubrics</strong> ‚Äî LLM-as-judge systems don‚Äôt just score 3/5. They explain: ‚ÄúResponse was accurate but exceeded the 200-word limit. Missing the requested bullet-point format. Tone too formal for the specified Slack context.‚Äù</li>
</ul>
<p>Each of these is dramatically richer than <code>reward = 0.73</code>.</p>
<p><strong>The key realization</strong>: these traces aren‚Äôt just logs for debugging‚Äîthey‚Äôre potential <em>input to the optimizer</em>. A compiler error that says ‚Äúdid you mean ‚Äòy‚Äô?‚Äù contains the fix. A rubric that says ‚Äútoo verbose‚Äù specifies exactly what to change.</p>
<p>Traditional RL ignores all of this. It reduces the entire trajectory to a scalar, then tries to reconstruct what went wrong by comparing thousands of trajectories.</p>
<p>But what if we could just‚Ä¶ read the feedback?</p>
<p>This is the opportunity GEPA exploits. The question becomes: can an LLM reflect on these traces and propose improvements directly?</p>
</section>
<section id="the-opportunity-llms-can-reflect-on-their-own-failures" class="level3">
<h3 class="anchored" data-anchor-id="the-opportunity-llms-can-reflect-on-their-own-failures">The Opportunity: LLMs Can Reflect on Their Own Failures</h3>
<p>Here‚Äôs the key insight enabling a new optimization paradigm: <strong>LLMs already have prior knowledge about the domains they‚Äôre working in, and they‚Äôre increasingly capable of self-reflection.</strong></p>
<p>Consider what happens with different types of feedback:</p>
<p><strong>Compiler errors</strong> ‚Äî When the compiler returns <code>cannot find symbol 'x', did you mean 'y'?</code>, the LLM doesn‚Äôt need thousands of examples to learn the fix. It already knows the library‚Äôs API. One error message is enough.</p>
<blockquote class="blockquote">
<p><em>‚ÄúThe language model already knows that x is not a valid API name in the library but y is. Next time I should try this.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p><strong>LLM-as-judge feedback</strong> ‚Äî When a judge says ‚Äúyour summary was accurate but exceeded the 200-word limit and used overly formal tone for Slack,‚Äù the model can directly incorporate ‚Äúbe concise, match casual tone‚Äù into its next attempt. No statistical signal extraction required.</p>
<p><strong>Reasoning trace failures</strong> ‚Äî When a multi-hop QA trace shows the model correctly retrieved ‚ÄúParis is the capital of France‚Äù but then hallucinated ‚ÄúParis is in Germany,‚Äù the failure point is <em>visible in the text</em>. You can see exactly where the reasoning derailed.</p>
<p><strong>Privacy-aware rewriting (PUPA task)</strong> ‚Äî In the paper‚Äôs experiments, an LLM must rewrite prompts to remove private information while preserving response quality. The LLM-as-judge explains <em>why</em> a rewrite failed‚Äî‚Äúleaked the user‚Äôs company name‚Äù or ‚Äúremoved too much context, degrading response quality‚Äù‚Äîgiving the optimizer actionable signal from each example.</p>
<section id="the-core-asymmetry" class="level4">
<h4 class="anchored" data-anchor-id="the-core-asymmetry">The Core Asymmetry</h4>
<p>This is the fundamental difference GEPA exploits:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>How it learns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>RL</strong></td>
<td>Compare thousands of trajectories statistically: ‚ÄúThese 500 scored 0.8, those 500 scored 0.4‚Äîwhat differed?‚Äù</td>
</tr>
<tr class="even">
<td><strong>Reflection</strong></td>
<td>Read the feedback directly: ‚ÄúThe compiler said use y, so use y.‚Äù</td>
</tr>
</tbody>
</table>
<p>RL would need hundreds of rollouts to statistically isolate that <code>x‚Üíy</code> is the fix. The LLM gets it from one error message.</p>
</section>
<section id="from-fixes-to-generalizable-rules" class="level4">
<h4 class="anchored" data-anchor-id="from-fixes-to-generalizable-rules">From Fixes to Generalizable Rules</h4>
<p>Better still, modern LLMs don‚Äôt just extract point fixes‚Äîthey can derive <em>generalizable lessons</em>:</p>
<ul>
<li>Not just ‚Äúuse <code>y</code> instead of <code>x</code>‚Äù ‚Üí but ‚Äúalways verify symbol names against the library‚Äôs namespace before generating code‚Äù</li>
<li>Not just ‚Äúresponse was too long‚Äù ‚Üí but ‚Äúfor Slack contexts, limit responses to 150 words and use bullet points‚Äù</li>
<li>Not just ‚Äúleaked company name‚Äù ‚Üí but ‚Äúscan for proper nouns and replace with generic placeholders‚Äù</li>
</ul>
<blockquote class="blockquote">
<p><em>‚ÄúLLMs can reflect on their own entire trajectories and extract lessons or generalizable rules that can be incorporated into the prompt.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>These rules get folded directly into the prompt as instructions‚Äîcompounding improvements across examples rather than treating each failure in isolation.</p>
<p>This capability unlock‚ÄîLLMs that can genuinely reflect and generalize‚Äîis what makes GEPA viable now when it wouldn‚Äôt have been two years ago.</p>
</section>
</section>
<section id="why-now-the-reflection-capability-unlock" class="level3">
<h3 class="anchored" data-anchor-id="why-now-the-reflection-capability-unlock">Why Now? The Reflection Capability Unlock</h3>
<p>This approach wasn‚Äôt viable with earlier LLMs. In March 2023, roboticist <a href="https://evjang.com/2023/03/26/self-reflection.html">Eric Jang observed</a> that self-reflection capability ‚Äúseems to be emergent in GPT-4 but not GPT-3.5 or Claude.‚Äù When asked to write a non-rhyming poem, GPT-4 produced rhymes‚Äîbut when prompted ‚Äúdid the poem meet the assignment?‚Äù it apologized and corrected itself. GPT-3.5 and Claude couldn‚Äôt recognize their errors.</p>
<p>The <a href="https://arxiv.org/abs/2303.11366">Reflexion paper</a> (NeurIPS 2023) demonstrated the impact quantitatively: by maintaining verbal reflections across trials, LLMs achieved 91% on HumanEval coding benchmark versus GPT-4‚Äôs baseline 80%‚Äîwithout any weight updates. Similarly, <a href="https://arxiv.org/abs/2303.17651">Self-Refine</a> showed ~20% average improvement by having the same LLM generate, critique, and refine iteratively.</p>
<p>But there‚Äôs a crucial nuance. A <a href="https://aclanthology.org/2024.tacl-1.78/">comprehensive 2024 survey</a> found that pure ‚Äúintrinsic‚Äù self-correction‚Äîwhere the LLM reflects with no external signal‚Äîrarely helps, and can even degrade performance. What <em>does</em> work is self-correction with <strong>reliable external feedback</strong>: compiler errors, test results, structured rubrics.</p>
<p>This is precisely what GEPA exploits. The <a href="https://arxiv.org/abs/2305.11738">CRITIC paper</a> (ICLR 2024) highlights that external feedback is ‚Äúcrucial‚Äù for successful self-improvement. GEPA doesn‚Äôt ask the LLM to magically know it was wrong. It feeds the LLM rich textual feedback‚Äîthe compiler said this, the profiler showed that, the judge flagged this rubric‚Äîand asks it to reflect on <em>that</em>.</p>
<p>The capability unlock isn‚Äôt ‚ÄúLLMs can now introspect perfectly.‚Äù It‚Äôs ‚ÄúLLMs can now <em>process feedback and generalize lessons</em> effectively.‚Äù</p>
<blockquote class="blockquote">
<p><em>‚ÄúEarlier LLMs could not actually reflect that well on their trajectories and extract meaningful insights or lessons‚Ä¶ But now we are seeing that as LLMs are getting better they can also reflect on their own entire trajectories and extract lessons that can be incorporated into the prompt.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
</section>
<section id="optimizer-evolution-from-few-shot-to-reflection" class="level3">
<h3 class="anchored" data-anchor-id="optimizer-evolution-from-few-shot-to-reflection">Optimizer Evolution: From Few-Shot to Reflection</h3>
<p>To understand where GEPA fits, it helps to trace the lineage of prompt optimizers. Each generation solved a limitation of its predecessor‚Äîand GEPA represents the latest capability unlock.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/optimizer evolution.webp" class="img-fluid figure-img"></p>
<figcaption>Optimizer Evolution</figcaption>
</figure>
</div>
<section id="bootstrap-few-shot-dspy-2023" class="level4">
<h4 class="anchored" data-anchor-id="bootstrap-few-shot-dspy-2023">Bootstrap Few-Shot (DSPy, 2023)</h4>
<p>The original insight: you don‚Äôt need hand-crafted demonstrations. Given a task and metric, run the pipeline on your training examples, score the outputs, and keep the high-scoring (input, output) pairs as <a href="https://www.promptingguide.ai/techniques/fewshot">few-shot demonstrations</a> for future runs. The system bootstraps its own examples from successful executions.</p>
<p><strong>Example</strong>: Your QA system correctly answers ‚ÄúWhat‚Äôs the capital of France?‚Äù ‚Üí ‚ÄúParis‚Äù. That (question, answer) pair becomes a demonstration shown to the model on future queries.</p>
<p><strong>Limitation</strong>: Demonstrations are static snapshots. Once selected, they don‚Äôt adapt when new failure modes emerge. And there‚Äôs no instruction optimization ‚Äî the system prompt stays identical whether you‚Äôre handling edge cases or common inputs.</p>
</section>
<section id="opro-google-deepmind-2023" class="level4">
<h4 class="anchored" data-anchor-id="opro-google-deepmind-2023">OPRO (Google DeepMind, 2023)</h4>
<p><a href="https://arxiv.org/abs/2309.03409">OPRO</a> (Optimization by PROmpting, NeurIPS 2023) introduced the idea of using an LLM as the optimizer itself. The key mechanism: show the LLM a history of prompts and their scores, then ask it to propose a better one. Higher-scoring prompts appear more frequently in this history, nudging the LLM toward successful patterns.</p>
<p><strong>Example</strong>: The optimizer sees:</p>
<ul>
<li><code>"Solve the math problem step by step"</code> ‚Üí score 0.65</li>
<li><code>"Show your work and verify the answer"</code> ‚Üí score 0.72</li>
<li><code>"Break the problem into cases and check each"</code> ‚Üí score 0.78</li>
</ul>
<p>It proposes: <code>"Systematically enumerate cases and verify each solution"</code> ‚Üí score 0.81</p>
<p><strong>Limitation</strong>: Score-only signal. The optimizer sees <em>that</em> <code>prompt_v3</code> scored 0.72 but not <em>why</em>. Did it make algebraic errors? Miss edge cases? The number alone doesn‚Äôt say.</p>
</section>
<section id="mipro-2024" class="level4">
<h4 class="anchored" data-anchor-id="mipro-2024">MiPRO (2024)</h4>
<p><a href="https://arxiv.org/abs/2406.11695">MiPRO</a> recognized that instructions and demonstrations interact‚Äîthe <em>same</em> instruction performs differently with different example sets. The best instruction with bad demos might score worse than a mediocre instruction with perfect demos.</p>
<p><strong>The search space problem</strong>: Say you have 10 candidate instructions and 5 possible demo sets. That‚Äôs 50 combinations. Now add instruction variants (‚ÄúBe concise‚Äù vs ‚ÄúBe brief‚Äù vs ‚ÄúAnswer in one sentence‚Äù)‚Äîsuddenly you have hundreds of candidates. Each full evaluation means running your pipeline on your entire dev set. At $0.10 per run with 100 dev examples, evaluating all 500 combinations costs $5,000. Not feasible.</p>
<p><strong>MiPRO‚Äôs solution: a cheap surrogate model</strong>. Instead of running the full pipeline, MiPRO trains a small predictor (think: logistic regression) on the evaluations you <em>have</em> run. The predictor learns patterns like ‚Äúinstructions mentioning ‚Äòstep-by-step‚Äô tend to score higher‚Äù or ‚Äúdemos with longer reasoning traces correlate with better performance.‚Äù</p>
<p>The workflow:</p>
<ol type="1">
<li><strong>Bootstrap</strong>: Run a small random sample of combinations (say, 30 out of 500)</li>
<li><strong>Train surrogate</strong>: Fit the predictor on those 30 (instruction, demos) ‚Üí score pairs</li>
<li><strong>Predict cheaply</strong>: Score all 500 combinations using the surrogate (milliseconds, not dollars)</li>
<li><strong>Evaluate selectively</strong>: Only run full evaluation on the top-predicted candidates</li>
<li><strong>Repeat</strong>: Add new results to training data, retrain surrogate, sample again</li>
</ol>
<p><strong>Example</strong>: After 30 random evaluations, the surrogate learns:</p>
<ul>
<li>Instructions with ‚Äústep-by-step‚Äù ‚Üí +0.08 average</li>
<li>Demo set B (which has chain-of-thought examples) ‚Üí +0.05 average</li>
<li>Combining both ‚Üí predicted 0.79</li>
</ul>
<p>MiPRO focuses budget on high-predicted combinations rather than exhaustive search.</p>
<p><strong>Limitation</strong>: The surrogate learns <em>correlations</em>, not <em>causation</em>. It knows ‚Äústep-by-step instructions score higher‚Äù but not <em>why</em>‚Äîmaybe they help on math problems but hurt on simple lookups. And MiPRO optimizes for aggregate score: a prompt that‚Äôs 0.75 on everything beats one that‚Äôs 0.95 on hard cases but 0.60 overall‚Äîeven though that hard-case specialist might contain crucial insights.</p>
</section>
<section id="simba-dspy-2024" class="level4">
<h4 class="anchored" data-anchor-id="simba-dspy-2024">SIMBA (DSPy, 2024)</h4>
<p><a href="https://dspy.ai/api/optimizers/SIMBA/">SIMBA</a> (Stochastic Introspective Mini-Batch Ascent) introduced self-reflection into prompt optimization. Rather than treating prompts as black boxes with hidden payout rates, SIMBA has the LLM analyze its own performance and propose improvements.</p>
<p><strong>How it works</strong>:</p>
<ol type="1">
<li><strong>Sample mini-batches</strong>: Instead of evaluating on the full dataset, SIMBA samples small batches of examples</li>
<li><strong>Identify hard cases</strong>: Track which examples show high output variability or consistent failures</li>
<li><strong>Generate reflective rules</strong>: Ask the LLM to analyze why those cases failed and propose improvement rules</li>
<li><strong>Update prompts</strong>: Incorporate the rules as new instructions, or add successful examples as demonstrations</li>
<li><strong>Repeat</strong>: Iterate with new mini-batches, accumulating insights</li>
</ol>
<p><strong>Example</strong>: After several mini-batches, SIMBA notices examples 7 and 12 consistently fail. It prompts the LLM: ‚ÄúThese examples failed. What pattern do you see?‚Äù The LLM reflects: ‚ÄúBoth involve multi-step calculations where I lost track of units.‚Äù SIMBA adds: ‚ÄúAlways track units through each calculation step.‚Äù</p>
<p><strong>The key innovation</strong>: SIMBA bridges the gap between score-only optimization (OPRO, MiPRO) and full trajectory reflection (GEPA). It uses introspection on failure patterns rather than just comparing aggregate scores.</p>
<p><strong>Limitation</strong>: SIMBA‚Äôs reflection is still relatively shallow‚Äîit identifies patterns across examples but doesn‚Äôt deeply analyze individual execution traces. GEPA extends this by feeding the optimizer rich textual feedback (compiler errors, rubrics, reasoning traces) from each trajectory.</p>
</section>
<section id="gepa-2025-the-reflection-shift" class="level4">
<h4 class="anchored" data-anchor-id="gepa-2025-the-reflection-shift">GEPA (2025): The Reflection Shift</h4>
<p>GEPA breaks from this trajectory in two fundamental ways:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 34%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>What changed</th>
<th>Before GEPA</th>
<th>With GEPA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Learning signal</strong></td>
<td><code>score = 0.6</code></td>
<td>‚ÄúExceeded word limit. Missing keyword. Compiler error: use y not x.‚Äù</td>
</tr>
<tr class="even">
<td><strong>Selection strategy</strong></td>
<td>Best aggregate score</td>
<td>Pareto frontier of diverse specialists</td>
</tr>
</tbody>
</table>
<p><strong>1. From scalar scores to textual feedback</strong> ‚Äî Instead of just knowing <em>that</em> a prompt scored 0.6, GEPA sees <em>why</em>: the compiler error, the rubric failures, the reasoning trace where hallucination occurred. The optimizer reads the feedback directly.</p>
<p><strong>Example</strong>: OPRO sees <code>score = 0.6</code>. GEPA sees: &gt; ‚ÄúFailed on example 7: response was 340 words (limit: 200). Failed on example 12: missing required keyword ‚Äòdisclaimer‚Äô. Passed examples 1-6, 8-11.‚Äù</p>
<p>The LLM reflects: ‚ÄúI should add an instruction about word limits and required keywords.‚Äù</p>
<p><strong>2. From greedy to Pareto selection</strong> ‚Äî Instead of always promoting the highest-scoring candidate, GEPA maintains a <em><a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto frontier</a></em>: candidates that each excel at <em>something</em> no other candidate beats.</p>
<p><strong>Example</strong>: Three candidates evaluated on 10 examples:</p>
<ul>
<li><code>prompt_A</code>: 8/10 overall, but fails hard cases #7 and #9</li>
<li><code>prompt_B</code>: 6/10 overall, but nails hard cases #7 and #9<br>
</li>
<li><code>prompt_C</code>: 7/10 overall, no unique strengths</li>
</ul>
<p>Greedy selection keeps only <code>prompt_A</code>. Pareto selection keeps both <code>prompt_A</code> <em>and</em> <code>prompt_B</code>‚Äîbecause B‚Äôs insights about hard cases might combine with A‚Äôs general strength. <code>prompt_C</code> gets dropped (dominated by A on everything).</p>
<p>The contrast with prior optimizers is stark: OPRO knows the score dropped from 0.8 to 0.6. GEPA reads the compiler error that explains why‚Äîand proposes the fix directly.</p>
</section>
</section>
<section id="how-gepa-works-building-it-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-gepa-works-building-it-from-scratch">How GEPA Works: Building It From Scratch</h2>
<p>GEPA combines two key innovations: <strong>reflective prompt mutation</strong> (learning from textual feedback) and <strong>Pareto selection</strong> (preserving diverse specialists). Each is powerful alone; together they compound.</p>
<p>We‚Äôll build them from scratch in this section:</p>
<ol type="1">
<li><p><strong>Reflective mutation</strong> ‚Äî How GEPA extracts generalizable lessons from rollout traces and feedback, proposing improved prompts directly. This is where the sample efficiency comes from.</p></li>
<li><p><strong>Pareto selection</strong> ‚Äî Why always improving your ‚Äúbest‚Äù prompt gets stuck, and how tracking per-instance performance preserves insights that would otherwise be lost.</p></li>
<li><p><strong>Merge</strong> ‚Äî How GEPA combines insights from divergent lineages (covered after the complete algorithm).</p></li>
</ol>
<p>By the end, you‚Äôll see how these mechanisms combine into GEPA‚Äôs full evolutionary loop.</p>
<hr>
<section id="quick-start-using-gepa-in-30-seconds" class="level3">
<h3 class="anchored" data-anchor-id="quick-start-using-gepa-in-30-seconds">Quick Start: Using GEPA in 30 Seconds</h3>
<blockquote class="blockquote">
<p>üìñ <strong>Full tutorial</strong>: <a href="https://dspy.ai/tutorials/gepa_aime/">GEPA for AIME (Math)</a> ‚Äî optimizing GPT-4.1 Mini from 46.6% ‚Üí 56.6% on AIME 2025.</p>
</blockquote>
<p>Before diving deep, here‚Äôs what using GEPA looks like in practice:</p>
<p><strong>Step 1: Configure your language model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dspy</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> dspy.LM(<span class="st">"openai/gpt-4.1-mini"</span>, temperature<span class="op">=</span><span class="dv">1</span>, max_tokens<span class="op">=</span><span class="dv">32000</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>dspy.configure(lm<span class="op">=</span>lm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 2: Define your program</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>program <span class="op">=</span> dspy.ChainOfThought(<span class="st">"problem -&gt; answer"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 3: Define a metric that returns feedback (not just a score)</strong></p>
<p>This is the key difference from other optimizers‚Äîyour metric explains <em>why</em> something failed:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metric_with_feedback(example, prediction, trace<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    correct_answer <span class="op">=</span> example.answer</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    pred_answer <span class="op">=</span> prediction.answer</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> <span class="bu">int</span>(correct_answer <span class="op">==</span> pred_answer)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> score <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="ss">f"Correct! The answer is '</span><span class="sc">{</span>correct_answer<span class="sc">}</span><span class="ss">'."</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="ss">f"Incorrect. Expected '</span><span class="sc">{</span>correct_answer<span class="sc">}</span><span class="ss">', got '</span><span class="sc">{</span>pred_answer<span class="sc">}</span><span class="ss">'."</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add any additional context that could help improvement:</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(example, <span class="st">'solution'</span>):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            feedback <span class="op">+=</span> <span class="ss">f" Solution: </span><span class="sc">{</span>example<span class="sc">.</span>solution<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dspy.Prediction(score<span class="op">=</span>score, feedback<span class="op">=</span>feedback)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 4: Optimize with GEPA</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dspy <span class="im">import</span> GEPA</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> GEPA(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    metric<span class="op">=</span>metric_with_feedback,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    auto<span class="op">=</span><span class="st">"light"</span>,           <span class="co"># Budget preset: "light", "medium", or "heavy"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    num_threads<span class="op">=</span><span class="dv">32</span>,         <span class="co"># Parallel evaluation threads</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>optimized_program <span class="op">=</span> optimizer.<span class="bu">compile</span>(</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    program,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    trainset<span class="op">=</span>train_set,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    valset<span class="op">=</span>val_set,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>That‚Äôs it. GEPA handles the evolutionary loop, Pareto selection, and reflective mutation internally.</p>
<hr>
<p><strong>What‚Äôs happening under the hood?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 56%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Textual feedback</strong></td>
<td>Your metric returns <em>why</em> something failed, not just a score</td>
</tr>
<tr class="even">
<td><strong>Reflective mutation</strong></td>
<td>An LLM reads the feedback and proposes improved instructions</td>
</tr>
<tr class="odd">
<td><strong>Pareto selection</strong></td>
<td>Diverse specialists are preserved, not just the ‚Äúbest‚Äù prompt</td>
</tr>
<tr class="even">
<td><strong>Merge operations</strong></td>
<td>Insights from divergent lineages get combined</td>
</tr>
</tbody>
</table>
<p>The rest of this section explains <em>why</em> each of these pieces matters and how they work together. If you just want to use GEPA, the code above is all you need‚Äîsee the <a href="https://dspy.ai/api/optimizers/GEPA/overview/">DSPy GEPA API Reference</a> for full parameter details.</p>
<p>Now let‚Äôs make this concrete by building the core mechanism from scratch.</p>
</section>
<section id="gepa-reflective-prompt-evolution-flow-diagram-from-paper" class="level3">
<h3 class="anchored" data-anchor-id="gepa-reflective-prompt-evolution-flow-diagram-from-paper">GEPA: REFLECTIVE PROMPT EVOLUTION (Flow diagram from paper)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/GEPA_flowchart.png" class="img-fluid figure-img"></p>
<figcaption>GEPA Flowchart</figcaption>
</figure>
</div>
<p>The diagram above shows how these pieces fit together. Starting from the candidate pool (left), GEPA evaluates each prompt on every training task to build a per-instance scores matrix. Pareto filtering then preserves prompts that excel at <em>something</em>‚Äînot just the highest aggregate scorer. New candidates emerge either through <strong>reflective mutation</strong> (learning from textual feedback on a minibatch) or <strong>merge</strong> (combining insights from two specialists). Only candidates that pass a cheap minibatch screening get full evaluation.</p>
<p>Let‚Äôs now build the core mechanism‚Äîreflective mutation‚Äîfrom scratch to see exactly how the ‚ÄúReflect and Propose New Prompt‚Äù step works.</p>
</section>
<section id="hands-on-building-reflective-mutation-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-building-reflective-mutation-from-scratch">Hands-On: Building Reflective Mutation from Scratch</h3>
<p>Let‚Äôs make this concrete by implementing GEPA‚Äôs core mechanism on a real task.</p>
<p><strong>The Problem: AIME Math Competition</strong></p>
<p>We‚Äôll optimize prompts for solving <a href="https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination">AIME</a> (American Invitational Mathematics Examination) problems ‚Äî challenging competition math that tests algebra, number theory, geometry, and combinatorics. These problems are hard: even frontier LLMs struggle without careful prompting.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dset <span class="op">=</span> load_dataset(<span class="st">"AI-MO/aimo-validation-aime"</span>)[<span class="st">'train'</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 90 problems with solutions and integer answers</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 37%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>problem</th>
<th>solution</th>
<th>answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Quadratic polynomials <span class="math inline">\(P(x)\)</span> and <span class="math inline">\(Q(x)\)</span> have l‚Ä¶</td>
<td>Let <span class="math inline">\(R(x)=P(x)+Q(x).\)</span> Since the <span class="math inline">\(x^2\)</span>-terms of‚Ä¶</td>
<td>116</td>
</tr>
<tr class="even">
<td>Three spheres with radii <span class="math inline">\(11\)</span>, <span class="math inline">\(13\)</span>, and <span class="math inline">\(19\)</span> ‚Ä¶</td>
<td>This solution refers to the Diagram section‚Ä¶</td>
<td>756</td>
</tr>
</tbody>
</table>
<p><strong>Why AIME for testing prompt optimization?</strong></p>
<ol type="1">
<li><strong>Clear ground truth</strong> ‚Äî Every answer is an integer (0-999), so evaluation is unambiguous</li>
<li><strong>Rich failure modes</strong> ‚Äî Wrong answers come from algebraic errors, missed cases, misread constraints</li>
<li><strong>Domain knowledge helps</strong> ‚Äî Prompts that encode strategies (‚Äúsubtract equations pairwise‚Äù, ‚Äúenumerate all cases‚Äù) measurably improve performance</li>
<li><strong>Small dataset</strong> ‚Äî Only 90 problems, so sample efficiency matters</li>
</ol>
<p><strong>The Setup</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split: 10 train, 10 validation (simulating scarce labeled data)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>tdf <span class="op">=</span> df.sample(<span class="dv">45</span>).iloc[:<span class="dv">10</span>]  <span class="co"># Training mini-batches drawn from here</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>vdf <span class="op">=</span> df.drop(tdf.index).iloc[:<span class="dv">10</span>]  <span class="co"># Held-out validation</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Base model: Gemini 2.5 Flash via LiteLLM</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Metric: Exact match (predicted integer == ground truth)</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metric(ground_truth, prediction):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(ground_truth) <span class="op">==</span> prediction[<span class="st">'answer'</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Seed Prompt</strong></p>
<p>We start with a minimal instruction:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>seed_prompt <span class="op">=</span> <span class="st">"""You are given a problem and you have to give the answer </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="st">along with reasoning. Do not return anything apart from json. </span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="st">It should be parsable by json.loads()"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Baseline validation accuracy: <strong>10%</strong> (1/10 correct)</p>
<p>Can reflective mutation improve this? Let‚Äôs find out.</p>
<hr>
</section>
<section id="step-1-the-feedback-function" class="level3">
<h3 class="anchored" data-anchor-id="step-1-the-feedback-function">Step 1: The Feedback Function</h3>
<p>First, we need a function that tells the reflection LLM what went wrong. We‚Äôll start with <strong>minimal feedback</strong>‚Äîjust the correct answer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback(ground_truth, prediction):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">int</span>(ground_truth) <span class="op">!=</span> prediction[<span class="st">'answer'</span>]:</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f'You got it wrong! The solution is </span><span class="sc">{</span>ground_truth<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'You got it right!'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is deliberately simple. Later we‚Äôll discuss how richer feedback (like expert solutions) can improve results.</p>
<hr>
</section>
<section id="step-2-the-reflection-prompt" class="level3">
<h3 class="anchored" data-anchor-id="step-2-the-reflection-prompt">Step 2: The Reflection Prompt</h3>
<p>Following GEPA‚Äôs structure, we build a prompt that shows the LLM its failures:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>REFLECTION_TEMPLATE <span class="op">=</span> <span class="st">"""I provided an assistant with the following instructions:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;curr_instructions&gt;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="sc">{current_prompt}</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="st">The following are examples with assistant's responses and feedback:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;inputs_outputs_feedback&gt;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="sc">{examples}</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="st">Your task: write a new instruction for the assistant.</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="st">- Read inputs carefully and identify the input format and task description</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="st">- Read all responses and feedback. Identify niche/domain-specific factual information</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="st">- If the assistant used a generalizable strategy, include that in the instruction</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="st">Provide the new instructions.</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mk_reflection_prompt(df, curr_prompt):</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Build reflection prompt from minibatch results."""</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    examples <span class="op">=</span> []</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, row <span class="kw">in</span> df.reset_index().iterrows():</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        example <span class="op">=</span> <span class="ss">f"""# Example </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="ss">## problem</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>row[<span class="st">'problem'</span>]<span class="sc">}</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="ss">## prediction</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>row[<span class="st">'pred'</span>]<span class="sc">}</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="ss">## feedback</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>feedback(row.answer, row.pred)<span class="sc">}</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        examples.append(example)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> REFLECTION_TEMPLATE.<span class="bu">format</span>(</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        current_prompt<span class="op">=</span>curr_prompt,</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        examples<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(examples)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Example filled-in reflection prompt:</strong></p>
<pre><code>I provided an assistant with the following instructions:
&lt;curr_instructions&gt;
You are given a problem and you have to give the answer along with reasoning. 
Do not return anything apart from json. It should be parsable by json.loads()

The following are examples with assistant's responses and feedback:
&lt;inputs_outputs_feedback&gt;
# Example 1
## problem
Quadratic polynomials P(x) and Q(x) have leading coefficient 1. The sum of the roots of P(x) is 7...
## prediction
{"answer": 42, "reasoning": "I solved the system and got x=7, y=6"}
## feedback
You got it wrong! The solution is 116

# Example 2
## problem
Three spheres with radii 11, 13, and 19 are mutually externally tangent...
## prediction
{"answer": 756, "reasoning": "Using the tangent sphere formula..."}
## feedback
You got it right!

# Example 3
## problem
Find the remainder when 2^2024 is divided by 1000...
## prediction
{"answer": 16, "reasoning": "I computed powers of 2 mod 1000..."}
## feedback
You got it wrong! The solution is 896

Your task: write a new instruction for the assistant.

- Read inputs carefully and identify the input format and task description
- Read all responses and feedback. Identify niche/domain-specific factual information
- If the assistant used a generalizable strategy, include that in the instruction

Provide the new instructions.</code></pre>
<hr>
</section>
<section id="step-3-the-complete-optimization-loop" class="level3">
<h3 class="anchored" data-anchor-id="step-3-the-complete-optimization-loop">Step 3: The Complete Optimization Loop</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mk_reflection_prompt(mb, curr_prompt):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Build reflection prompt from minibatch results."""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    examples <span class="op">=</span> []</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, row <span class="kw">in</span> df.reset_index().iterrows():</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        example <span class="op">=</span> <span class="ss">f"""# Example </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="ss">## problem</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>row[<span class="st">'problem'</span>]<span class="sc">}</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="ss">## prediction</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>row[<span class="st">'pred'</span>]<span class="sc">}</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="ss">## feedback</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>feedback(row.answer, row.pred)<span class="sc">}</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        examples.append(example)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> REFLECTION_TEMPLATE.<span class="bu">format</span>(</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        current_prompt<span class="op">=</span>curr_prompt,</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        examples<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(examples)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reflect(mb, curr_prompt):</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Ask LLM to reflect on failures and propose improved instruction."""</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    refl_prompt <span class="op">=</span> mk_reflection_prompt(mb, curr_prompt)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _call(refl_prompt, <span class="bu">format</span><span class="op">=</span>ReflectionModel)[<span class="st">'new_instruction'</span>]</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize_prompt(seed_prompt, traindf, valdf, n_iters<span class="op">=</span><span class="dv">3</span>, mb_size<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Greedy reflective prompt optimization."""</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    prompts, train_scores, val_scores <span class="op">=</span> [seed_prompt], [], []</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    mb <span class="op">=</span> traindf.sample(mb_size)  <span class="co"># Fixed minibatch for this run</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Baseline validation: </span><span class="sc">{</span>eval_val(seed_prompt, valdf)<span class="sc">:.2%}</span><span class="ss">'</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters):</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate current prompt on minibatch</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        mb_eval, mb_score <span class="op">=</span> eval_mb(prompts[<span class="op">-</span><span class="dv">1</span>], mb)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"üìä Minibatch: </span><span class="sc">{</span>mb_score<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reflect and propose new instruction</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        new_instr <span class="op">=</span> reflect(mb_eval, prompts[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        new_prompt <span class="op">=</span> new_instr  <span class="co"># The new instruction becomes the new prompt</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate on validation set</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        val_score <span class="op">=</span> eval_val(new_prompt, valdf)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"üìä Validation: </span><span class="sc">{</span>val_score<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>        prompts.append(new_prompt)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>        val_scores.append(val_score)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(prompts<span class="op">=</span>prompts, val_scores<span class="op">=</span>val_scores)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="what-actually-happened" class="level3">
<h3 class="anchored" data-anchor-id="what-actually-happened">What Actually Happened</h3>
<p>Running this on AIME problems with Gemini 2.5 Flash:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 19%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Minibatch</th>
<th>Validation</th>
<th>What the reflection learned</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td>‚Äî</td>
<td>10%</td>
<td>‚Äî</td>
</tr>
<tr class="even">
<td>1</td>
<td>0%</td>
<td>10%</td>
<td>JSON formatting details, output structure rules</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0%</td>
<td><strong>30%</strong></td>
<td>Systems of equations strategy, remainder/modular arithmetic tips</td>
</tr>
<tr class="even">
<td>3</td>
<td>67%</td>
<td><strong>10%</strong></td>
<td>Over-specialized on number theory, solved #9 but lost generality</td>
</tr>
</tbody>
</table>
<p><strong>The good</strong>: Iteration 2 extracted genuinely useful domain knowledge. Despite 0% minibatch accuracy, the reflection LLM identified patterns from the problems themselves and added this to the prompt:</p>
<blockquote class="blockquote">
<p><em>‚ÄúWhen dealing with systems of equations like <span class="math inline">\(xy + Az = C\)</span>, <span class="math inline">\(yz + Ax = C\)</span>, <span class="math inline">\(zx + Ay = C\)</span>, consider subtracting equations pairwise to find relationships between variables, such as <span class="math inline">\((x-z)(y-A)=0\)</span>, which implies <span class="math inline">\(x=z\)</span> or <span class="math inline">\(y=A\)</span>. Systematically explore all such cases.‚Äù</em></p>
</blockquote>
<p>This is directly from the actual output‚Äîthe reflection LLM read the failed attempt on a systems-of-equations problem and generalized a useful heuristic.</p>
<p><strong>The bad</strong>: Iteration 3 achieved 67% on its minibatch but dropped to <strong>10% validation</strong>. Why? The minibatch happened to contain number theory problems, so the reflection added specialized number-theory guidance:</p>
<blockquote class="blockquote">
<p><em>‚ÄúWhen the problem involves number theory and remainders (e.g., <span class="math inline">\(n \pmod x\)</span>, <span class="math inline">\(n \pmod y\)</span>), pay close attention to inconsistencies or contradictions that might arise from given conditions, especially when distinct remainders are required, as these can lead to an answer of 0.‚Äù</em></p>
</blockquote>
<p>This over-specialized advice (‚Äúcan lead to an answer of 0‚Äù) actively hurt performance on non-number-theory problems, dropping validation from 30% back to <strong>10%</strong> (1/10)‚Äîthough notably, it did solve problem #9, which earlier prompts couldn‚Äôt.</p>
<hr>
</section>
<section id="the-greedy-selection-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-greedy-selection-problem">The Greedy Selection Problem</h3>
<p>This demonstrates exactly why GEPA uses <strong>Pareto selection</strong> instead of always taking the ‚Äúbest‚Äù prompt:</p>
<ol type="1">
<li><strong>Iteration 2‚Äôs prompt</strong> was a specialist‚Äîit learned something valuable about systems of equations</li>
<li><strong>Iteration 3</strong> tried to improve on iteration 2, but the minibatch had different problems</li>
<li>The reflection <strong>overwrote</strong> the systems-of-equations insight while adding number-theory tips that were <em>too specific</em></li>
<li>Result: <strong>catastrophic forgetting</strong></li>
</ol>
<p>With greedy selection, we would have discarded iteration 2‚Äôs valuable insight. Pareto selection would keep it‚Äîbecause it was <em>best on at least one validation instance</em>.</p>
<hr>
</section>
<section id="the-missing-ingredient-rich-feedback" class="level3">
<h3 class="anchored" data-anchor-id="the-missing-ingredient-rich-feedback">The Missing Ingredient: Rich Feedback</h3>
<p>Our minimal feedback (<code>"You got it wrong! The solution is 349"</code>) only tells the model <em>that</em> it failed, not <em>why</em> or <em>how to fix it</em>.</p>
<p>The AIME dataset includes expert solutions. A richer feedback function could use them:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback_rich(row):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">int</span>(row.answer) <span class="op">!=</span> row.pred[<span class="st">'answer'</span>]:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        sol <span class="op">=</span> row.solution[:<span class="dv">500</span>] <span class="op">+</span> <span class="st">"..."</span> <span class="cf">if</span> <span class="bu">len</span>(row.solution) <span class="op">&gt;</span> <span class="dv">500</span> <span class="cf">else</span> row.solution</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"""Wrong! Expected </span><span class="sc">{</span>row<span class="sc">.</span>answer<span class="sc">}</span><span class="ss">, got </span><span class="sc">{</span>row<span class="sc">.</span>pred[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">.</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="ss">        </span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="ss">Model's reasoning: </span><span class="sc">{</span>row<span class="sc">.</span>pred[<span class="st">'short_reasoning'</span>]<span class="sc">}</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="ss">Expert solution approach:</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>sol<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"Correct!"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Example output</strong> for a wrong answer:</p>
<pre><code>Wrong! Expected 116, got 42.

Model's reasoning: I set up the system of equations and solved for x=7, y=6, giving 7*6=42.

Expert solution approach:
Let R(x)=P(x)+Q(x). Since the x¬≤-terms of P and Q have leading coefficient 1, 
R(x) is quadratic with leading coefficient 2. Given the roots condition, we can 
write R(x) = 2(x-r‚ÇÅ)(x-r‚ÇÇ). Expanding and comparing coefficients...</code></pre>
<p>With rich feedback, the reflection LLM can extract <em>specific strategies</em> from the expert solution rather than having to guess what went wrong. This is the key insight from the GEPA paper: <strong>the feedback contains the fix</strong>.</p>
<p>Compare this to RL, which would only see <code>reward = 0</code> and have to statistically infer what went wrong across thousands of trajectories.</p>
<hr>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li><strong>Reflective mutation works</strong> ‚Äî Even with minimal feedback, the LLM extracted useful domain knowledge</li>
<li><strong>Greedy selection fails</strong> ‚Äî Iteration 3‚Äôs collapse shows why we need to preserve specialist insights</li>
<li><strong>Feedback quality matters</strong> ‚Äî Rich feedback (expert solutions, compiler errors, rubric explanations) gives the reflection LLM more to work with</li>
<li><strong>Sample efficiency is real</strong> ‚Äî We saw meaningful optimization with just 3 iterations on 3 examples each</li>
</ol>
<p>This is the core limitation of greedy optimization: <strong>catastrophic forgetting</strong>. The solution? Pareto selection‚Äîwhich we‚Äôll build from scratch next.</p>
</section>
<section id="hands-on-building-the-pareto-frontier" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-building-the-pareto-frontier">Hands-On: Building the Pareto Frontier</h3>
<p>In the reflective mutation section, we saw greedy selection fail‚Äîiteration 3‚Äôs over-specialized prompt dropped validation from 30% to 10%, losing iteration 2‚Äôs valuable systems-of-equations insights. The fundamental problem: always improving your ‚Äúbest‚Äù prompt discards specialist knowledge.</p>
<p>GEPA‚Äôs solution: <strong>Pareto selection</strong>. Instead of keeping one best prompt, maintain a <em>frontier</em> of prompts where each excels at something no other prompt beats.</p>
<hr>
<section id="what-is-pareto-dominance" class="level4">
<h4 class="anchored" data-anchor-id="what-is-pareto-dominance">What is Pareto Dominance?</h4>
<p>A prompt <strong>dominates</strong> another if it‚Äôs at least as good everywhere, and strictly better somewhere:</p>
<ul>
<li><strong>‚â•</strong> on <em>every</em> validation instance, AND<br>
</li>
<li><strong>&gt;</strong> on <em>at least one</em> instance</li>
</ul>
<p>If prompt A dominates prompt B, we can safely discard B‚ÄîA is strictly better in every way that matters. But if neither dominates the other (each wins on different instances), both belong on the frontier.</p>
<p><strong>Example</strong>: Consider four prompts evaluated on 10 validation instances. We‚Äôll use labels P0-P3, which map to our earlier experiment: P0 = Seed, P1 = Iteration 1, P2 = Iteration 2, P3 = Iteration 3:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Instances Solved</th>
<th>Aggregate</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P0 (seed)</td>
<td>#0 only</td>
<td>10%</td>
<td>Dominated by P2</td>
</tr>
<tr class="even">
<td>P1 (iter 1)</td>
<td>#0 only</td>
<td>10%</td>
<td>Dominated by P2</td>
</tr>
<tr class="odd">
<td>P2 (iter 2)</td>
<td>#0, #1, #2</td>
<td>30%</td>
<td><strong>Frontier</strong> ‚úì</td>
</tr>
<tr class="even">
<td>P3 (iter 3)</td>
<td>#9 only</td>
<td>10%</td>
<td><strong>Frontier</strong> ‚úì</td>
</tr>
</tbody>
</table>
<p>P2 dominates both P0 and P1‚Äîit solves everything they solve, plus more. But P3 survives despite its low aggregate score! It solved instance #9, which <em>nothing else could</em>.</p>
<p>The Pareto frontier is <strong>{P2, P3}</strong>. Both contain unique value.</p>
<hr>
</section>
<section id="implementation-dominance-checking" class="level4">
<h4 class="anchored" data-anchor-id="implementation-dominance-checking">Implementation: Dominance Checking</h4>
<p>We represent per-instance scores as boolean arrays (1 = solved, 0 = failed):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dominates(candidate_scores, other_scores):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Does candidate dominate other? (&gt;= everywhere, &gt; somewhere)"""</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    candidate <span class="op">=</span> np.array(candidate_scores)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    other <span class="op">=</span> np.array(other_scores)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (candidate <span class="op">&gt;=</span> other).<span class="bu">all</span>() <span class="kw">and</span> (candidate <span class="op">&gt;</span> other).<span class="bu">any</span>()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_dominated_by_any(new_scores, frontier_scores):</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Is new_scores dominated by ANY prompt in the frontier?"""</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    new <span class="op">=</span> np.array(new_scores)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> existing <span class="kw">in</span> frontier_scores:</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dominates(np.array(existing), new):</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dominated_indices(new_scores, frontier_scores):</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Which frontier prompts does new_scores dominate?"""</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    new <span class="op">=</span> np.array(new_scores)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [i <span class="cf">for</span> i, existing <span class="kw">in</span> <span class="bu">enumerate</span>(frontier_scores) </span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> dominates(new, np.array(existing))]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="tracing-through-why-p3-survives" class="level4">
<h4 class="anchored" data-anchor-id="tracing-through-why-p3-survives">Tracing Through: Why P3 Survives</h4>
<p>Let‚Äôs verify the dominance relationships from our example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>P0 <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]  <span class="co"># Solves: #0</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>P1 <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]  <span class="co"># Solves: #0</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>P2 <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]  <span class="co"># Solves: #0, #1, #2</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>P3 <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>]  <span class="co"># Solves: #9</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P2 dominate P0?</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>dominates(P2, P0)  <span class="co"># True: P2 &gt;= P0 everywhere, P2 &gt; P0 on #1, #2</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P2 dominate P1?</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>dominates(P2, P1)  <span class="co"># True: P2 &gt;= P1 everywhere, P2 &gt; P1 on #1, #2</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P2 dominate P3?</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>dominates(P2, P3)  <span class="co"># False! P2 loses on #9 (0 &lt; 1)</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Does P3 dominate P2?</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>dominates(P3, P2)  <span class="co"># False! P3 loses on #0, #1, #2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Neither P2 nor P3 dominates the other‚Äîthey‚Äôre <strong>Pareto incomparable</strong>. Each solves problems the other can‚Äôt. Both stay on the frontier.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/pareto_visual.webp" class="img-fluid figure-img"></p>
<figcaption>Pareto Frontier Explained</figcaption>
</figure>
</div>
<hr>
</section>
<section id="the-complete-frontier-manager" class="level4">
<h4 class="anchored" data-anchor-id="the-complete-frontier-manager">The Complete Frontier Manager</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ParetoFrontier:</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prompts <span class="op">=</span> []</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scores <span class="op">=</span> []  <span class="co"># scores[i][j] = prompt i's score on instance j</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add(<span class="va">self</span>, prompt, instance_scores):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Try to add a prompt. Returns True if it joins the frontier."""</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reject if dominated by existing frontier member</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_dominated_by_any(instance_scores, <span class="va">self</span>.scores):</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove any frontier members this prompt dominates</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        dominated <span class="op">=</span> get_dominated_indices(instance_scores, <span class="va">self</span>.scores)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">sorted</span>(dominated, reverse<span class="op">=</span><span class="va">True</span>):  <span class="co"># Remove from end first</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> <span class="va">self</span>.prompts[i]</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> <span class="va">self</span>.scores[i]</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add to frontier</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prompts.append(prompt)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scores.append(instance_scores)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>):</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample a prompt, weighted by unique wins."""</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> []</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        scores_arr <span class="op">=</span> np.array(<span class="va">self</span>.scores)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.prompts)):</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># How many instances is this prompt *uniquely* best on?</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>            others_best <span class="op">=</span> np.delete(scores_arr, i, axis<span class="op">=</span><span class="dv">0</span>).<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.prompts) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> np.zeros_like(scores_arr[i])</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>            unique_wins <span class="op">=</span> (scores_arr[i] <span class="op">&gt;</span> others_best).<span class="bu">sum</span>()</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>            weights.append(unique_wins <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># +1 smoothing</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(<span class="va">self</span>.prompts, p<span class="op">=</span>np.array(weights)<span class="op">/</span><span class="bu">sum</span>(weights))</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> best_aggregate(<span class="va">self</span>):</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the prompt with highest aggregate score."""</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        aggregates <span class="op">=</span> [<span class="bu">sum</span>(s) <span class="cf">for</span> s <span class="kw">in</span> <span class="va">self</span>.scores]</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.prompts[np.argmax(aggregates)]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="putting-it-together-pareto-guided-optimization" class="level4">
<h4 class="anchored" data-anchor-id="putting-it-together-pareto-guided-optimization">Putting It Together: Pareto-Guided Optimization</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize_with_pareto(seed_prompt, traindf, valdf, n_iters<span class="op">=</span><span class="dv">5</span>, mb_size<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Reflective mutation with Pareto frontier selection."""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    frontier <span class="op">=</span> ParetoFrontier()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize with seed</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    seed_scores <span class="op">=</span> evaluate_per_instance(seed_prompt, valdf)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    frontier.add(seed_prompt, seed_scores)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Baseline: </span><span class="sc">{</span><span class="bu">sum</span>(seed_scores)<span class="op">/</span><span class="bu">len</span>(seed_scores)<span class="sc">:.1%}</span><span class="ss">'</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters):</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">40</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">Iteration </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">40</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample parent from frontier (weighted by unique wins)</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        parent <span class="op">=</span> frontier.sample()</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run on minibatch, reflect, propose mutation</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        mb <span class="op">=</span> traindf.sample(mb_size)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        mb_results <span class="op">=</span> evaluate_with_traces(parent, mb)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        new_prompt <span class="op">=</span> reflect_and_mutate(parent, mb_results)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate on full validation set</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        new_scores <span class="op">=</span> evaluate_per_instance(new_prompt, valdf)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        new_agg <span class="op">=</span> <span class="bu">sum</span>(new_scores) <span class="op">/</span> <span class="bu">len</span>(new_scores)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"New prompt: </span><span class="sc">{</span>new_agg<span class="sc">:.1%}</span><span class="ss"> aggregate"</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Try to add to frontier</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> frontier.add(new_prompt, new_scores):</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"‚úì Added to frontier (size: </span><span class="sc">{</span><span class="bu">len</span>(frontier.prompts)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"‚úó Dominated, rejected"</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frontier.best_aggregate()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="what-we-observed-on-aime" class="level4">
<h4 class="anchored" data-anchor-id="what-we-observed-on-aime">What We Observed on AIME</h4>
<p>Running this on AIME problems with Gemini 2.5 Flash:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Iteration</th>
<th>Aggregate</th>
<th>Instances Solved</th>
<th>Frontier Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Seed</td>
<td>10%</td>
<td>#0</td>
<td>Initialize</td>
</tr>
<tr class="even">
<td>1</td>
<td>10%</td>
<td>#0</td>
<td>Dominated by seed, rejected</td>
</tr>
<tr class="odd">
<td>2</td>
<td>30%</td>
<td>#0, #1, #2</td>
<td>Added, dominates seed</td>
</tr>
<tr class="even">
<td>3</td>
<td>10%</td>
<td>#9 only</td>
<td><strong>Added</strong> ‚úì (unique win on #9)</td>
</tr>
</tbody>
</table>
<p><strong>The key moment</strong>: Iteration 3 scored only 10%‚Äîworse than iteration 2‚Äôs 30%. Greedy selection would discard it entirely.</p>
<p>But it solved <strong>instance #9</strong>, which nothing else could. Pareto selection preserves it.</p>
<p>Our final frontier: <strong>{P2, P3}</strong></p>
<ul>
<li>P2: Strong generalist (30%), knows systems-of-equations strategies</li>
<li>P3: Instance-9 specialist (10%), knows whatever cracked that specific problem</li>
</ul>
<p>Both insights survive. The merge operation (covered later) can combine them.</p>
<hr>
</section>
<section id="why-this-matters-no-more-catastrophic-forgetting" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-no-more-catastrophic-forgetting">Why This Matters: No More Catastrophic Forgetting</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Selection Strategy</th>
<th>What happens to specialists</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Greedy</strong></td>
<td>Discarded whenever aggregate score drops</td>
</tr>
<tr class="even">
<td><strong>Pareto</strong></td>
<td>Preserved if they solve <em>anything</em> unique</td>
</tr>
</tbody>
</table>
<p>Greedy selection caused our iteration 3 collapse‚Äîthe number-theory prompt overwrote the algebra prompt‚Äôs insights. Pareto selection prevents this by construction: you can‚Äôt remove a prompt from the frontier unless something else does everything it does, <em>plus more</em>.</p>
<p>This is the core of GEPA‚Äôs sample efficiency. Instead of needing thousands of examples to statistically rediscover lost insights, the frontier <strong>never loses them in the first place</strong>.</p>
<hr>
<p>We just saw Pareto selection preserve iteration 3‚Äôs prompt despite its 10% aggregate score‚Äîbecause it solved instance #9, which nothing else could. But this raises a question: <em>why</em> does keeping ‚Äúlosers‚Äù help optimization? Shouldn‚Äôt we focus resources on the best candidates?</p>
<p>The answer comes from <strong>quality-diversity</strong> algorithms, a family of techniques from evolutionary computation that GEPA draws from directly.</p>
</section>
</section>
<section id="why-pareto-works-quality-diversity-and-map-elites" class="level3">
<h3 class="anchored" data-anchor-id="why-pareto-works-quality-diversity-and-map-elites">Why Pareto Works: Quality-Diversity and Map Elites</h3>
<p>We‚Äôve now built both core mechanisms from scratch‚Äîreflective mutation and Pareto selection. Before diving into the full algorithm, let‚Äôs briefly step back and understand <em>why</em> this approach works so well.</p>
<p>The Pareto frontier isn‚Äôt a novel invention‚Äîit draws from <strong>quality-diversity (QD)</strong> algorithms, a family of techniques from evolutionary computation that have proven remarkably effective in domains where diversity itself is valuable.</p>
<section id="the-core-insight" class="level4">
<h4 class="anchored" data-anchor-id="the-core-insight">The Core Insight</h4>
<p>Traditional optimization asks: <em>‚ÄúWhat‚Äôs the single best solution?‚Äù</em></p>
<p>Quality-diversity asks: <em>‚ÄúWhat‚Äôs the best solution of each <em>type</em>?‚Äù</em></p>
<p>Complex problems often have multiple valid approaches. A chess engine that always plays aggressively might beat one that always plays defensively‚Äîbut the best engine knows <em>both</em> styles and picks situationally. Maintaining diverse specialists, then combining their insights, outperforms converging prematurely on one ‚Äúbest‚Äù approach.</p>
</section>
<section id="map-elites-the-inspiration" class="level4">
<h4 class="anchored" data-anchor-id="map-elites-the-inspiration">Map Elites: The Inspiration</h4>
<p><a href="https://arxiv.org/abs/1504.04909">Map Elites</a> (Mouret &amp; Clune, 2015) maintains an <em>archive</em> organized by behavior:</p>
<ol type="1">
<li><strong>Define behavior dimensions</strong> ‚Äî characteristics describing <em>how</em> a solution works (not just how well)</li>
<li><strong>Discretize into bins</strong> ‚Äî each cell represents a ‚Äúniche‚Äù</li>
<li><strong>Keep the best per bin</strong> ‚Äî new solutions compete only within their niche</li>
<li><strong>Mutate from the archive</strong> ‚Äî sample from any occupied bin, mutate, place in appropriate bin</li>
</ol>
<p>The result: diverse specialists, each optimal <em>of its type</em>. <strong>Key insight</strong>: the archive provides <em>stepping stones</em>‚Äîa mutation from one niche might discover something useful for another. Diversity isn‚Äôt just nice to have; it‚Äôs a search strategy.</p>
</section>
<section id="gepas-adaptation-validation-instances-as-niches" class="level4">
<h4 class="anchored" data-anchor-id="gepas-adaptation-validation-instances-as-niches">GEPA‚Äôs Adaptation: Validation Instances as Niches</h4>
<p>GEPA recognizes that <strong>the validation set itself defines the behavior space</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 66%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Map Elites</th>
<th>GEPA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Behavior = continuous dimensions</td>
<td>Behavior = which validation instances are solved</td>
</tr>
<tr class="even">
<td>Bins = discretized regions</td>
<td>‚ÄúBins‚Äù = individual validation instances</td>
</tr>
<tr class="odd">
<td>Archive = best per bin</td>
<td>Pareto frontier = non-dominated prompts across instances</td>
</tr>
</tbody>
</table>
<p>Each validation instance encodes what domain-specific insights are required. A prompt solving only instance #7 is the ‚Äúspecialist for that niche‚Äù‚Äîit stays because it demonstrates <em>something works</em>, even with low aggregate score.</p>
<blockquote class="blockquote">
<p><em>‚ÄúBy tracking the best-performing candidate on each validation instance, GEPA can maintain all the domain-specific insights that solve any of these problems.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>This directly motivated GEPA‚Äôs design: Pareto selection over per-instance scores naturally implements QD‚Äôs ‚Äúbest per niche‚Äù principle, while the merge operation recombines insights across niches‚Äîexactly what stepping-stone search requires.</p>
<p>With both mechanisms in place, there‚Äôs one more operation that makes GEPA powerful: <strong>merge</strong>‚Äîcombining insights from divergent lineages.</p>
</section>
</section>
<section id="the-lineage-tree-and-system-aware-merge" class="level3">
<h3 class="anchored" data-anchor-id="the-lineage-tree-and-system-aware-merge">The Lineage Tree and System-Aware Merge</h3>
<p>Pareto selection preserves specialists‚Äîbut it creates a new problem: <strong>insights get siloed in separate branches</strong>.</p>
<p>Consider what happens after 10 iterations of GEPA on AIME problems:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/Merge_Visual.webp" class="img-fluid figure-img"></p>
<figcaption>Lineage Tree with Merge Operation</figcaption>
</figure>
</div>
<p>The P2‚ÜíP4 lineage accumulated <strong>algebra insights</strong>. The P3‚ÜíP5 lineage accumulated <strong>number theory insights</strong>. Both survive on the Pareto frontier because each solves problems the other can‚Äôt.</p>
<p>But what about a problem requiring <em>both</em>?</p>
<hr>
<section id="the-recombination-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-recombination-problem">The Recombination Problem</h4>
<p>Suppose validation instance #7 needs algebraic manipulation to set up a system of equations, then modular arithmetic to constrain the solution space. Neither P4 nor P5 can solve it:</p>
<ul>
<li>P4 knows to subtract equations pairwise, but doesn‚Äôt think to reduce mod p</li>
<li>P5 knows CRT, but misses the algebraic setup</li>
</ul>
<p>With mutation alone, P4 would need to <em>independently rediscover</em> number theory (which P5 already knows), or vice versa. The knowledge exists in our population‚Äîjust in different branches.</p>
<p><strong>Merge</strong> solves this by combining insights from divergent lineages into a single candidate.</p>
<hr>
</section>
<section id="how-merge-works" class="level4">
<h4 class="anchored" data-anchor-id="how-merge-works">How Merge Works</h4>
<p>GEPA‚Äôs merge is ‚Äúsystem-aware‚Äù‚Äîthe LLM understands what it‚Äôs combining, so it can resolve contradictions and synthesize coherently rather than blindly concatenating (unlike genetic algorithm crossover which swaps segments randomly). The reflection LLM receives:</p>
<ol type="1">
<li><strong>Both parent prompts</strong> with their full instruction text</li>
<li><strong>Lineage context</strong> ‚Äî what types of problems each lineage solved</li>
<li><strong>Conflict guidance</strong> ‚Äî instructions to resolve contradictions, not ignore them</li>
</ol>
<p>The prompt asks the LLM to <em>synthesize</em>, not concatenate:</p>
<blockquote class="blockquote">
<p>‚ÄúCreate a SINGLE unified instruction set that incorporates key insights from BOTH. Preserve specific strategies. Resolve contradictions thoughtfully. Don‚Äôt simply concatenate‚Äîsynthesize into coherent guidance.‚Äù</p>
</blockquote>
<p><strong>Concrete example</strong> ‚Äî merging our AIME specialists:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 30%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Parent</th>
<th>Specialty</th>
<th>Key instruction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P4</td>
<td>Algebra</td>
<td>‚ÄúSubtract equations pairwise to expose (x-z)(y-A)=0. Enumerate all cases.‚Äù</td>
</tr>
<tr class="even">
<td>P5</td>
<td>Number theory</td>
<td>‚ÄúApply CRT for modular constraints. Check for contradictions.‚Äù</td>
</tr>
</tbody>
</table>
<p><strong>Merged offspring P6:</strong> &gt; ‚ÄúApproach: (1) For equation systems, subtract pairwise to expose factor relationships‚Äîenumerate all cases including edge cases. (2) When modular conditions appear, apply CRT and check for contradictions. (3) Competition problems often combine both: set up the algebra first, then use number-theoretic constraints to eliminate impossible cases.‚Äù</p>
<p>P6 inherits both toolkits AND adds <strong>meta-knowledge</strong> about when to combine them. This is the ‚Äúsystem-aware‚Äù part‚Äîthe merge understands the semantics of what it‚Äôs combining.</p>
<hr>
</section>
<section id="when-to-merge-vs.-mutate" class="level4">
<h4 class="anchored" data-anchor-id="when-to-merge-vs.-mutate">When to Merge vs.&nbsp;Mutate</h4>
<p>GEPA alternates between operations based on frontier state:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Condition</th>
<th>Operation</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Early iterations (&lt; 5)</td>
<td>Mutate</td>
<td>Let lineages diverge first; nothing to merge yet</td>
</tr>
<tr class="even">
<td>Frontier has one dominant lineage</td>
<td>Mutate</td>
<td>No orthogonal insights to combine</td>
</tr>
<tr class="odd">
<td>Frontier has divergent specialists</td>
<td>Merge</td>
<td>Recombine discoveries from parallel explorations</td>
</tr>
<tr class="even">
<td>Recent merge succeeded</td>
<td>Mutate</td>
<td>Refine the merged candidate</td>
</tr>
</tbody>
</table>
<p>The paper describes the decision:</p>
<blockquote class="blockquote">
<p><em>‚ÄúAs we run GEPA for longer, an evolutionary tree appears where different lineages can have different insights gathered into them. System-aware merge tries to merge two different lineages to encapsulate the insights gathered in them into a single candidate.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<hr>
</section>
<section id="merge-vs.-genetic-algorithm-crossover" class="level4">
<h4 class="anchored" data-anchor-id="merge-vs.-genetic-algorithm-crossover">Merge vs.&nbsp;Genetic Algorithm Crossover</h4>
<p>GEPA‚Äôs merge is analogous to crossover in genetic algorithms, but smarter:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 62%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Genetic Algorithms</th>
<th>GEPA Merge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Genes = bit positions</td>
<td>Insights = natural language instructions</td>
</tr>
<tr class="even">
<td>Crossover = swap bit segments randomly</td>
<td>Merge = LLM synthesizes with understanding</td>
</tr>
<tr class="odd">
<td>Can create invalid offspring</td>
<td>Can resolve contradictions</td>
</tr>
<tr class="even">
<td>Blind to semantics</td>
<td>Aware of what instructions <em>mean</em></td>
</tr>
</tbody>
</table>
<p>Random crossover might produce: ‚ÄúSubtract equations pairwise. Apply CRT. Subtract equations pairwise.‚Äù (nonsense duplication)</p>
<p>LLM merge produces: ‚ÄúSet up algebra first, then apply number-theoretic constraints.‚Äù (coherent synthesis)</p>
<hr>
</section>
<section id="the-compounding-effect" class="level4">
<h4 class="anchored" data-anchor-id="the-compounding-effect">The Compounding Effect</h4>
<p>The components reinforce each other:</p>
<ol type="1">
<li><strong>Pareto selection</strong> preserves the diversity that makes merge valuable</li>
<li><strong>Lineage tracking</strong> identifies which candidates come from divergent branches<br>
</li>
<li><strong>Merge</strong> recombines orthogonal discoveries into unified candidates</li>
<li><strong>Pareto selection</strong> then preserves successful merges alongside remaining specialists</li>
</ol>
<p>Without Pareto selection, there‚Äôs nothing interesting to merge‚Äîyou‚Äôd just have variants of one ‚Äúbest‚Äù prompt. Without merge, insights stay siloed even when the frontier is diverse.</p>
<blockquote class="blockquote">
<p><strong>Implementation note</strong>: The full GEPA implementation includes safeguards to ensure merge candidates actually have different insights worth combining‚Äîchecking for common ancestors, avoiding redundant merges, and verifying that descendants improved on their ancestor. See the <a href="https://github.com/stanfordnlp/dspy">DSPy source</a> for details.</p>
</blockquote>
<p>Mutation explores <em>depth</em>‚Äîrefining one approach through successive reflections.<br>
Merge explores <em>breadth</em>‚Äîcombining orthogonal discoveries from parallel paths.</p>
<p>Together, they cover the search space efficiently: diversify through mutation, consolidate through merge, preserve all valuable discoveries through Pareto selection.</p>
<p>With reflective mutation, Pareto selection, and merge all in place, here‚Äôs how they combine into GEPA‚Äôs full optimization loop.</p>
</section>
</section>
<section id="the-complete-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-complete-algorithm">The Complete Algorithm</h3>
<p>With all pieces in place, here‚Äôs how they combine into the full optimization loop.</p>
<section id="algorithm-overview" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-overview">Algorithm Overview</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gepa(</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    base_prompt: <span class="bu">str</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    trainset: <span class="bu">list</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    valset: <span class="bu">list</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    evaluate_fn: Callable,          <span class="co"># (prompt, example) -&gt; score (0 or 1)</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    run_with_feedback_fn: Callable, <span class="co"># (prompt, examples) -&gt; (traces: list[str], feedback: list[str])</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    reflect_fn: Callable,           <span class="co"># (parent_prompt, traces, feedback) -&gt; new_prompt: str</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    merge_fn: Callable,             <span class="co"># (prompt1, prompt2) -&gt; merged_prompt: str</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    max_iterations: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    minibatch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">    GEPA: Genetic-Pareto prompt optimization.</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns the best aggregate prompt; access full frontier via returned dict.</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --- Helper functions ---</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> evaluate_all(prompt, dataset):</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return per-instance scores as list."""</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [evaluate_fn(prompt, ex) <span class="cf">for</span> ex <span class="kw">in</span> dataset]</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> evaluate_minibatch(prompt, minibatch):</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return aggregate score on minibatch."""</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(evaluate_fn(prompt, ex) <span class="cf">for</span> ex <span class="kw">in</span> minibatch) <span class="op">/</span> <span class="bu">len</span>(minibatch)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dominates(scores_a, scores_b):</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Does A dominate B? (&gt;= everywhere, &gt; somewhere)"""</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> np.array(scores_a), np.array(scores_b)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (a <span class="op">&gt;=</span> b).<span class="bu">all</span>() <span class="kw">and</span> (a <span class="op">&gt;</span> b).<span class="bu">any</span>()</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> is_dominated_by_frontier(new_scores, frontier, scores):</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Is new_scores dominated by ANY frontier member?"""</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">any</span>(dominates(scores[c], new_scores) <span class="cf">for</span> c <span class="kw">in</span> frontier)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_from_frontier(frontier, scores):</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample weighted by unique wins."""</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        n_instances <span class="op">=</span> <span class="bu">len</span>(<span class="bu">next</span>(<span class="bu">iter</span>(scores.values())))</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> []</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> candidate <span class="kw">in</span> frontier:</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(frontier) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>                unique_wins <span class="op">=</span> n_instances</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>                others_max <span class="op">=</span> np.array([scores[o] <span class="cf">for</span> o <span class="kw">in</span> frontier <span class="cf">if</span> o <span class="op">!=</span> candidate]).<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>                unique_wins <span class="op">=</span> (np.array(scores[candidate]) <span class="op">&gt;</span> others_max).<span class="bu">sum</span>()</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>            weights.append(unique_wins <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># +1 smoothing</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choices(frontier, weights<span class="op">=</span>weights)[<span class="dv">0</span>]</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_root(prompt, lineage):</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Trace lineage back to root."""</span></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> lineage.get(prompt) <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">=</span> lineage[prompt]</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prompt</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> should_merge(frontier, lineage, iteration):</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Decide whether to merge or mutate."""</span></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(frontier) <span class="op">&lt;</span> <span class="dv">2</span> <span class="kw">or</span> iteration <span class="op">&lt;</span> <span class="dv">5</span>:</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>        n_lineages <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(get_root(c, lineage) <span class="cf">for</span> c <span class="kw">in</span> frontier))</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.random() <span class="op">&lt;</span> (n_lineages <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> <span class="bu">len</span>(frontier)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> select_divergent(frontier, parent, lineage):</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Select a candidate from a different lineage."""</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>        parent_root <span class="op">=</span> get_root(parent, lineage)</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>        others <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> frontier <span class="cf">if</span> get_root(c, lineage) <span class="op">!=</span> parent_root]</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(others) <span class="cf">if</span> others <span class="cf">else</span> random.choice(frontier)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> best_aggregate(frontier, scores):</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return prompt with highest aggregate score."""</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">max</span>(frontier, key<span class="op">=</span><span class="kw">lambda</span> c: <span class="bu">sum</span>(scores[c]))</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --- Main loop ---</span></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">=</span> [base_prompt]</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> {base_prompt: evaluate_all(base_prompt, valset)}</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>    pareto_frontier <span class="op">=</span> [base_prompt]</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>    lineage <span class="op">=</span> {base_prompt: <span class="va">None</span>}</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_iterations <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. SAMPLE: Select parent from Pareto frontier</span></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>        parent <span class="op">=</span> sample_from_frontier(pareto_frontier, scores)</span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. PROPOSE: Either mutate or merge</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>        minibatch <span class="op">=</span> random.sample(trainset, <span class="bu">min</span>(minibatch_size, <span class="bu">len</span>(trainset)))</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> should_merge(pareto_frontier, lineage, iteration):</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a>            other_parent <span class="op">=</span> select_divergent(pareto_frontier, parent, lineage)</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>            new_prompt <span class="op">=</span> merge_fn(parent, other_parent)</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>            traces, feedback <span class="op">=</span> run_with_feedback_fn(parent, minibatch)</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a>            new_prompt <span class="op">=</span> reflect_fn(parent, traces, feedback)</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. EVALUATE: Mini-batch gate</span></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>        parent_mb_score <span class="op">=</span> evaluate_minibatch(parent, minibatch)</span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>        new_mb_score <span class="op">=</span> evaluate_minibatch(new_prompt, minibatch)</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> new_mb_score <span class="op">&lt;=</span> parent_mb_score:</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span>  <span class="co"># Reject: didn't improve on mini-batch</span></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Full evaluation</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a>        new_scores <span class="op">=</span> evaluate_all(new_prompt, valset)</span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. UPDATE: Pareto frontier maintenance</span></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_dominated_by_frontier(new_scores, pareto_frontier, scores):</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span>  <span class="co"># Reject: dominated by existing candidate</span></span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove dominated candidates</span></span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a>        pareto_frontier <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> pareto_frontier </span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">if</span> <span class="kw">not</span> dominates(new_scores, scores[c])]</span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add new candidate</span></span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a>        candidates.append(new_prompt)</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>        scores[new_prompt] <span class="op">=</span> new_scores</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a>        pareto_frontier.append(new_prompt)</span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a>        lineage[new_prompt] <span class="op">=</span> parent</span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>        <span class="st">'best'</span>: best_aggregate(pareto_frontier, scores),</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a>        <span class="st">'frontier'</span>: pareto_frontier,</span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a>        <span class="st">'scores'</span>: scores,</span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lineage'</span>: lineage,</span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>    }</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="the-key-decision-points" class="level4">
<h4 class="anchored" data-anchor-id="the-key-decision-points">The Key Decision Points</h4>
<p><strong>1. Candidate Sampling</strong> ‚Äî Weighted by unique wins, so specialists get attention proportional to their unique value.</p>
<p><strong>2. Mutation vs Merge</strong> ‚Äî Early iterations favor mutation; merge probability increases as frontier diversifies.</p>
<p><strong>3. Mini-Batch Gating</strong></p>
<p>Before expensive full evaluation, GEPA checks if the new candidate improves on the mini-batch it was designed to fix. This saves compute on obviously bad mutations:</p>
<blockquote class="blockquote">
<p><em>‚ÄúWe propose one new candidate, do a mini-batch evaluation to see whether this new candidate improves on this mini-batch or not. And if it does improve, then we track the score for this new candidate on all validation instances.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p><strong>4. Pareto Update</strong></p>
<p>The frontier update follows the dominance logic we implemented earlier: - <strong>Reject</strong> new candidates dominated by existing ones (they add nothing) - <strong>Remove</strong> existing candidates dominated by the new one (they‚Äôre obsolete) - <strong>Keep</strong> all non-dominated candidates (each offers unique value)</p>
<hr>
</section>
<section id="complexity-analysis" class="level4">
<h4 class="anchored" data-anchor-id="complexity-analysis">Complexity Analysis</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 64%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mutation (3-4 rollouts + reflection)</td>
<td>3-4 LLM calls + 1 reflection call</td>
</tr>
<tr class="even">
<td>Mini-batch evaluation</td>
<td>3-4 metric evaluations</td>
</tr>
<tr class="odd">
<td>Full validation evaluation</td>
<td>N metric evaluations (N = valset size)</td>
</tr>
<tr class="even">
<td>Pareto check</td>
<td>O(F √ó N) comparisons (F = frontier size)</td>
</tr>
</tbody>
</table>
<p>The mini-batch gate is crucial for efficiency. Most mutations fail‚Äîthey either don‚Äôt improve on their target examples or regress elsewhere. Catching failures early (3 evaluations) rather than late (N evaluations) saves ~90% of evaluation budget on rejected candidates.</p>
<hr>
</section>
<section id="why-each-component-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-each-component-matters">Why Each Component Matters</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 37%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Without it</th>
<th>With it</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Textual feedback</strong></td>
<td>Optimizer sees only <code>score=0.6</code></td>
<td>Optimizer reads ‚Äúwrong answer, expected X, got Y because‚Ä¶‚Äù</td>
</tr>
<tr class="even">
<td><strong>Pareto selection</strong></td>
<td>Specialists discarded when aggregate drops</td>
<td>Specialists preserved if they solve anything unique</td>
</tr>
<tr class="odd">
<td><strong>Lineage tracking</strong></td>
<td>No memory of evolutionary history</td>
<td>Can identify divergent branches for merge</td>
</tr>
<tr class="even">
<td><strong>Merge operation</strong></td>
<td>Insights stay siloed in separate branches</td>
<td>Orthogonal discoveries can combine</td>
</tr>
<tr class="odd">
<td><strong>Mini-batch gating</strong></td>
<td>Evaluate every candidate fully</td>
<td>Reject obvious failures cheaply</td>
</tr>
</tbody>
</table>
<p>The components compound. Pareto selection preserves the diversity that makes merge valuable. Textual feedback makes both mutation and merge more effective. Mini-batch gating makes the whole loop affordable.</p>
<hr>
</section>
<section id="what-gets-returned" class="level4">
<h4 class="anchored" data-anchor-id="what-gets-returned">What Gets Returned</h4>
<p>The algorithm returns <code>best_aggregate(pareto_frontier)</code>‚Äîthe prompt with highest overall validation score. But for analysis, the full frontier is valuable: in DSPy, use <code>track_stats=True</code> to access all candidates and their per-instance scores.</p>
<hr>
</section>
</section>
<section id="bringing-it-together" class="level3">
<h3 class="anchored" data-anchor-id="bringing-it-together">Bringing It Together</h3>
<p>The algorithm above integrates everything we‚Äôve built: reflective mutation extracts lessons from textual feedback, Pareto selection preserves specialist insights that greedy selection would discard, and merge recombines discoveries from divergent lineages.</p>
<p>But what does all this machinery actually <em>produce</em>? The answer reveals why GEPA matters beyond just better benchmark scores.</p>
</section>
<section id="what-gepa-learns-domain-specific-knowledge-encoding" class="level3">
<h3 class="anchored" data-anchor-id="what-gepa-learns-domain-specific-knowledge-encoding">What GEPA Learns: Domain-Specific Knowledge Encoding</h3>
<p>We‚Äôve built the full algorithm‚Äîreflective mutation, Pareto selection, merge. But what does all this machinery actually <em>produce</em>? Let‚Äôs examine the output: prompts that encode domain expertise.</p>
<p>One of GEPA‚Äôs most striking capabilities is its ability to <strong>encode domain-specific knowledge directly into prompts</strong>‚Äîtransforming tacit expertise into explicit instructions that persist across examples.</p>
<section id="prompts-as-knowledge-containers" class="level4">
<h4 class="anchored" data-anchor-id="prompts-as-knowledge-containers">Prompts as Knowledge Containers</h4>
<p>Traditional optimization treats prompts as opaque strings to be scored. GEPA treats them as <strong>knowledge containers</strong> that accumulate insights through the reflection loop:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/failure_acc_experience.webp" class="img-fluid figure-img"></p>
<figcaption>Failure Accumulation Experience</figcaption>
</figure>
</div>
<p>Each iteration doesn‚Äôt just fix one error‚Äîit extracts the <em>lesson</em> behind the error.</p>
<p><strong>Concrete example from our AIME experiments:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Prompt excerpt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Seed</strong></td>
<td>‚ÄúYou are given a problem and you have to give the answer along with reasoning.‚Äù</td>
</tr>
<tr class="even">
<td><strong>After iter 2</strong></td>
<td>‚Äú‚Ä¶For systems like xy+Az=C, yz+Ax=C, zx+Ay=C, subtract equations pairwise to expose factor relationships like (x-z)(y-A)=0. Enumerate all cases including x=z and y=A.‚Äù</td>
</tr>
<tr class="odd">
<td><strong>After iter 3</strong></td>
<td>‚Äú‚Ä¶When remainders appear (n mod x, n mod y), check for contradictions via modular arithmetic. An answer of 0 often indicates impossible constraints.‚Äù</td>
</tr>
</tbody>
</table>
<p>The prompt evolved from generic instruction to encoding <strong>competition math heuristics</strong>. The LLM didn‚Äôt invent these‚Äîit extracted them from its training knowledge, triggered by seeing specific failure modes.</p>
</section>
<section id="three-categories-of-captured-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="three-categories-of-captured-knowledge">Three Categories of Captured Knowledge</h4>
<p>We observe GEPA capturing different types of domain expertise (our interpretive taxonomy, not from the paper):</p>
<p><strong>1. Format and Interface Knowledge</strong> - Output schemas (‚Äúreturn JSON with keys: answer, reasoning‚Äù) - API conventions (‚Äúuse library.method(), not library_method()‚Äù) - Parsing requirements (‚Äúintegers only, no leading zeros‚Äù)</p>
<p>This is easiest to extract‚Äîformat errors produce explicit feedback.</p>
<p><strong>2. Strategic Knowledge</strong> - Problem-solving heuristics (‚Äútry small cases first‚Äù) - Domain patterns (‚Äúcompetition problems often combine algebra and number theory‚Äù) - Failure mode awareness (‚Äúwatch for off-by-one errors in counting‚Äù)</p>
<p>This emerges from reflecting on <em>why</em> approaches failed, not just <em>that</em> they failed.</p>
<p><strong>3. Factual Domain Knowledge</strong> - API names and signatures (‚Äútorch.einsum, not torch.einstein_sum‚Äù) - Domain constants (‚Äústandard gravity = 9.81 m/s¬≤‚Äù) - Constraint relationships (‚Äúin valid Sudoku, each row/column/box contains 1-9 exactly once‚Äù)</p>
<p>The LLM already knows this‚Äîreflection surfaces it into the prompt where it‚Äôs consistently applied.</p>
</section>
<section id="why-prompts-beat-weights-sometimes" class="level4">
<h4 class="anchored" data-anchor-id="why-prompts-beat-weights-sometimes">Why Prompts Beat Weights (Sometimes)</h4>
<p>Fine-tuning encodes knowledge in model weights‚Äîopaque, distributed, hard to inspect. GEPA encodes knowledge in natural language‚Äîreadable, editable, composable.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>Fine-tuning</th>
<th>GEPA prompts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Inspectability</strong></td>
<td>Black box</td>
<td>Human-readable instructions</td>
</tr>
<tr class="even">
<td><strong>Editability</strong></td>
<td>Requires retraining</td>
<td>Edit the text directly</td>
</tr>
<tr class="odd">
<td><strong>Composability</strong></td>
<td>Train new model</td>
<td>Merge prompt sections</td>
</tr>
<tr class="even">
<td><strong>Sample efficiency</strong></td>
<td>Thousands of examples</td>
<td>Tens of examples</td>
</tr>
</tbody>
</table>
<p>A GEPA-optimized prompt can be read by a human to understand what strategies it learned, edited to add domain knowledge the optimizer missed, and even transferred to different LLMs.</p>
</section>
<section id="the-preservation-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-preservation-problem">The Preservation Problem</h4>
<p>Knowledge accumulation has a failure mode: <strong>new insights can overwrite old ones</strong>. We saw this in our hands-on experiment‚Äîiteration 3‚Äôs number theory insights overwrote iteration 2‚Äôs algebra insights, causing catastrophic forgetting.</p>
<p>This is precisely why GEPA uses <strong>Pareto selection</strong>. By preserving prompts that are best on <em>any</em> validation instance, Pareto ensures specialized knowledge survives even when aggregate scores dip. The algebra specialist and number theory specialist both stay on the frontier‚Äîand the merge operation can later combine their insights.</p>
<blockquote class="blockquote">
<p><em>Without Pareto selection, GEPA would be a knowledge accumulator that periodically erases its own discoveries.</em></p>
</blockquote>
</section>
<section id="limitation-knowledge-must-be-triggerable" class="level4">
<h4 class="anchored" data-anchor-id="limitation-knowledge-must-be-triggerable">Limitation: Knowledge Must Be Triggerable</h4>
<p>GEPA can only surface knowledge the base LLM already has‚Äîit extracts and organizes existing knowledge, not creates new knowledge. This is why it works better with stronger base models: more latent knowledge to extract. For highly specialized domains (custom hardware APIs, proprietary protocols), you may need human-curated seed instructions or few-shot examples to bootstrap the reflection loop.</p>
<hr>
<p>So far we‚Äôve focused on <em>training</em>: optimize prompts on labeled examples, deploy the best one. But GEPA‚Äôs machinery‚Äîreflective mutation, Pareto selection, merge‚Äîcan also be deployed at <em>inference time</em> as a search algorithm. This opens a second paradigm worth understanding.</p>
</section>
</section>
</section>
<section id="beyond-training-gepa-for-inference-time-search" class="level2">
<h2 class="anchored" data-anchor-id="beyond-training-gepa-for-inference-time-search">Beyond Training: GEPA for Inference-Time Search</h2>
<p>Everything we‚Äôve covered so far assumes a familiar workflow: optimize on training data, deploy the result on new tasks. But GEPA supports a second paradigm that inverts this relationship entirely.</p>
<section id="two-paradigms-of-operation" class="level3">
<h3 class="anchored" data-anchor-id="two-paradigms-of-operation">Two Paradigms of Operation</h3>
<p><strong>Train-then-generalize</strong> (what we‚Äôve built so far):</p>
<ul>
<li>Optimize prompts on a training set</li>
<li>Select the best-aggregate prompt from the Pareto frontier</li>
<li>Deploy that prompt on new, unseen tasks</li>
<li>Goal: learn <em>generalizable</em> lessons that transfer</li>
</ul>
<p><strong>Test-time search</strong> (inference-time optimization):</p>
<ul>
<li>You have a batch of hard tasks you need to solve <em>now</em></li>
<li>Optimize directly on the tasks themselves</li>
<li>GEPA searches for solutions, storing the best prompt <em>per task</em></li>
<li>Goal: maximize performance on <em>these specific instances</em></li>
</ul>
<p>The mechanics are identical‚Äîreflective mutation, Pareto selection, merge. What changes is the intent: instead of learning transferable knowledge, you‚Äôre using GEPA as a <strong>search algorithm</strong> over the solution space. This aligns with the broader trend of <a href="https://openai.com/index/learning-to-reason-with-llms/">inference-time compute scaling</a>‚Äîinvesting more computation at inference to solve harder problems.</p>
<p><strong>The key mechanical change</strong>: In normal GEPA, you have separate <code>trainset</code> (to learn from via reflection) and <code>valset</code> (to evaluate generalization). The Pareto frontier tracks per-instance performance on <code>valset</code>, preserving prompts that generalize well.</p>
<p>For test-time search, pass the <em>same</em> problems as both <code>trainset</code> and <code>valset</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test-time search: optimize directly on the tasks you want to solve</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>optimized <span class="op">=</span> GEPA(metric<span class="op">=</span>metric_with_feedback, auto<span class="op">=</span><span class="st">"medium"</span>).<span class="bu">compile</span>(</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    program,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    trainset<span class="op">=</span>hard_problems,  <span class="co"># The actual tasks you need solved</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    valset<span class="op">=</span>hard_problems,    <span class="co"># Same held-out validation</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This tells GEPA: ‚ÄúI don‚Äôt care about generalization‚Äîoptimize directly on <em>these specific problems</em>.‚Äù The Pareto frontier now tracks ‚Äúbest prompt for each problem‚Äù rather than ‚Äúprompts that transfer to unseen data.‚Äù See the <a href="https://dspy.ai/api/optimizers/GEPA/overview/">GEPA API documentation</a> for full parameter details.</p>
<blockquote class="blockquote">
<p><em>‚ÄúGiven a batch of tasks that we want to solve and given some budget‚Ä¶ GEPA can propose and update its own strategy to solve that particular task iteratively till that task is solved.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<hr>
</section>
<section id="why-gepa-beats-high-temperature-sampling" class="level3">
<h3 class="anchored" data-anchor-id="why-gepa-beats-high-temperature-sampling">Why GEPA Beats High-Temperature Sampling</h3>
<p>Traditional inference-time strategies sample at high temperature to generate many candidates, then pick the best. But these samples tend to be <em>similar</em>‚Äîvariations on the same approach. GEPA induces <strong>genuine diversity</strong> through Pareto tracking (maintaining candidates that excel at <em>something different</em>) and reflective mutation (proposing <em>structurally different</em> strategies based on what went wrong, not random perturbations).</p>
<p>When feedback says ‚Äúmemory bandwidth bottleneck,‚Äù the next candidate might switch from a naive loop to shared-memory tiling‚Äîa qualitative change that temperature variation rarely discovers. On the <a href="https://arxiv.org/abs/2103.03874">MATH benchmark</a>, this approach achieves <strong>93% accuracy</strong> compared to 67% with basic DSPy ChainOfThought.</p>
<hr>
</section>
<section id="self-bootstrapping-at-inference" class="level3">
<h3 class="anchored" data-anchor-id="self-bootstrapping-at-inference">Self-Bootstrapping at inference</h3>
<p>During training, you iterate a fixed number of times and deploy the result. At inference time, you can keep iterating <em>on a single hard problem</em> until it‚Äôs solved‚Äîand GEPA‚Äôs reflective loop creates a self-bootstrapping dynamic:</p>
<ol type="1">
<li><strong>Round 1</strong>: Generate rollout ‚Üí compiler error (‚Äúundefined variable x‚Äù) ‚Üí reflect ‚Üí propose fix</li>
<li><strong>Round 2</strong>: Code compiles ‚Üí runtime error (division by zero) ‚Üí reflect ‚Üí propose fix<br>
</li>
<li><strong>Round 3</strong>: Runtime works ‚Üí wrong output (‚Äúexpected 42, got 41‚Äù) ‚Üí reflect ‚Üí propose fix</li>
<li><strong>Round 4</strong>: Correct output ‚úì</li>
</ol>
<p>Each iteration surfaces <em>the next</em> failure mode‚Äîyou can‚Äôt discover the runtime error until the compile error is fixed. Traditional sampling generates 100 candidates that all hit the same compiler error. GEPA‚Äôs iterative reflection <em>progresses through</em> the failure cascade.</p>
<blockquote class="blockquote">
<p><em>‚ÄúIt identifies new challenges that the system is going to encounter as well as at every step it is going to propose a solution to that challenge. So it‚Äôs kind of like self-bootstrapping data to train itself.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>This is why test-time GEPA can solve problems that stumped training: it has the budget to chase failure modes deeper than any fixed training run.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: When optimizing a batch of tasks, Pareto selection ensures that fixing one problem doesn‚Äôt discard prompts that solved others‚Äîsee ‚ÄúCross-Task Transfer Within a Batch‚Äù below.</p>
</blockquote>
<hr>
</section>
<section id="cross-task-transfer-within-a-batch" class="level3">
<h3 class="anchored" data-anchor-id="cross-task-transfer-within-a-batch">Cross-Task Transfer Within a Batch</h3>
<p>When solving related tasks (e.g., a batch of <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">CUDA kernels</a>), insights compound across the batch:</p>
<blockquote class="blockquote">
<p><em>‚ÄúAll of these tasks are highly related. So if I discover an insight that works well on task one, there is a high possibility that it will also work well on task two. So what happens is I use the rollout from task one to update my prompt and then I use that prompt to generate the solution for task two.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>The frontier maintains multiple specialized prompts simultaneously:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt</th>
<th>Specialization</th>
<th>Problems solved</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P_conv</td>
<td>Convolutional operators</td>
<td>#1, #4, #7</td>
</tr>
<tr class="even">
<td>P_reduce</td>
<td>Reduction/summation operators</td>
<td>#2, #5, #8</td>
</tr>
<tr class="odd">
<td>P_matmul</td>
<td>Matrix multiplication</td>
<td>#3, #6</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><em>‚ÄúOne prompt will be highly specialized to the convolution one and this can work well for three-four task instances. Another might specialize to the summation one and this could cater to another three-four instances.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>When a new problem arrives, GEPA tries candidates from across the frontier‚Äîthe convolution specialist might crack it, or the reduction specialist, or insights from both might merge.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="static/cross_task_transfer.webp" class="img-fluid figure-img"></p>
<figcaption>Cross-Task Insight Transfer</figcaption>
</figure>
</div>
<hr>
</section>
<section id="background-optimization-loops" class="level3">
<h3 class="anchored" data-anchor-id="background-optimization-loops">Background Optimization Loops</h3>
<p>The self-bootstrapping pattern suggests an intriguing application: <strong>background GEPA loops for personalization</strong> in tools like Cursor and other AI-assisted environments.</p>
<blockquote class="blockquote">
<p><em>‚ÄúFor your cursor agent use case, I can imagine that there can be a background GEPA loop that runs continuously and every time you give it feedback, it iterates and generates a new generalizing prompt‚Ä¶ cursor can learn a user-specific prompt that works well specifically for you.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>Every correction you provide‚Äîrejecting a verbose explanation, fixing a code style‚Äîbecomes a training signal that accumulates into a personalized prompt. The infrastructure isn‚Äôt widespread yet, but the pattern is clear: continuous adaptation rather than one-shot optimization.</p>
<hr>
</section>
<section id="what-gepa-stores" class="level3">
<h3 class="anchored" data-anchor-id="what-gepa-stores">What GEPA Stores</h3>
<p>For each task in the batch, GEPA tracks both artifacts:</p>
<blockquote class="blockquote">
<p><em>‚ÄúGEPA tracks the best prompt per test case and you can store both the prompt as well as the output generated by that prompt.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<ul>
<li><strong>Best outputs</strong> ‚Äî the actual solutions, ready to use</li>
<li><strong>Best prompts</strong> ‚Äî specialized strategies representing different subdomains of your problem space</li>
</ul>
<p>You can deploy these domain-specific prompts for future similar tasks, or simply extract the outputs and move on.</p>
<hr>
<p>When the conditions <em>are</em> right‚Äîrich feedback, high-value tasks worth the compute, domains where the LLM has strong priors‚Äîtest-time GEPA can dramatically outperform sampling-based approaches. The next section demonstrates this on code generation: GEPA-optimized CUDA kernels that exceed human-written PyTorch baselines, in a domain where even frontier models like OpenAI-o1 and DeepSeek-R1 match the baseline on less than 20% of tasks.</p>
</section>
<section id="case-study-code-optimization-for-novel-hardware" class="level3">
<h3 class="anchored" data-anchor-id="case-study-code-optimization-for-novel-hardware">Case Study: Code Optimization for Novel Hardware</h3>
<p>The GEPA paper demonstrates test-time search on domains where GEPA‚Äôs strengths shine brightest: <strong>code optimization for hardware with limited pre-training data</strong>.</p>
<section id="amd-npu-kernels-optimization-without-pre-training-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="amd-npu-kernels-optimization-without-pre-training-knowledge">AMD NPU Kernels: Optimization Without Pre-Training Knowledge</h4>
<p>AMD‚Äôs NPU (Neural Processing Unit) represents a novel hardware architecture. LLMs have minimal pre-training knowledge about its programming model, memory hierarchy, or optimization patterns. The <a href="https://arxiv.org/abs/2507.14403">NPUEval benchmark</a> reveals just how challenging this is: even with compiler feedback and RAG, state-of-the-art LLMs achieve only ~10% mean vectorization score.</p>
<p>GEPA‚Äôs approach doesn‚Äôt require prior examples‚Äîit iteratively generates kernels, receives compiler errors or performance metrics, reflects on feedback, and proposes targeted improvements. The compiler error messages contain the fix: ‚ÄúSymbol not found: <code>npu_matmul</code>‚Äù triggers reflection that surfaces the correct API.</p>
<blockquote class="blockquote">
<p><em>‚ÄúGEPA can be used to generate optimized kernels for AMD‚Äôs NPU hardware, which is a very novel hardware architecture, without any pre-training knowledge because of how novel the architecture is.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
</section>
<section id="cuda-kernels-outperforming-human-baselines" class="level4">
<h4 class="anchored" data-anchor-id="cuda-kernels-outperforming-human-baselines">CUDA Kernels: Outperforming Human Baselines</h4>
<p><a href="https://scalingintelligence.stanford.edu/blogs/kernelbench/">KernelBench</a> (Stanford, 2025) evaluates LLMs on generating efficient CUDA kernels. The benchmark reveals a sobering baseline: <strong>frontier reasoning models match the PyTorch baseline on less than 20% of tasks</strong> using the fast‚ÇÅ metric (correct <em>and</em> faster than PyTorch). Efficient GPU programming requires optimization patterns (memory coalescing, shared memory tiling, warp-level primitives) that models haven‚Äôt learned to apply reliably.</p>
<p>The KernelBench paper shows that feedback-driven refinement dramatically improves results‚Äîfast‚ÇÅ scores jumped 3-6x when execution results and profiler feedback were provided in context. GEPA applies this same principle systematically, with Pareto tracking to preserve diverse optimization strategies rather than ad-hoc iteration.</p>
<blockquote class="blockquote">
<p><em>‚ÄúFor CUDA we show results on KernelBench where GEPA was able to generate better kernels, sometimes even outperforming the PyTorch baseline which are human-written.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>The <a href="#self-bootstrapping-at-inference">self-bootstrapping dynamic</a> is especially effective here: each compilation error or profiler bottleneck reveals the next fix, letting GEPA progress through failure cascades that stump one-shot sampling.</p>
</section>
<section id="cross-kernel-transfer" class="level4">
<h4 class="anchored" data-anchor-id="cross-kernel-transfer">Cross-Kernel Transfer</h4>
<p>When optimizing a batch of related kernels, insights compound across tasks:</p>
<blockquote class="blockquote">
<p><em>‚ÄúIf I discover an insight that works well on task one, there is a high possibility that it will also work well on task two.‚Äù</em> ‚Äî Lakshya A Agrawal</p>
</blockquote>
<p>As GEPA optimizes each kernel, the Pareto frontier accumulates specialized prompts‚Äîone excelling at memory-bound operations (convolutions), another at compute-bound work (matrix multiplies), a third at reductions. Memory tiling strategies discovered on convolution kernels may transfer to pooling; warp-level primitives learned for reductions may help softmax. This cross-task transfer is why batch optimization outperforms solving each kernel independently.</p>
</section>
</section>
</section>
<section id="conclusion-when-to-reach-for-gepa" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-when-to-reach-for-gepa">Conclusion: When to Reach for GEPA</h2>
<p>We opened with a familiar bind: 50 labeled examples, a prompt that works 70% of the time, and no scalable path forward. GEPA changes that calculus‚Äîmatching RL‚Äôs optimization performance at a fraction of the sample cost by doing something RL can‚Äôt: <em>reading the feedback</em>.</p>
<p>The results speak for themselves: 46.6% ‚Üí 56.6% on AIME math competition problems. 67% ‚Üí 93% on MATH benchmark. CUDA kernels that outperform human-written PyTorch baselines. All achieved through prompt optimization alone, no fine-tuning required.</p>
<p>This represents a genuinely new point in the optimization design space‚Äîone that becomes viable as LLMs get better at self-reflection. Where RL needs thousands of trajectories to statistically isolate what went wrong, GEPA reads the compiler error and proposes the fix. That‚Äôs not incremental improvement‚Äîit‚Äôs a 100x reduction in sample requirements.</p>
<hr>
<section id="use-gepa-for-train-then-generalize-when" class="level3">
<h3 class="anchored" data-anchor-id="use-gepa-for-train-then-generalize-when">Use GEPA for Train-Then-Generalize When:</h3>
<ul>
<li>You have <strong>rich textual feedback</strong> (compiler errors, profiler output, LLM-as-judge rubrics, expert solutions)</li>
<li>Your evaluation budget is <strong>limited</strong> (50-500 examples, not 5,000)</li>
<li>You‚Äôre optimizing <strong>compound AI systems</strong> where prompts orchestrate multi-step pipelines</li>
<li>You need <strong>interpretable results</strong>‚Äîprompts you can read, edit, and reason about</li>
</ul>
</section>
<section id="use-gepa-for-test-time-search-when" class="level3">
<h3 class="anchored" data-anchor-id="use-gepa-for-test-time-search-when">Use GEPA for Test-Time Search When:</h3>
<ul>
<li>You have a <strong>batch of high-value tasks</strong> worth the compute investment</li>
<li>Each task produces <strong>execution feedback</strong> (tests, profilers, validators)</li>
<li>Tasks are <strong>related enough</strong> for cross-task transfer to help</li>
</ul>
</section>
<section id="stick-with-traditional-approaches-when" class="level3">
<h3 class="anchored" data-anchor-id="stick-with-traditional-approaches-when">Stick with Traditional Approaches When:</h3>
<ul>
<li>You have <strong>abundant labeled data</strong> and compute budget for fine-tuning</li>
<li>Feedback is <strong>purely scalar</strong> with no explanatory signal</li>
<li>The task is <strong>already solved</strong> by few-shot prompting</li>
<li>You need <strong>sub-second latency</strong></li>
</ul>
<hr>
</section>
<section id="get-started" class="level3">
<h3 class="anchored" data-anchor-id="get-started">Get Started</h3>
<p>Ready to try GEPA on your own pipelines?</p>
<ul>
<li><strong><a href="https://dspy.ai/tutorials/gepa_aime/">GEPA for AIME Tutorial</a></strong> ‚Äî Complete walkthrough from setup to optimized results</li>
<li><strong><a href="https://dspy.ai/api/optimizers/GEPA/overview/">GEPA API Reference</a></strong> ‚Äî Full parameter documentation</li>
<li><strong><a href="https://arxiv.org/abs/2507.19457">Paper</a></strong> ‚Äî Algorithm details and experimental methodology</li>
</ul>
<p>The core insight is simple: your LLM pipelines already produce rich textual feedback. GEPA just reads it‚Äîand learns.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/risheekkumar\.in");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>